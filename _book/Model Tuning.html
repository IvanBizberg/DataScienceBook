<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.353">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>DS Baseline - 4&nbsp; Model Tuning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Linear Models and Its Cousins.html" rel="next">
<link href="./Pre-processing.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Model Tuning.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Model Tuning</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">DS Baseline</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Terms &amp; definitions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Terms &amp; definitions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Choose model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Choose machine learning model</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Pre-processing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Pre-processing</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Model Tuning.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Model Tuning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Linear Models and Its Cousins.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Linear Regression and Its Cousins</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Nonlinear Models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Nonlinear Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Tree models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Tree based models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Deal with suboptimal data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Remedies for suboptimal data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Measuring Predictor Importance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Measuring Predictor Importance</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Feature Selection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Feature Selection</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#data-splitting-method" id="toc-data-splitting-method" class="nav-link active" data-scroll-target="#data-splitting-method"><span class="header-section-number">4.1</span> Data splitting method</a>
  <ul class="collapse">
  <li><a href="#nonrandom-approaches-to-splitting-the-data" id="toc-nonrandom-approaches-to-splitting-the-data" class="nav-link" data-scroll-target="#nonrandom-approaches-to-splitting-the-data"><span class="header-section-number">4.1.1</span> Nonrandom approaches to splitting the data</a></li>
  <li><a href="#random-sampling-methods" id="toc-random-sampling-methods" class="nav-link" data-scroll-target="#random-sampling-methods"><span class="header-section-number">4.1.2</span> Random sampling methods</a></li>
  </ul></li>
  <li><a href="#choosing-tuning-parameters" id="toc-choosing-tuning-parameters" class="nav-link" data-scroll-target="#choosing-tuning-parameters"><span class="header-section-number">4.2</span> Choosing Tuning Parameters</a></li>
  <li><a href="#metrics-performance-measures" id="toc-metrics-performance-measures" class="nav-link" data-scroll-target="#metrics-performance-measures"><span class="header-section-number">4.3</span> Metrics / performance measures</a>
  <ul class="collapse">
  <li><a href="#for-models-predicting-a-categorical-outcome" id="toc-for-models-predicting-a-categorical-outcome" class="nav-link" data-scroll-target="#for-models-predicting-a-categorical-outcome"><span class="header-section-number">4.3.1</span> For models predicting a categorical outcome</a></li>
  <li><a href="#for-models-predicting-a-numeric-outcome" id="toc-for-models-predicting-a-numeric-outcome" class="nav-link" data-scroll-target="#for-models-predicting-a-numeric-outcome"><span class="header-section-number">4.3.2</span> For models predicting a numeric outcome</a></li>
  </ul></li>
  <li><a href="#model-choosing" id="toc-model-choosing" class="nav-link" data-scroll-target="#model-choosing"><span class="header-section-number">5</span> Model Choosing</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Model Tuning</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p><em>Identify settings for the model’s parameters that yield the best and most realistic predictive performance</em></p>
<section id="data-splitting-method" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="data-splitting-method"><span class="header-section-number">4.1</span> Data splitting method</h2>
<p>A good rule of thumb is about 75–80 % on train subset and the rest for the test subset. Proportionally large test sets divide the data in a way that increases bias in the performance estimates.</p>
<section id="nonrandom-approaches-to-splitting-the-data" class="level3" data-number="4.1.1">
<h3 data-number="4.1.1" class="anchored" data-anchor-id="nonrandom-approaches-to-splitting-the-data"><span class="header-section-number">4.1.1</span> Nonrandom approaches to splitting the data</h3>
<p><strong>Example:</strong> - If a model was being used to predict patient outcomes, the model may be created using certain patient sets (e.g., from the same clinical site or disease stage), and then tested on a different sample population to understand how well the model generalizes.</p>
<ul>
<li><p>In chemical modeling for drug discovery, new “chemical space” is constantly being explored. We are most interested in accurate predictions in the chemical space that is currently being investigated rather than the space that was evaluated years prior.</p></li>
<li><p>In spam filtering; it is more important for the model to catch the new spamming techniques rather than prior spamming schemes.</p></li>
</ul>
</section>
<section id="random-sampling-methods" class="level3" data-number="4.1.2">
<h3 data-number="4.1.2" class="anchored" data-anchor-id="random-sampling-methods"><span class="header-section-number">4.1.2</span> Random sampling methods</h3>
<section id="simple-random-sample" class="level4" data-number="4.1.2.1">
<h4 data-number="4.1.2.1" class="anchored" data-anchor-id="simple-random-sample"><span class="header-section-number">4.1.2.1</span> Simple random sample</h4>
<ul>
<li><p>The simplest way to split the data randomly into a training and test.</p>
<p><strong>Disadvantage</strong>: limited ability to characterize the uncertainty in the results.</p></li>
<li><p>simple k-Fold Cross-Validation: The samples are randomly partitioned into k sets of roughly equal size. A model is fit using the all samples except one subset. The held-out samples are predicted by this model and used to estimate performance measures. The first subset is returned to the training set an procedure repeats with the next subset held out, and so on. Performance estimates, are calculated from each set of held-out samples and then averaged.</p>
<p><strong>NOTES</strong>: The choice of k is usually <strong>5</strong> or <strong>10</strong>, but there is no formal rule. <strong>The bias is smaller for k = 10 than k = 5.</strong> As k gets larger, the difference in size between the training set and the resampling subsets gets smaller. But larger values of k are more computationally burdensome.</p>
<p><strong>Advantage</strong>: Low computational costs. . <strong>Disadvantage</strong>: k-fold cross-validation generally has high variance compared to other methods (only for small training sets). <strong>USE</strong>: If sample sizes are large (&gt; 10 000) and we want to choose tuning parameters</p></li>
<li><p>repeated k-Fold Cross-Validation</p>
<p><strong>Advantage</strong>: Increase the precision of the estimates while still maintaining a small bias. The bias and variance properties are good and, given the sample size, the computational costs are not large. <strong>Disdvantage</strong>: Large computational costs. <strong>USE</strong>: with k = 10; If the samples size is small (&lt; 1000 obs) and we want to choose tuning parameters</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/k-Fold Cross-Validation.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">k-Fold Cross-Validation</figcaption>
</figure>
</div>
<ul>
<li><p>leave-one-out Cross-Validation / LOOCV: fits as many models as there are samples in the training set, should only be considered when the number of samples is very small.</p></li>
<li><p>leave-group-out Cross-Validation / Repeated training/test splits / Monte Carlo cross-validation: Same as k-fold cross-validation except that samples can be represented in multiple held-out subsets. Also, the number of repetitions is usually larger than in k-fold cross-validation</p>
<p><strong>Disadvantage</strong>:</p>
<p><strong>NOTES</strong>: Increase the number of repetition can allow to increase the proportion of data in the train set and decreasing the uncertainty of the performance estimates. To get stable estimates of performance, it is suggested to choose a larger number of repetitions <strong>(say 50–200)</strong></p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/leave-group-out Cross-Validation.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">leave-group-out Cross-Validation</figcaption>
</figure>
</div>
<ul>
<li><p>The Bootstrap: Each train subset is the same size as the original and can contain multiple instances of the same data point (taken with replacement). Samples not selected by the bootstrap (“out-of-bag” samples) are predicted and used to estimate model performance</p>
<p><strong>Advantage</strong>: error rates have less uncertainty than k-fold cross-validation. Very low variance. <strong>Disadvantage</strong>: On average, 63.2 % of the data points the bootstrap sample are represented at least once, so this technique has bias. similar to k-fold cross-validation when k ≈ 2. If the training set size is small, this bias may be problematic, but will decrease as the training set sample size becomes larger. <strong>USE</strong>: If the goal is to choose between models (boosted trees vs support vector machines…), as opposed to getting the best indicator of performance</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/Bootstrap.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Bootstrap</figcaption>
</figure>
</div>
<ul>
<li><p>The Bootstrap 632 method</p>
<p><strong>Advantage</strong>: The modified bootstrap estimate reduces the bias.</p>
<p><strong>Disadvantage</strong>: The estimate is unstable with small samples sizes. This estimate can also result in unduly optimistic results when the model severely over-fits the data, since the apparent error rate will be close to zero.</p></li>
<li><p>The Bootstrap 632+ method <strong>Advantage</strong>: Allows to adjust the bootstrap 632 method estimates</p></li>
</ul>
</section>
<section id="stratified-random" class="level4" data-number="4.1.2.2">
<h4 data-number="4.1.2.2" class="anchored" data-anchor-id="stratified-random"><span class="header-section-number">4.1.2.2</span> Stratified random</h4>
<p>To account for the outcome when splitting the data. Applies random sampling within subgroups (such as the classes or is outcomes are numbers the numeric values are broken into similar groups (e.g., low, medium, and high)).</p>
<ul>
<li>k-Fold Cross-Validation</li>
</ul>
</section>
<section id="maximum-dissimilarity-sampling" class="level4" data-number="4.1.2.3">
<h4 data-number="4.1.2.3" class="anchored" data-anchor-id="maximum-dissimilarity-sampling"><span class="header-section-number">4.1.2.3</span> Maximum dissimilarity sampling</h4>
<p>The data is split on the basis of the predictor values.</p>
</section>
</section>
</section>
<section id="choosing-tuning-parameters" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="choosing-tuning-parameters"><span class="header-section-number">4.2</span> Choosing Tuning Parameters</h2>
<ul>
<li><p>Pick the settings associated with the numerically best performance estimates. <strong>Disadvantage</strong>: lead to models that are overly complicated</p></li>
<li><p>Pick simpler models that provide acceptable performance (relative to the numerically optimal settings)</p>
<ul>
<li><p>The “one-standard error” method: pick the simplier model within a single standard error of the numerically best value. In table below we would pick cost value of 2.</p></li>
<li><p>the “percent decrease in performance” method: pick the simplier model that is within a certain tolerance of the numerically best value. (e.g., The percent decrease in performance could be quantified by (X − O)/O where X is the performance value and O is the numerically optimal value. For example, in Fig. 4.9, the best accuracy value across the profile was 75 %. If a 4 % loss in accuracy was acceptable as a trade-off for a simpler model, accuracy values greater than 71.2 % would be acceptable. For the profile in Fig. 4.9, a cost value of 1 would be chosen using this approach.)</p></li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/Choosing Tuning Parameters.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Cross-validation accuracy</figcaption>
</figure>
</div>
</section>
<section id="metrics-performance-measures" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="metrics-performance-measures"><span class="header-section-number">4.3</span> Metrics / performance measures</h2>
<section id="for-models-predicting-a-categorical-outcome" class="level3" data-number="4.3.1">
<h3 data-number="4.3.1" class="anchored" data-anchor-id="for-models-predicting-a-categorical-outcome"><span class="header-section-number">4.3.1</span> For models predicting a categorical outcome</h3>
<section id="accuracy-based-metrics" class="level4" data-number="4.3.1.1">
<h4 data-number="4.3.1.1" class="anchored" data-anchor-id="accuracy-based-metrics"><span class="header-section-number">4.3.1.1</span> Accuracy based metrics</h4>
<p>A good model has generally a metric above 0.7 / 70%</p>
<ol type="1">
<li><strong>Accuracy</strong>:</li>
</ol>
<ul>
<li><p><em>Higher Value</em>: Better performance (values form 0 to 1)</p></li>
<li><p><em>When to use</em>: Use when the classes are balanced (e.g., Suppose the rate of this disorder1 in fetuses is approximately 1 in 800 or about one-tenth of one percent. A predictive model can achieve almost perfect accuracy by predicting all samples to be negative for Down syndrome.), and misclassification of different classes has similar consequences.</p></li>
<li><p><em>Advantage</em>: Simple and easy to interpret.</p></li>
<li><p><em>Disadvantage</em>: Can be misleading when classes are imbalanced / make no distinction about the type of errors being made</p></li>
<li><p><em>Description</em>: Accuracy measures the proportion of correct predictions out of all predictions made by the model.</p></li>
<li><p><em>Example</em>: Suppose you have a binary classification problem to identify whether an email is spam or not. If your model has an accuracy of 90%, it means it correctly classified 90% of the emails.</p></li>
<li><p><em>Calculation</em>: (True Positives + True Negatives) / (True Positives + True Negatives + False Positives + False Negatives) (Number of Correct Predictions) / (Total Number of Predictions)</p></li>
<li><p><em>Notes</em>:</p>
<ul>
<li><p>When evaluating the accuracy of a model, the baseline accuracy rate to beat would be the percentage which could be achieve by simply predicting all samples to the dominant category (e.g., In the data set, 70 % were rated as having good, accuracy rate to beat would be 70 % which is the no-information rate).</p></li>
<li><p><strong>error rate</strong>: (Number of Incorrect Predictions) / (Total Number of Predictions)</p></li>
</ul></li>
</ul>
<p>1.5) <strong>Kappa</strong>:</p>
<ul>
<li><em>Higher Value</em>: Better performance (values from -1 to 1; 0.30 to 0.50 indicate reasonable agreement)</li>
<li><em>When to use</em>: Rather than calculate the overall accuracy and compare it to the no-information rate, <em>Kappa</em> can be used that take into account the class distributions of the training set samples.</li>
<li><em>Advantage</em>: Takes into account the accuracy that would be generated simply by chance.</li>
<li><em>Disadvantage</em>: NA</li>
<li><em>Description</em>: assess the agreement between two raters</li>
<li><em>Example</em>: 0 means there is no agreement between the observed and predicted classes, while a value of 1 indicates perfect concordance of the model prediction and the observed classes. Negative values indicate that the prediction is in the opposite direction of the truth, but large negative values seldom occur, if ever, when working with predictive models.</li>
<li><em>Calculation</em>: Kappa = O − E / 1−E: O is the observed accuracy and E is the expected accuracy based on the marginal totals of the confusion matrix.</li>
<li><em>Note</em>: The Kappa statistic can also be extended to evaluate concordance in problems with more than two classes. When there is a natural ordering to the classes (e.g., “low,”“medium,” and “high”), an alternate form of the statistic called weighted Kappa can be used to enact more substantial penalties on errors that are further away from the true result. For example, a “low” sample erroneously predicted as “high” would reduce the Kappa statistic more than an error were “low” was predicted to be “medium.” See (Agresti 2002)for more details.</li>
</ul>
<ol start="2" type="1">
<li><strong>Precision</strong>:</li>
</ol>
<ul>
<li><em>Higher Value</em>: Better performance (good = 0.7)</li>
<li><em>When to use</em>: Use when the cost of false positives is high (e.g., medical diagnosis, fraud detection).</li>
<li><em>Advantage</em>: Focuses on the relevance of positive predictions.</li>
<li><em>Disadvantage</em>: Ignores true negatives and may not be suitable for imbalanced datasets.</li>
<li><em>Description</em>: Precision is the proportion of true positive predictions (correctly predicted positive class) out of all positive predictions made by the model.</li>
<li><em>Example</em>: In the spam email example, if your model has a precision of 80%, it means that out of all the emails it predicted as spam, 80% of them were actually spam.</li>
<li><em>Calculation</em>: True Positives / (True Positives + False Positives)</li>
</ul>
<ol start="3" type="1">
<li><strong>Sensitivity / True positive rate / Recall</strong>:</li>
</ol>
<ul>
<li><em>Higher Value</em>: Better performance</li>
<li><em>When to use</em>: Use when the cost of false negatives is high (e.g., medical diagnosis, safety-critical applications).</li>
<li><em>Advantage</em>: Focuses on the completeness of positive predictions (includes true positive and false negatives).</li>
<li><em>Disadvantage</em>: Ignores true negatives. For many classification problems, sensitivity may be misleading specially under class imbalance. Since a better cutoff may be possible, an analysis of the ROC curve can lead to improvements in these metrics. Consequently, performance metrics that are independent of probability cutoffs are likely to produce more meaningful contrasts between models.</li>
<li><em>Description</em>: Is the proportion of true positive predictions out of all actual positive instances in the dataset.</li>
<li><em>Example</em>: % of people <em>with</em> heart diseases were correctly identify by the model</li>
<li><em>Calculation</em>: TP / (TP + FN)</li>
<li><em>Notes</em>:
<ul>
<li><p>If the data set includes more events than nonevents, the sensitivity can be estimated with greater precision than the specificity and sensitivity shouls be use to choose between models.</p></li>
<li><p>When we want to make unconditional evaluations of the data: know for example what are the chances that … (e.g., If PPV = 0.75 this means that out of all the individuals who tested positive for Disease X, 75% of them actually have the disease, while the remaining 25% are false positives) we can use positive predicted value (PPV = Sensitivity × Prevalence / (Sensitivity × Prevalence) + ((1 − Specificity) × (1 − Prevalence))) IMPOTANT: Predictive values are not often used to characterize the model. There are several reasons why, most of which are related to prevalence. First, prevalence is hard to quantify.</p></li>
</ul></li>
</ul>
<ol start="4" type="1">
<li><strong>Specificity / True Negative Rate</strong>:</li>
</ol>
<ul>
<li><em>Higher Value</em>: Better performance</li>
<li><em>When to use</em>: Use when you want to focus on correctly identifying negative cases and the cost of false positives is high.</li>
<li><em>Advantage</em>: Focuses on the negative class and avoids false positives. Can be misleading specially under class imbalance.</li>
<li><em>Disadvantage</em>: Ignores true positives.</li>
<li><em>Description</em>: Specificity measures the proportion of true negative predictions out of all actual negative samples.</li>
<li><em>Example</em>: % of people <em>without</em> heart diseases were correctly identify by the model</li>
<li><em>Calculation</em>: True Negatives / (True Negatives + False Positives);</li>
<li><em>Notes</em>:
<ul>
<li><p>When we want to make unconditional evaluations of the data: know for example what are the chances that … (If NPV = 0.966, this means that out of all the individuals who tested negative for Disease X, 96.6% of them truly do not have the disease, while the remaining 3.4% are false negatives (individuals who have the disease but were incorrectly identified as negative).) we can use negative predicted value (NPV = Specificity × (1 − Prevalence) / (Prevalence × (1 − Sensitivity)) + (Specificity × (1 − Prevalence))). IMPORTANT: <em>idem</em></p></li>
<li><p><em>False-positive rate</em> : one minus the specificity</p></li>
</ul></li>
</ul>
<p>4.3) <strong>Youden’s J Index</strong></p>
<ul>
<li><em>Higher Value</em>: Better performance</li>
<li><em>When to use</em>: Use when you want a measure that reflects the false-positive and false-negative rates and summarize the magnitude of both types of errors.</li>
<li><em>Advantage</em>: Focuses on the negative class and avoids false positives.</li>
<li><em>Disadvantage</em>: Ignores true positives and may not be suitable for imbalanced datasets.</li>
<li><em>Description</em>: measures the proportions of correctly predicted samples for both the event and nonevent groups.</li>
<li><em>Example</em>: % of people <em>without</em> heart diseases were correctly identify by the model</li>
<li><em>Calculation</em>: J = Sensitivity + Specificity − 1</li>
</ul>
<ol start="5" type="1">
<li><strong>F1 Score</strong>:</li>
</ol>
<ul>
<li><em>Higher Value</em>: Better performance</li>
<li><em>When to use</em>: Use when there is a trade-off between precision and recall.</li>
<li><em>Advantage</em>: Incorporates both precision and recall into a single metric.</li>
<li><em>Disadvantage</em>: Ignores true negatives, which can be important in some cases. May not be ideal for highly imbalanced datasets.</li>
<li><em>Description</em>: F1 score is the harmonic mean of precision and recall, providing a balance between the two.</li>
<li><em>Example</em>: Let’s say your model has an F1 Score of 0.75, it means there is a balanced trade-off between correctly identifying positive samples and minimizing false positives.</li>
<li><em>Calculation</em>: 2 * (Precision * Recall) / (Precision + Recall)</li>
</ul>
</section>
<section id="class-probabilities" class="level4" data-number="4.3.1.2">
<h4 data-number="4.3.1.2" class="anchored" data-anchor-id="class-probabilities"><span class="header-section-number">4.3.1.2</span> Class probabilities</h4>
<p>Class probabilities potentially offer more information about model predictions than the simple class value. This</p>
<p>4.7) <strong>ROC</strong>:</p>
<ul>
<li><em>Higher Value</em>: Better performance (A perfect model that completely separates the two classes would have 100 % sensitivity and specificity / A completely ineffective model would result in an ROC curve that closely follows the 45◦ diagonal line and would have an area under the ROC curve of approximately 0.50.) Area under the curve can be used as a quantitative measure of performance</li>
<li><em>When to use</em>: Helpful tool for choosing a threshold that appropriately maximizes the trade-off between sensitivity and specificity (e.g., Lowering the threshold (aka 50%) can we improve the sensitivity to capture more true positives). Make a quantitative assessment of the model</li>
<li><em>Advantage</em>: the curve is insensitive to disparities in the class proportions. Metrics that is independent of probability cutoffs</li>
<li><em>Disadvantage</em>: disadvantage of using the area under the curve to evaluate models is that it obscures information (i.e., the curves cross both AUC can be the same).</li>
<li><em>Description</em>: AUC-ROC measures the area under the receiver operating characteristic curve, which plots the true positive rate (recall) against the false positive rate at various classification thresholds (10%, 20%… 50% = commonly used).</li>
<li><em>Example</em>: An AUC-ROC score of 0.85 indicates that the model has an 85% chance of correctly ranking a randomly chosen positive instance higher than a randomly chosen negative instance.</li>
<li><em>Calculation</em>: AUC-ROC can be calculated using various methods, such as the trapezoidal rule or Mann-Whitney U statistic.</li>
<li><em>Notes</em>:
<ul>
<li>We can use the partial area under the ROC curve as a technique to summarize these curves that focuses on specific parts of the curve.</li>
<li>ROC technique can be extended to fit three or more classes problems</li>
</ul></li>
</ul>
<ol start="6" type="1">
<li><strong>Lift Charts</strong>:</li>
</ol>
<ul>
<li><em>Higher Value</em>: Better performance (Figure)</li>
<li><em>When to use</em>: To assess the ability of a model to detect events in a data set with two classes and allow us to choose a quasithreshold for a model.</li>
<li><em>Advantage</em>: Easy connect the model to the buisness value: Using the lift plot, the expected profit can be calculated for each point on the curve to determine if the lift is sufficient to beat the baseline profit</li>
<li><em>Disadvantage</em>: Bad for comparing different models</li>
<li><em>Description</em>: The lift chart plots the cumulative gain/lift against the cumulative percentage of samples that have been screened</li>
<li><em>Example</em>: Figure shows the best and worse case lift curves for a data set with a 50 % event rate. The non-informative model has a curve that is close to the 45◦ reference line, meaning that the model has no benefit for ranking samples. The other curve is indicative of a model that can perfectly separate two classes. At the 50 % point on the x-axis, all of the events have been captured by the model.</li>
<li><em>Calculation</em>: <em>NA</em></li>
<li><em>Notes</em>:
<ul>
<li>The section of the curve associated with the highest-ranked samples should have an enriched true-positive rate and is likely to be the most important part of the curve.</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/Lift Charts.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Lift Charts</figcaption>
</figure>
</div>
<p><strong>NOTES</strong>:</p>
<ul>
<li>It is important to test whether the estimated class probabilities are reflective of the true underlying probability of the sample (well-calibrated Probabilities) using a <em>calibration plot</em>. This plot shows some measure of the observed probability of an event versus the predicted class probability. One approach for creating this visualization is to score a collection of samples with known outcomes (preferably a test set) using a classification model. The next step is to bin the data into groups based on their class probabilities. For example, a set of bins might be [0, 10 %], (10 %, 20 %], …, (90 %, 100 %]. For each bin, determine the observed event rate. Suppose that 50 samples fell into the bin for class probabilities less than 10 % and there was a single event. The midpoint of the bin is 5 % and the observed event rate would be 2 %. The calibration plot would display the midpoint of the bin on the x-axis and the observed event rate on the y-axis. If the points fall along a 45◦ line, the model has produced well-calibrated probabilities.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/calibration plot.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">A calibration plot of the test set probabilities for random forest and quadratic discriminant analysis models</figcaption>
</figure>
</div>
<ul>
<li><p>If there are three or more classes, a heat map of the class probabilities can help gauge the confidence in the predictions.</p></li>
<li><p>An approach to improving classification performance is to create an <em>equivocal</em> or <em>indeterminate zone</em> where the class is not formally predicted when the confidence is not high. (e.g., For a two-class problem that is nearly balanced in the response, the equivocal zone could be defined as 0.50 ± z.Ifz were 0.10, then samples with prediction probabilities between 0.40 and 0.60 would be called “equivocal.” In this case, model performance would be calculated excluding the samples in the indeterminate zone.)</p></li>
</ul>
</section>
<section id="non-accuracy-based-criteria" class="level4" data-number="4.3.1.3">
<h4 data-number="4.3.1.3" class="anchored" data-anchor-id="non-accuracy-based-criteria"><span class="header-section-number">4.3.1.3</span> Non-Accuracy-Based Criteria</h4>
<p>When accuracy is not the primary goal for the predictive model and we want to quantify the consequences of correct and incorrect predictions (i.e., the benefits and costs)</p>
<p><em>Examples</em>:</p>
<ul>
<li>Predict investment opportunities that maximize return</li>
<li>Improve customer satisfaction by market segmentation</li>
<li>Lower inventory costs by improving product demand forecasts</li>
<li>Reduce costs associated with fraudulent transactions: For example, in fraud detection, a model might be used to quantify the likelihood that a transaction is fraudulent. Suppose that fraud is the event of interest. Any model predictions of fraud (correct or not) have an associated cost for a more in-depth review of the case. For true positives, there is also a quantifiable benefit to catching bad transactions. Likewise, a false negative results in a loss of income.</li>
</ul>
<ol type="1">
<li><p><strong>profit</strong> = <em>Cost/Benefit</em> * TP − <em>Cost/Benefit</em> FP − <em>Cost/Benefit</em> FN</p></li>
<li><p><strong>NEC</strong> (normalized expected cost / classification_cost_penalized) = PCF × (1 − TP)+(1− PCF) × FP (between 0 and 1)</p></li>
</ol>
</section>
</section>
<section id="for-models-predicting-a-numeric-outcome" class="level3" data-number="4.3.2">
<h3 data-number="4.3.2" class="anchored" data-anchor-id="for-models-predicting-a-numeric-outcome"><span class="header-section-number">4.3.2</span> For models predicting a numeric outcome</h3>
<ol type="1">
<li><strong>RMSE</strong>:</li>
</ol>
<ul>
<li><em>Higher Value</em>: Worse performance</li>
<li><em>When to use</em>: Commonly used to measure the average magnitude of prediction errors.</li>
<li><em>Advantage</em>: Penalizes larger errors more heavily, sensitive to outliers and unit is the same as the target variable, making it more interpretable.</li>
<li><em>Disadvantage</em>: Sensitive to outliers.</li>
<li><em>Description</em>: The average distance between the observed values and the model predictions.</li>
<li><em>Example</em>: Continuing with the house price prediction example, an RMSE of 100 means that, on average, the predicted house prices deviate from the actual prices by $100.</li>
<li><em>Calculation</em>: Squared root of the (sum the residuals (the observed values minus the model predictions) and dividing by the number of samples) For example, if we have actual values [5, 10, 15] and predicted values [6, 12, 10], the MSE would be calculated as ((1^2) + (2^2) + (5^2)) / 3 = 10.</li>
</ul>
<ol start="2" type="1">
<li><strong>MAE</strong>:</li>
</ol>
<ul>
<li><em>Higher Value</em>: Worse performance</li>
<li><em>When to use</em>: Suitable when you want to avoid the influence of outliers.</li>
<li><em>Advantage</em>: Not sensitive to outliers as it uses the absolute error.</li>
<li><em>Disadvantage</em>: It does not penalize large errors as heavily as RMSE.</li>
<li><em>Description</em>: The Mean Absolute Error measures the average of the absolute differences between predicted and actual values.</li>
<li><em>Example</em>: For the house price prediction, an MAE of $50 means that, on average, the predicted house prices deviate from the actual prices by $50.</li>
<li><em>Calculation</em>: For example, with the same actual and predicted values, the MAE would be calculated as (|1| + |2| + |5|) / 3 = 2.67.</li>
</ul>
<ol start="3" type="1">
<li><strong>R2</strong>:</li>
</ol>
<ul>
<li><em>Higher Value</em>: Better performance</li>
<li><em>When to use</em>: Commonly used to measure the average magnitude of prediction errors. It is a measure of correlation, not accuracy. <strong>Bad</strong> for predicting a <strong>number</strong> (accuracy) but <strong>good</strong> for determining the <strong>rank</strong> correlation between the observed and predicted values (e.g., pharmaceutical scientists want to find the compounds predicted to be the most biologically active).</li>
<li><em>Advantage</em>: Represents the proportion of the variance in the dependent variable that is predictable from the independent variables.</li>
<li><em>Disadvantage</em>: It can be misleading when used with complex models or when the number of predictors is large. It is dependent on the variation in the outcome (e.g., If the range of the houses in the test set was large, say from $60K to $2M, the variance of the sale price would also be very large. One might view a model with a 90 % R2 positively, but the RMSE may be in the tens of thousands of dollars—poor predictive accuracy for anyone selling a moderately priced property)</li>
<li><em>Description</em>: The proportion of the information in the data that is explained by the model</li>
<li><em>Example</em>: An R-squared of 0.75 means that 75% of the variance in the house prices can be explained by the model, and the remaining 25% is due to random variation.</li>
<li><em>Calculation</em>: Correlation coefficient between the observed and predicted values</li>
<li><em>Note</em>: By plotting R2 we can see where the model is overpredict (e.g., low values) and underpredict (e.g., higher values). If this happend depending on the context, this systematic bias in the predictions may be acceptable if the model otherwise works well.</li>
</ul>
<ol start="4" type="1">
<li><strong>R2 adjusted</strong>:</li>
</ol>
<ul>
<li><em>Higher Value</em>: Better performance</li>
<li><em>When to use</em>: Helpful when you have multiple predictors and want to account for model complexity.</li>
<li><em>Advantage</em>: It adjusts R-squared for the number of predictors, giving a more reliable assessment of model performance when compared to R-squared.</li>
<li><em>Disadvantage</em>: It might not penalize overfitting adequately with large numbers of predictors.</li>
<li><em>Description</em>: R-squared adjusted is similar to R-squared but takes into account the number of predictors in the model. It penalizes models with more predictors if they don’t contribute significantly to the variance explained.</li>
<li><em>Example</em>:</li>
<li><em>Calculation</em>:</li>
</ul>
<ol start="5" type="1">
<li><strong>MAPE</strong>:</li>
</ol>
<ul>
<li><em>Higher Value</em>: Worse performance</li>
<li><em>When to use</em>: Useful when you want to evaluate the performance in percentage terms.</li>
<li><em>Advantage</em>: Represents the percentage difference between predicted and actual values, making it interpretable and independent of the scale of the data.</li>
<li><em>Disadvantage</em>: It can be problematic when actual values are close to zero.</li>
<li><em>Description</em>: The Mean Absolute Percentage Error calculates the mean percentage difference between predicted and actual values.</li>
<li><em>Example</em>: An MAPE of 10 means that, on average, the predicted house prices deviate from the actual prices by 10%.</li>
<li><em>Calculation</em>: For example, if we have actual values [100, 50, 75] and predicted values [90, 40, 70], the MAPE would be calculated as (|(100-90)/100| + |(50-40)/50| + |(75-70)/75|) / 3 ≈ 0.16.</li>
</ul>
<ol start="6" type="1">
<li><strong>EV</strong>:</li>
</ol>
<ul>
<li><em>Higher Value</em>: Better performance (good value &gt; 0.6)</li>
<li><em>When to use</em>: Useful to understand how well the model explains the variance in the target variable.</li>
<li><em>Advantage</em>: Measures the proportion of variance explained by the model, similar to R-squared.</li>
<li><em>Disadvantage</em>: It might not penalize the model adequately for underfitting or overfitting.</li>
<li><em>Description</em>: The Explained Variance Score quantifies the proportion of variance in the target variable that is explained by the model. It ranges from 0 to 1, with 1 indicating a perfect fit. For example, an EV of 0.85 means that 85% of the variance is explained by the model.</li>
<li><em>Example</em>: An EV of 0.9 means that the model explains 90% of the variance in the house prices, leaving 10% unexplained by the model.</li>
<li><em>Calculation</em>:</li>
</ul>
<ol start="7" type="1">
<li><strong>MSLE</strong>:</li>
</ol>
<ul>
<li><em>Higher Value</em>: Worse performance</li>
<li><em>When to use</em>: Suitable when you want to focus on the ratio of errors rather than their absolute differences. It can be useful when predictions are on a large scale.</li>
<li><em>Advantage</em>: Penalizes underestimation and overestimation proportionally and is less sensitive to large errors.</li>
<li><em>Disadvantage</em>: The logarithmic transformation can be problematic for data containing zero or negative values.</li>
<li><em>Description</em>: The Mean Squared Logarithmic Error calculates the mean of the squared logarithmic differences between predicted and actual values.</li>
<li><em>Example</em>: For the house price prediction, an MSLE of 0.1 means that, on average, the predicted house prices deviate from the actual prices by 10% when measured on a logarithmic scale.</li>
<li><em>Calculation</em>: For instance, if we have actual values [100, 50, 75] and predicted values [110, 40, 80], the MSLE would be calculated as ((log(110) - log(100))^2 + (log(40) - log(50))^2 + (log(80) - log(75))^2) / 3 ≈ 0.015.</li>
</ul>
</section>
</section>
<section id="model-choosing" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Model Choosing</h1>
<p>Once the settings for the tuning parameters have been determined for each model, the question remains: how do we choose between multiple models?</p>
<ol type="1">
<li>Start with several models that are the least interpretable and most flexible, such as boosted trees or support vector machines. Across many problem domains, these models have a high likelihood of producing the empirically optimum results (i.e., most accurate).</li>
<li>Investigate simpler models that are less opaque (e.g., not complete black boxes), such as multivariate adaptive regression splines (MARS), partial least squares, generalized additive models, or näıve Bayes models.</li>
<li>Consider using the simplest model that reasonably approximates the performance of the more complex methods.</li>
</ol>
<p><strong>NOTE</strong>: A paired t-test can be used to evaluate if the differences between models are statistically significant. It is also recommended to plot confidence intervals that were derived using the bootstrap (Figure) fot two reasons.</p>
<ul>
<li>The interval quantifies the variation in the model but is also reflective of the data. For example, smaller test sets or noise (or mislabeling) in the response can lead to wider intervals.</li>
<li>Facilitate trade-offs between models. If the confidence intervals for two models significantly overlap, this is an indication of (statistical) equivalence between the two and might provide a reason to favor the less complex or more interpretable model.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/Confidence intervals.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">A plot of the test set ROC curve AUCs and their associated 95 % confidence intervals</figcaption>
</figure>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./Pre-processing.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Pre-processing</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Linear Models and Its Cousins.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Linear Regression and Its Cousins</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>