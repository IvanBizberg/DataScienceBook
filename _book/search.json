[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DS Baseline",
    "section": "",
    "text": "Preface\nWelcome to the world of data science, a captivating realm where art, mathematics, and technology converge to unlock the hidden insights lying dormant within vast troves of information. In this book, we embark on an exhilarating journey through the most important aspects of data science techniques, exploring their foundations, applications, and transformative potential.\nThe rapid advancement of technology and the explosive growth of data have ushered in an era of unprecedented opportunities. Every click, every transaction, and every interaction generates a data footprint that can be harnessed to drive innovation, solve complex problems, and shape the future. Data science equips us with the tools to make sense of this digital universe, enabling us to extract meaning from seemingly chaotic data and turn it into actionable knowledge.\nAt its core, data science is a multidisciplinary field that draws from various domains, including statistics, mathematics, computer science, and domain expertise. It encompasses a wide range of techniques, algorithms, and methodologies designed to collect, analyze, interpret, and visualize data to uncover patterns, make predictions, and generate insights.\nThis book is intended to serve as a comprehensive compilation of the most important aspects of data science techniques. However, it is important to note that the content presented herein reflects the author’s personal understanding and interpretation of these concepts which may occasionally deviate from the precise and mathematical definitions of certain data science principles."
  },
  {
    "objectID": "Terms & definitions.html",
    "href": "Terms & definitions.html",
    "title": "1  Terms & definitions",
    "section": "",
    "text": "2 Greedy algorithms\nWhen a procedure is greedy, it means that it does not reevaluate past solutions.\nExtrapolation is commonly defined as using a model to predict samples that are outside the range of the training data. To know if we can trust our model we need to compare the predictor space between the training data and new data."
  },
  {
    "objectID": "Terms & definitions.html#correlation-vs-covariance",
    "href": "Terms & definitions.html#correlation-vs-covariance",
    "title": "1  Terms & definitions",
    "section": "1.1 Correlation vs covariance",
    "text": "1.1 Correlation vs covariance\n\nCovariance measures the direction and magnitude of the linear relationship between two variables. It calculates how changes in one variable are related to changes in another variable. Covariance can take any value, positive or negative, depending on the nature of the relationship.\n\nWhen to use it:\n\nScaling and Interpretation: If you are primarily interested in the magnitude of the relationship between two variables, without the need for standardized interpretation, covariance can be used. Since covariance is not standardized, it preserves the original scale of the variables. This can be helpful when the units of measurement carry important information or when you want to maintain the original context of the data.\nNon-linear Relationships: If you suspect a non-linear relationship between variables, covariance can still provide insights into the direction and magnitude of the relationship, albeit without quantifying the strength in a standardized manner.\n\nExample:\nBy calculating the covariance between height and weight, you can obtain a measure of how the two variables vary together.\n\nCorrelation measures the strength and direction of the linear relationship between two variables, but it standardizes the measure to fall between -1 and 1.\n\nWhen to use it: Correlation is a more useful measure than covariance when comparing relationships across different datasets or variables, as it removes the influence of the scales of the variables.\nExample:\nIf you were interested in comparing the relationship between height and weight with other datasets or variables, or if you wanted a standardized measure of the strength of the relationship, then calculating the correlation coefficient would be more appropriate. The correlation coefficient would provide a standardized measure between -1 and 1, allowing for easier comparison and interpretation across different contexts."
  },
  {
    "objectID": "Terms & definitions.html#correlation-vs-cointegration",
    "href": "Terms & definitions.html#correlation-vs-cointegration",
    "title": "1  Terms & definitions",
    "section": "1.2 Correlation vs cointegration",
    "text": "1.2 Correlation vs cointegration\nBoth are statistical concepts used to measure the relationship between variables.\n\nCointegration measures whether the variables tend to move together over time, despite possibly having short-term fluctuations.\n\nExample: A drunk man leaves the pub with his dog.\nWhen the man and the dog first leave the pub, their paths are correlated. They generally move in the same direction, but the distance between the dog and the man has no actual limit. It increases at times, decreases at times, but is generally random and poorly defined. The direction of the two, however, is generally the same.\nWhen the man leashes his dog to cross the road, they become cointegrated. Now, while their direction is still the same, their distance from one another is finite. The dog cannot move beyond the length of the leash from the man. (“What Is the Difference Between Correlation and Cointegration? Is Cointegration a Good Measure of Risk?” n.d.)"
  },
  {
    "objectID": "Terms & definitions.html#p-value",
    "href": "Terms & definitions.html#p-value",
    "title": "1  Terms & definitions",
    "section": "1.3 P-value",
    "text": "1.3 P-value\nA p-value of 0.001 indicates that if the null hypothesis tested were indeed true, then there would be a one-in-1,000 chance of observing results at least as extreme."
  },
  {
    "objectID": "Terms & definitions.html#bias-and-variance",
    "href": "Terms & definitions.html#bias-and-variance",
    "title": "1  Terms & definitions",
    "section": "1.4 Bias and Variance",
    "text": "1.4 Bias and Variance\nWe search a model with low bias and low variance\nBias: The inability for a machine learning method to capture the true relationship is called bias Variance: The difference in fits between train set and test set\nRed model: low variance: This model has low variance if a new point it would not substantially change the model fit (leads to under-fitting) high bias: Ineffective at modeling the data\nBlue model: high variance: Small perturbations in the data will significantly change the model fit (leads to over-fitting) low bias: More complex and flexible allowing to model very good the data\n\n\n\nvariance-bias trade-off"
  },
  {
    "objectID": "Terms & definitions.html#entropy",
    "href": "Terms & definitions.html#entropy",
    "title": "1  Terms & definitions",
    "section": "1.5 Entropy",
    "text": "1.5 Entropy"
  },
  {
    "objectID": "Terms & definitions.html#regularization",
    "href": "Terms & definitions.html#regularization",
    "title": "1  Terms & definitions",
    "section": "1.7 Regularization",
    "text": "1.7 Regularization\nIt make the prediction less sensitive to the training data."
  },
  {
    "objectID": "Terms & definitions.html#quantiles",
    "href": "Terms & definitions.html#quantiles",
    "title": "1  Terms & definitions",
    "section": "1.8 Quantiles",
    "text": "1.8 Quantiles\nMedian: The 50th percentile, which divides the data into two equal halves. Half of the data points are below the median, and half are above it.\nQuartiles: The 25th, 50th, and 75th percentiles are called the first quartile (Q1), the median (Q2), and the third quartile (Q3), respectively. They divide the data into four equal parts, each containing 25% of the data.\nPercentiles: These are any values that divide the data into 100 equal parts. For example, the 20th percentile is the value below which 20% of the data falls, and the 80th percentile is the value below which 80% of the data falls."
  },
  {
    "objectID": "Choose model.html",
    "href": "Choose model.html",
    "title": "2  Choose machine learning model",
    "section": "",
    "text": "Model\n      Mode\n      Allows n &lt; p\n      Pre-processing\n      Interpretable\n      Automatic feature selection\n      Number of tuning parameters\n      Robust to predictor noise\n      Computation time\n      Details\n    \n  \n  \n    Linear regression\nregression\nno\ncentering and scaling, remove near-zero predictors, remove highly correlated predictors, remove non-informative predictors\nhigh\nno\n0\nlow\nfast\n\n    Partial least squares\nregression\ndepends\ncentering and scaling, remove non-informative predictors\nlow\nyes*\n1\nNA\nfast\n\n    Logistic regression\nclassification\nno\ncentering and scaling, remove near-zero predictors, remove highly correlated predictors, remove non-informative predictors\nhigh\nno\n0\nlow\nfast\n\n    Ridge regression\nregression & classification\nno\ncentering and scaling, remove near-zero predictors, remove non-informative predictors\nhigh\nno\n1\nlow\nfast\n\n    Elastic net/lasso\nregression & classification\nyes\ncentering and scaling, remove near-zero predictors\nhigh\nyes\n1-2\nlow\nfast\n\n    Support vector machines\nregression & classification\nyes\ncentering and scaling, remove non-informative predictors\nlow\nno\n1-3\nlow\nslow\n\n    MARS/FDA\nregression & classification\nyes\n\nmedium\nyes\n1-2\nmid\nmid\n\n    K-nearest neighbors\nregression & classification\nyes\ncentering and scaling, remove near-zero predictors\nlow\nno\n1\nmid\nfast\n\n    Random forest\nregression & classification\nyes\n\nlow\ndepends\n0-1\nhigh\nslow\n\n    Boosted trees\nregression & classification\nyes\n\nlow\nyes\n3\nhigh\nslow\n\n    Neural networks\nregression & classification\nyes\ncentering and scaling, remove near-zero predictors, remove highly correlated predictors, remove non-informative predictors\nlow\nno\n2\nlow\nslow\n\n    LDA\nclassification\nno\nremove near-zero predictors\nmedium\nno\n0-2\nlow\nfast\n\n    Naive Bayes\nclassification\nyes\nremove near-zero predictors\nlow\nno\n0-1\nmid\nmid\n\n    C5.0\nclassification\nyes\n\nmedium\nyes\n0-3\nhigh\nlow\n\n  \n  \n  \n\n\n\n\n(Kuhn and Johnson 2013)\n Model processing 1) Exploratory data analysis (evaluating simple summary measures or identifying predictors that have strong correlations with the outcome / how the predictors will be represented)\nModel building 1) Pre-processing the predictor data 2) Estimating model parameters 3) Selecting predictors for the model 4) Evaluating model performance 5) Fine tuning class prediction rules (via ROC curves, etc.)\n\noptimization routines (e.g., Nelder–Mead simplex method = direct methods) can later be use to search the optimal key value (e.g., determine possible mixtures with improved compressive strength)\nEDA can be conducted on the model results (e.g., residual analysis)\n\n\n\n\n\nKuhn, Max, and Kjell Johnson. 2013. Applied Predictive Modeling. New York, NY: Springer. https://doi.org/10.1007/978-1-4614-6849-3."
  },
  {
    "objectID": "Pre-processing.html#transform-the-predictorindependent-variables",
    "href": "Pre-processing.html#transform-the-predictorindependent-variables",
    "title": "3  Pre-processing",
    "section": "3.1 Transform the predictor/independent variables",
    "text": "3.1 Transform the predictor/independent variables\nImportant to transform independent variables that are skewn or containt outliers for models sensitive to them\n\n3.1.1 Centering and scaling\nAdvantages: Improve the numerical stability to minimize potential numerical errors Disadvantage: Loss of interpretability\n\n\n3.1.2 Resolve Skewness\nSkewed data: Ratio of the highest value to the lowest value is greater than 20 have significant skewness.\n\nlog\nsquare root\ninverse\nBox and Cox (can only be applied to data that is strictly positive)\n\n\n\n3.1.3 Resolve Outliers\n\nSpatial sign\n\nNOTES:\n\nit is important to center and scale the predictor data prior to using this transformation\nspatial sign transformation of the predictors transforms them as a group. Removing predictor variables after applying this technique may be problematic.\n\n\n\n3.1.4 Data Reduction and Signal/Feature Extraction\nThese methods reduce the data by generating a smaller set of predictors that seek to capture a majority of the information in the original variables.\n\n3.1.4.1 PCA (unsupervised technique)\nThe number of components to retain is choosen by creating a scree plot (Fig 1)\nFor most data sets, the first few PCs will summarize a majority of the variability, and the plot will show a steep descent; variation will then taper off for the remaining components. Generally, the component number prior to the tapering off of variation is the maximal component that is retained. In an automated model building process, the optimal number of components can be determined by cross-validation (see Resampling Techniques).\n\n\n\nFigure 1: The variation tapers off at component 5. Using this rule of thumb, four PCs would be retained\n\n\nAdvantages: The primary advantage of PCA, and the reason that it has retained its popularity as a data reduction method, is that it creates components that are uncorrelated.\nDisadvantage:\n\nLoss of interpretability. PCA can generate components that summarize characteristics of the data that are irrelevant to the underlying structure of the data and also to the ultimate modeling objective.\nUnsupervised technique which means that PCA it does not consider the modeling objective or response variable when summarizing variability. If the predictive relationship between the predictors and response is not connected to the predictors’ variability, then the derived PCs will not provide a suitable relationship with the response. In this case, a supervised technique, like PLS, will derive components while simultaneously considering the corresponding response.\n\nNOTES:\n\nfirst transform skewed predictors and then center and scale the predictors prior to performing PCA. Centering and scaling enables PCA to find the underlying relationships in the data without being influenced by the original measurement scales.\nIf PCA has captured a sufficient amount of information in the data. Visually examining the principal components is a critical step for assessing data quality and gaining intuition for the problem. To do this, the first few principal components can be plotted against each other and the plot symbols can be colored by relevant characteristics, such as the class labels.\n\nCheck for blatant outliers that may prompt a closer examination of the individual data points\nCheck for clusters of samples (for classification problems; Try other models that could better accommodate the data to have a final conclusion)\nChecks loadings to characterize which predictors are associated with each component (Loadings close to zero indicate that the predictor variable did not contribute much to that component; Fig 2)\nCheck for multicollinearity (substantial correlation between multiple predictors): For example, if the first principal component accounts for a large percentage of the variance, this implies that there is at least one group of predictors that represent the same information. For example, Fig 1 indicates that the first 3–4 components have relative contributions to the total variance. This would indicate that there are at least 3–4 significant relationships between the predictors. Colinearity can increase the model variance\n\nIf the percentages of variation explained are not large (e.g., less than 48 %) for the first three components, it is important not to over-interpret the resulting image.\n\n\n\n\nFigure 2: loadings for the first three components in the cell segmentation data. For the first principal component, the loadings for the first channel are on the extremes. This indicates that channel 1 have the largest effect on the first principal component and by extension the predictor values. Also note that the majority of the loadings for the third channel are closer to zero for the first component. Conversely, the third principal component is mostly associated with the third channel while the first channel plays a minor role here.\n\n\n\n\n3.1.4.2 PLS (supervised technique)\nDerive components while simultaneously considering the corresponding response.\n\n\n\n3.1.5 Dealing with Missing Values\nBefore proceeding: It is important to understand why the values are missing to check for informative missing using visualization such as heatmap or cooccurrence plot (for smaller data) or by plotting the first two scores from a PCA model of the missing data indicator matrix (for larger data sets). Informative missingness can induce significant bias in the model.\nExamples: - people are more compelled to rate products when they have strong opinions (good or bad) - The tested drug was extremely ineffective or had significant side effects. The patient may be likely to miss doctor visits or to drop out of the study.\n\n1) Remove predictors from models if the percentage of missing data is substantial\n2) For large data sets, removal of samples based on missing values is not a problem\n3 If we do not remove the missing data:\n\nUse tree-based techniques, which account for missing data\ncode mussing data as “missing”\nimpute missing data using information in the training set predictors (for small to moderate amounts of missingness). This amounts to a predictive model within a predictive model. If we are using resampling to select tuning parameter values or to estimate performance, the imputation should be incorporated within the resampling. This will increase the computational time for building models, but it will also provide honest estimates of model performance.\n\nK-nearest neighbor model: Advantages: The imputed data are confined to be within the range of the training set values. Disadvantages: The entire training set is required every time a missing value needs to be imputed and The number of neighbors is a tuning parameter\nLinear regression model: between a predictor with few missing points strongly associated with the predictor with missing data (correlation / visualizations / PCA.\nBagged trees\n\n\n\nNOTES Censored data ≠ Missing data: The exact value is missing but something is known about its value. For example: If a customer has not yet returned a movie to blockbuster, we do not know the actual time span, only that it is as least as long as the current duration.\n\nFor inference models: the censoring is usually taken into account in a formal manner by making assumptions about the censoring mechanism\nFor predictive models, it is more common to treat these data as simple missing data or use the censored value as the observed value.\n\n\n\n3.1.6 Removing Predictors\nAdvantages: Does not compromise the performance and stability of the model. Decreased computational time and complexity. Lead to a more parsimonious and interpretable model\n\nRemove near-zero predictors (e.g., predictor variable where the percentage of unique values is low &lt; 10% = unique values/total values and The ratio of the frequency of the most prevalent value to the frequency of the second most prevalent value is large &gt; 20).\nRemove problematic predictors with degenerate distributions as some models can be crippled by them\nRemove highly correlated predictors as both mesure the same underlaying information. For linear regressions use VIF for other models ensure that all pairwise correlations are below 0.75 threshold for sensitive models\nFeature Selection (see Feature Selection section)\n\n\n\n3.1.7 Adding Predictors (see Feature engineering)"
  },
  {
    "objectID": "Model Tuning.html",
    "href": "Model Tuning.html",
    "title": "4  Model Tuning",
    "section": "",
    "text": "5 Model Choosing\nOnce the settings for the tuning parameters have been determined for each model, the question remains: how do we choose between multiple models?\nNOTE: A paired t-test can be used to evaluate if the differences between models are statistically significant. It is also recommended to plot confidence intervals that were derived using the bootstrap (Figure) fot two reasons."
  },
  {
    "objectID": "Model Tuning.html#data-splitting-method",
    "href": "Model Tuning.html#data-splitting-method",
    "title": "4  Model Tuning",
    "section": "4.1 Data splitting method",
    "text": "4.1 Data splitting method\nA good rule of thumb is about 75–80 % on train subset and the rest for the test subset. Proportionally large test sets divide the data in a way that increases bias in the performance estimates.\n\n4.1.1 Nonrandom approaches to splitting the data\nExample: - If a model was being used to predict patient outcomes, the model may be created using certain patient sets (e.g., from the same clinical site or disease stage), and then tested on a different sample population to understand how well the model generalizes.\n\nIn chemical modeling for drug discovery, new “chemical space” is constantly being explored. We are most interested in accurate predictions in the chemical space that is currently being investigated rather than the space that was evaluated years prior.\nIn spam filtering; it is more important for the model to catch the new spamming techniques rather than prior spamming schemes.\n\n\n\n4.1.2 Random sampling methods\n\n4.1.2.1 Simple random sample\n\nThe simplest way to split the data randomly into a training and test.\nDisadvantage: limited ability to characterize the uncertainty in the results.\nsimple k-Fold Cross-Validation: The samples are randomly partitioned into k sets of roughly equal size. A model is fit using the all samples except one subset. The held-out samples are predicted by this model and used to estimate performance measures. The first subset is returned to the training set an procedure repeats with the next subset held out, and so on. Performance estimates, are calculated from each set of held-out samples and then averaged.\nNOTES: The choice of k is usually 5 or 10, but there is no formal rule. The bias is smaller for k = 10 than k = 5. As k gets larger, the difference in size between the training set and the resampling subsets gets smaller. But larger values of k are more computationally burdensome.\nAdvantage: Low computational costs. . Disadvantage: k-fold cross-validation generally has high variance compared to other methods (only for small training sets). USE: If sample sizes are large (&gt; 10 000) and we want to choose tuning parameters\nrepeated k-Fold Cross-Validation\nAdvantage: Increase the precision of the estimates while still maintaining a small bias. The bias and variance properties are good and, given the sample size, the computational costs are not large. Disdvantage: Large computational costs. USE: with k = 10; If the samples size is small (&lt; 1000 obs) and we want to choose tuning parameters\n\n\n\n\nk-Fold Cross-Validation\n\n\n\nleave-one-out Cross-Validation / LOOCV: fits as many models as there are samples in the training set, should only be considered when the number of samples is very small.\nleave-group-out Cross-Validation / Repeated training/test splits / Monte Carlo cross-validation: Same as k-fold cross-validation except that samples can be represented in multiple held-out subsets. Also, the number of repetitions is usually larger than in k-fold cross-validation\nDisadvantage:\nNOTES: Increase the number of repetition can allow to increase the proportion of data in the train set and decreasing the uncertainty of the performance estimates. To get stable estimates of performance, it is suggested to choose a larger number of repetitions (say 50–200)\n\n\n\n\nleave-group-out Cross-Validation\n\n\n\nThe Bootstrap: Each train subset is the same size as the original and can contain multiple instances of the same data point (taken with replacement). Samples not selected by the bootstrap (“out-of-bag” samples) are predicted and used to estimate model performance\nAdvantage: error rates have less uncertainty than k-fold cross-validation. Very low variance. Disadvantage: On average, 63.2 % of the data points the bootstrap sample are represented at least once, so this technique has bias. similar to k-fold cross-validation when k ≈ 2. If the training set size is small, this bias may be problematic, but will decrease as the training set sample size becomes larger. USE: If the goal is to choose between models (boosted trees vs support vector machines…), as opposed to getting the best indicator of performance\n\n\n\n\nBootstrap\n\n\n\nThe Bootstrap 632 method\nAdvantage: The modified bootstrap estimate reduces the bias.\nDisadvantage: The estimate is unstable with small samples sizes. This estimate can also result in unduly optimistic results when the model severely over-fits the data, since the apparent error rate will be close to zero.\nThe Bootstrap 632+ method Advantage: Allows to adjust the bootstrap 632 method estimates\n\n\n\n4.1.2.2 Stratified random\nTo account for the outcome when splitting the data. Applies random sampling within subgroups (such as the classes or is outcomes are numbers the numeric values are broken into similar groups (e.g., low, medium, and high)).\n\nk-Fold Cross-Validation\n\n\n\n4.1.2.3 Maximum dissimilarity sampling\nThe data is split on the basis of the predictor values."
  },
  {
    "objectID": "Model Tuning.html#choosing-tuning-parameters",
    "href": "Model Tuning.html#choosing-tuning-parameters",
    "title": "4  Model Tuning",
    "section": "4.2 Choosing Tuning Parameters",
    "text": "4.2 Choosing Tuning Parameters\n\nPick the settings associated with the numerically best performance estimates. Disadvantage: lead to models that are overly complicated\nPick simpler models that provide acceptable performance (relative to the numerically optimal settings)\n\nThe “one-standard error” method: pick the simplier model within a single standard error of the numerically best value. In table below we would pick cost value of 2.\nthe “percent decrease in performance” method: pick the simplier model that is within a certain tolerance of the numerically best value. (e.g., The percent decrease in performance could be quantified by (X − O)/O where X is the performance value and O is the numerically optimal value. For example, in Fig. 4.9, the best accuracy value across the profile was 75 %. If a 4 % loss in accuracy was acceptable as a trade-off for a simpler model, accuracy values greater than 71.2 % would be acceptable. For the profile in Fig. 4.9, a cost value of 1 would be chosen using this approach.)\n\n\n\n\n\nCross-validation accuracy"
  },
  {
    "objectID": "Model Tuning.html#metrics-performance-measures",
    "href": "Model Tuning.html#metrics-performance-measures",
    "title": "4  Model Tuning",
    "section": "4.3 Metrics / performance measures",
    "text": "4.3 Metrics / performance measures\n\n4.3.1 For models predicting a categorical outcome\n\n4.3.1.1 Accuracy based metrics\nA good model has generally a metric above 0.7 / 70%\n\nAccuracy:\n\n\nHigher Value: Better performance (values form 0 to 1)\nWhen to use: Use when the classes are balanced (e.g., Suppose the rate of this disorder1 in fetuses is approximately 1 in 800 or about one-tenth of one percent. A predictive model can achieve almost perfect accuracy by predicting all samples to be negative for Down syndrome.), and misclassification of different classes has similar consequences.\nAdvantage: Simple and easy to interpret.\nDisadvantage: Can be misleading when classes are imbalanced / make no distinction about the type of errors being made\nDescription: Accuracy measures the proportion of correct predictions out of all predictions made by the model.\nExample: Suppose you have a binary classification problem to identify whether an email is spam or not. If your model has an accuracy of 90%, it means it correctly classified 90% of the emails.\nCalculation: (True Positives + True Negatives) / (True Positives + True Negatives + False Positives + False Negatives) (Number of Correct Predictions) / (Total Number of Predictions)\nNotes:\n\nWhen evaluating the accuracy of a model, the baseline accuracy rate to beat would be the percentage which could be achieve by simply predicting all samples to the dominant category (e.g., In the data set, 70 % were rated as having good, accuracy rate to beat would be 70 % which is the no-information rate).\nerror rate: (Number of Incorrect Predictions) / (Total Number of Predictions)\n\n\n1.5) Kappa:\n\nHigher Value: Better performance (values from -1 to 1; 0.30 to 0.50 indicate reasonable agreement)\nWhen to use: Rather than calculate the overall accuracy and compare it to the no-information rate, Kappa can be used that take into account the class distributions of the training set samples.\nAdvantage: Takes into account the accuracy that would be generated simply by chance.\nDisadvantage: NA\nDescription: assess the agreement between two raters\nExample: 0 means there is no agreement between the observed and predicted classes, while a value of 1 indicates perfect concordance of the model prediction and the observed classes. Negative values indicate that the prediction is in the opposite direction of the truth, but large negative values seldom occur, if ever, when working with predictive models.\nCalculation: Kappa = O − E / 1−E: O is the observed accuracy and E is the expected accuracy based on the marginal totals of the confusion matrix.\nNote: The Kappa statistic can also be extended to evaluate concordance in problems with more than two classes. When there is a natural ordering to the classes (e.g., “low,”“medium,” and “high”), an alternate form of the statistic called weighted Kappa can be used to enact more substantial penalties on errors that are further away from the true result. For example, a “low” sample erroneously predicted as “high” would reduce the Kappa statistic more than an error were “low” was predicted to be “medium.” See (Agresti 2002)for more details.\n\n\nPrecision:\n\n\nHigher Value: Better performance (good = 0.7)\nWhen to use: Use when the cost of false positives is high (e.g., medical diagnosis, fraud detection).\nAdvantage: Focuses on the relevance of positive predictions.\nDisadvantage: Ignores true negatives and may not be suitable for imbalanced datasets.\nDescription: Precision is the proportion of true positive predictions (correctly predicted positive class) out of all positive predictions made by the model.\nExample: In the spam email example, if your model has a precision of 80%, it means that out of all the emails it predicted as spam, 80% of them were actually spam.\nCalculation: True Positives / (True Positives + False Positives)\n\n\nSensitivity / True positive rate / Recall:\n\n\nHigher Value: Better performance\nWhen to use: Use when the cost of false negatives is high (e.g., medical diagnosis, safety-critical applications).\nAdvantage: Focuses on the completeness of positive predictions (includes true positive and false negatives).\nDisadvantage: Ignores true negatives. For many classification problems, sensitivity may be misleading specially under class imbalance. Since a better cutoff may be possible, an analysis of the ROC curve can lead to improvements in these metrics. Consequently, performance metrics that are independent of probability cutoffs are likely to produce more meaningful contrasts between models.\nDescription: Is the proportion of true positive predictions out of all actual positive instances in the dataset.\nExample: % of people with heart diseases were correctly identify by the model\nCalculation: TP / (TP + FN)\nNotes:\n\nIf the data set includes more events than nonevents, the sensitivity can be estimated with greater precision than the specificity and sensitivity shouls be use to choose between models.\nWhen we want to make unconditional evaluations of the data: know for example what are the chances that … (e.g., If PPV = 0.75 this means that out of all the individuals who tested positive for Disease X, 75% of them actually have the disease, while the remaining 25% are false positives) we can use positive predicted value (PPV = Sensitivity × Prevalence / (Sensitivity × Prevalence) + ((1 − Specificity) × (1 − Prevalence))) IMPOTANT: Predictive values are not often used to characterize the model. There are several reasons why, most of which are related to prevalence. First, prevalence is hard to quantify.\n\n\n\nSpecificity / True Negative Rate:\n\n\nHigher Value: Better performance\nWhen to use: Use when you want to focus on correctly identifying negative cases and the cost of false positives is high.\nAdvantage: Focuses on the negative class and avoids false positives. Can be misleading specially under class imbalance.\nDisadvantage: Ignores true positives.\nDescription: Specificity measures the proportion of true negative predictions out of all actual negative samples.\nExample: % of people without heart diseases were correctly identify by the model\nCalculation: True Negatives / (True Negatives + False Positives);\nNotes:\n\nWhen we want to make unconditional evaluations of the data: know for example what are the chances that … (If NPV = 0.966, this means that out of all the individuals who tested negative for Disease X, 96.6% of them truly do not have the disease, while the remaining 3.4% are false negatives (individuals who have the disease but were incorrectly identified as negative).) we can use negative predicted value (NPV = Specificity × (1 − Prevalence) / (Prevalence × (1 − Sensitivity)) + (Specificity × (1 − Prevalence))). IMPORTANT: idem\nFalse-positive rate : one minus the specificity\n\n\n4.3) Youden’s J Index\n\nHigher Value: Better performance\nWhen to use: Use when you want a measure that reflects the false-positive and false-negative rates and summarize the magnitude of both types of errors.\nAdvantage: Focuses on the negative class and avoids false positives.\nDisadvantage: Ignores true positives and may not be suitable for imbalanced datasets.\nDescription: measures the proportions of correctly predicted samples for both the event and nonevent groups.\nExample: % of people without heart diseases were correctly identify by the model\nCalculation: J = Sensitivity + Specificity − 1\n\n\nF1 Score:\n\n\nHigher Value: Better performance\nWhen to use: Use when there is a trade-off between precision and recall.\nAdvantage: Incorporates both precision and recall into a single metric.\nDisadvantage: Ignores true negatives, which can be important in some cases. May not be ideal for highly imbalanced datasets.\nDescription: F1 score is the harmonic mean of precision and recall, providing a balance between the two.\nExample: Let’s say your model has an F1 Score of 0.75, it means there is a balanced trade-off between correctly identifying positive samples and minimizing false positives.\nCalculation: 2 * (Precision * Recall) / (Precision + Recall)\n\n\n\n4.3.1.2 Class probabilities\nClass probabilities potentially offer more information about model predictions than the simple class value. This\n4.7) ROC:\n\nHigher Value: Better performance (A perfect model that completely separates the two classes would have 100 % sensitivity and specificity / A completely ineffective model would result in an ROC curve that closely follows the 45◦ diagonal line and would have an area under the ROC curve of approximately 0.50.) Area under the curve can be used as a quantitative measure of performance\nWhen to use: Helpful tool for choosing a threshold that appropriately maximizes the trade-off between sensitivity and specificity (e.g., Lowering the threshold (aka 50%) can we improve the sensitivity to capture more true positives). Make a quantitative assessment of the model\nAdvantage: the curve is insensitive to disparities in the class proportions. Metrics that is independent of probability cutoffs\nDisadvantage: disadvantage of using the area under the curve to evaluate models is that it obscures information (i.e., the curves cross both AUC can be the same).\nDescription: AUC-ROC measures the area under the receiver operating characteristic curve, which plots the true positive rate (recall) against the false positive rate at various classification thresholds (10%, 20%… 50% = commonly used).\nExample: An AUC-ROC score of 0.85 indicates that the model has an 85% chance of correctly ranking a randomly chosen positive instance higher than a randomly chosen negative instance.\nCalculation: AUC-ROC can be calculated using various methods, such as the trapezoidal rule or Mann-Whitney U statistic.\nNotes:\n\nWe can use the partial area under the ROC curve as a technique to summarize these curves that focuses on specific parts of the curve.\nROC technique can be extended to fit three or more classes problems\n\n\n\nLift Charts:\n\n\nHigher Value: Better performance (Figure)\nWhen to use: To assess the ability of a model to detect events in a data set with two classes and allow us to choose a quasithreshold for a model.\nAdvantage: Easy connect the model to the buisness value: Using the lift plot, the expected profit can be calculated for each point on the curve to determine if the lift is sufficient to beat the baseline profit\nDisadvantage: Bad for comparing different models\nDescription: The lift chart plots the cumulative gain/lift against the cumulative percentage of samples that have been screened\nExample: Figure shows the best and worse case lift curves for a data set with a 50 % event rate. The non-informative model has a curve that is close to the 45◦ reference line, meaning that the model has no benefit for ranking samples. The other curve is indicative of a model that can perfectly separate two classes. At the 50 % point on the x-axis, all of the events have been captured by the model.\nCalculation: NA\nNotes:\n\nThe section of the curve associated with the highest-ranked samples should have an enriched true-positive rate and is likely to be the most important part of the curve.\n\n\n\n\n\nLift Charts\n\n\nNOTES:\n\nIt is important to test whether the estimated class probabilities are reflective of the true underlying probability of the sample (well-calibrated Probabilities) using a calibration plot. This plot shows some measure of the observed probability of an event versus the predicted class probability. One approach for creating this visualization is to score a collection of samples with known outcomes (preferably a test set) using a classification model. The next step is to bin the data into groups based on their class probabilities. For example, a set of bins might be [0, 10 %], (10 %, 20 %], …, (90 %, 100 %]. For each bin, determine the observed event rate. Suppose that 50 samples fell into the bin for class probabilities less than 10 % and there was a single event. The midpoint of the bin is 5 % and the observed event rate would be 2 %. The calibration plot would display the midpoint of the bin on the x-axis and the observed event rate on the y-axis. If the points fall along a 45◦ line, the model has produced well-calibrated probabilities.\n\n\n\n\nA calibration plot of the test set probabilities for random forest and quadratic discriminant analysis models\n\n\n\nIf there are three or more classes, a heat map of the class probabilities can help gauge the confidence in the predictions.\nAn approach to improving classification performance is to create an equivocal or indeterminate zone where the class is not formally predicted when the confidence is not high. (e.g., For a two-class problem that is nearly balanced in the response, the equivocal zone could be defined as 0.50 ± z.Ifz were 0.10, then samples with prediction probabilities between 0.40 and 0.60 would be called “equivocal.” In this case, model performance would be calculated excluding the samples in the indeterminate zone.)\n\n\n\n4.3.1.3 Non-Accuracy-Based Criteria\nWhen accuracy is not the primary goal for the predictive model and we want to quantify the consequences of correct and incorrect predictions (i.e., the benefits and costs)\nExamples:\n\nPredict investment opportunities that maximize return\nImprove customer satisfaction by market segmentation\nLower inventory costs by improving product demand forecasts\nReduce costs associated with fraudulent transactions: For example, in fraud detection, a model might be used to quantify the likelihood that a transaction is fraudulent. Suppose that fraud is the event of interest. Any model predictions of fraud (correct or not) have an associated cost for a more in-depth review of the case. For true positives, there is also a quantifiable benefit to catching bad transactions. Likewise, a false negative results in a loss of income.\n\n\nprofit = Cost/Benefit * TP − Cost/Benefit FP − Cost/Benefit FN\nNEC (normalized expected cost / classification_cost_penalized) = PCF × (1 − TP)+(1− PCF) × FP (between 0 and 1)\n\n\n\n\n4.3.2 For models predicting a numeric outcome\n\nRMSE:\n\n\nHigher Value: Worse performance\nWhen to use: Commonly used to measure the average magnitude of prediction errors.\nAdvantage: Penalizes larger errors more heavily, sensitive to outliers and unit is the same as the target variable, making it more interpretable.\nDisadvantage: Sensitive to outliers.\nDescription: The average distance between the observed values and the model predictions.\nExample: Continuing with the house price prediction example, an RMSE of 100 means that, on average, the predicted house prices deviate from the actual prices by $100.\nCalculation: Squared root of the (sum the residuals (the observed values minus the model predictions) and dividing by the number of samples) For example, if we have actual values [5, 10, 15] and predicted values [6, 12, 10], the MSE would be calculated as ((1^2) + (2^2) + (5^2)) / 3 = 10.\n\n\nMAE:\n\n\nHigher Value: Worse performance\nWhen to use: Suitable when you want to avoid the influence of outliers.\nAdvantage: Not sensitive to outliers as it uses the absolute error.\nDisadvantage: It does not penalize large errors as heavily as RMSE.\nDescription: The Mean Absolute Error measures the average of the absolute differences between predicted and actual values.\nExample: For the house price prediction, an MAE of $50 means that, on average, the predicted house prices deviate from the actual prices by $50.\nCalculation: For example, with the same actual and predicted values, the MAE would be calculated as (|1| + |2| + |5|) / 3 = 2.67.\n\n\nR2:\n\n\nHigher Value: Better performance\nWhen to use: Commonly used to measure the average magnitude of prediction errors. It is a measure of correlation, not accuracy. Bad for predicting a number (accuracy) but good for determining the rank correlation between the observed and predicted values (e.g., pharmaceutical scientists want to find the compounds predicted to be the most biologically active).\nAdvantage: Represents the proportion of the variance in the dependent variable that is predictable from the independent variables.\nDisadvantage: It can be misleading when used with complex models or when the number of predictors is large. It is dependent on the variation in the outcome (e.g., If the range of the houses in the test set was large, say from $60K to $2M, the variance of the sale price would also be very large. One might view a model with a 90 % R2 positively, but the RMSE may be in the tens of thousands of dollars—poor predictive accuracy for anyone selling a moderately priced property)\nDescription: The proportion of the information in the data that is explained by the model\nExample: An R-squared of 0.75 means that 75% of the variance in the house prices can be explained by the model, and the remaining 25% is due to random variation.\nCalculation: Correlation coefficient between the observed and predicted values\nNote: By plotting R2 we can see where the model is overpredict (e.g., low values) and underpredict (e.g., higher values). If this happend depending on the context, this systematic bias in the predictions may be acceptable if the model otherwise works well.\n\n\nR2 adjusted:\n\n\nHigher Value: Better performance\nWhen to use: Helpful when you have multiple predictors and want to account for model complexity.\nAdvantage: It adjusts R-squared for the number of predictors, giving a more reliable assessment of model performance when compared to R-squared.\nDisadvantage: It might not penalize overfitting adequately with large numbers of predictors.\nDescription: R-squared adjusted is similar to R-squared but takes into account the number of predictors in the model. It penalizes models with more predictors if they don’t contribute significantly to the variance explained.\nExample:\nCalculation:\n\n\nMAPE:\n\n\nHigher Value: Worse performance\nWhen to use: Useful when you want to evaluate the performance in percentage terms.\nAdvantage: Represents the percentage difference between predicted and actual values, making it interpretable and independent of the scale of the data.\nDisadvantage: It can be problematic when actual values are close to zero.\nDescription: The Mean Absolute Percentage Error calculates the mean percentage difference between predicted and actual values.\nExample: An MAPE of 10 means that, on average, the predicted house prices deviate from the actual prices by 10%.\nCalculation: For example, if we have actual values [100, 50, 75] and predicted values [90, 40, 70], the MAPE would be calculated as (|(100-90)/100| + |(50-40)/50| + |(75-70)/75|) / 3 ≈ 0.16.\n\n\nEV:\n\n\nHigher Value: Better performance (good value &gt; 0.6)\nWhen to use: Useful to understand how well the model explains the variance in the target variable.\nAdvantage: Measures the proportion of variance explained by the model, similar to R-squared.\nDisadvantage: It might not penalize the model adequately for underfitting or overfitting.\nDescription: The Explained Variance Score quantifies the proportion of variance in the target variable that is explained by the model. It ranges from 0 to 1, with 1 indicating a perfect fit. For example, an EV of 0.85 means that 85% of the variance is explained by the model.\nExample: An EV of 0.9 means that the model explains 90% of the variance in the house prices, leaving 10% unexplained by the model.\nCalculation:\n\n\nMSLE:\n\n\nHigher Value: Worse performance\nWhen to use: Suitable when you want to focus on the ratio of errors rather than their absolute differences. It can be useful when predictions are on a large scale.\nAdvantage: Penalizes underestimation and overestimation proportionally and is less sensitive to large errors.\nDisadvantage: The logarithmic transformation can be problematic for data containing zero or negative values.\nDescription: The Mean Squared Logarithmic Error calculates the mean of the squared logarithmic differences between predicted and actual values.\nExample: For the house price prediction, an MSLE of 0.1 means that, on average, the predicted house prices deviate from the actual prices by 10% when measured on a logarithmic scale.\nCalculation: For instance, if we have actual values [100, 50, 75] and predicted values [110, 40, 80], the MSLE would be calculated as ((log(110) - log(100))^2 + (log(40) - log(50))^2 + (log(80) - log(75))^2) / 3 ≈ 0.015."
  },
  {
    "objectID": "Linear Models and Its Cousins.html#ordinary-linear-regression",
    "href": "Linear Models and Its Cousins.html#ordinary-linear-regression",
    "title": "5  Linear Regression and Its Cousins",
    "section": "5.1 Ordinary linear regression",
    "text": "5.1 Ordinary linear regression\nFinds parameter estimates that have minimum bias using the NIPALS approach Advantages:\n\nhighly interpretable\nenables us to compute standard errors of the coefficients allowing to assess the statistical significance of each predictor"
  },
  {
    "objectID": "Linear Models and Its Cousins.html#partial-least-squares-pls",
    "href": "Linear Models and Its Cousins.html#partial-least-squares-pls",
    "title": "5  Linear Regression and Its Cousins",
    "section": "5.2 Partial least squares (PLS)",
    "text": "5.2 Partial least squares (PLS)\nFor regression and classification:\nsupervised dimension reduction procedure while PCR (PCA + linear regression) is unsupervised\nUSE: when there are correlated predictors and a linear regression-type solution is desired instead of PCA then linear regression (AKA PCR; If, the variability in the predictor space is not related to the variability of the response, then PCR can have difficulty identifying a predictive relationship when one might actually exist).\nEfficiently for data sets of small-to moderate size (e.g., &lt; 2,500 samples and &lt; 30 predictors)\nPre-prossesing:\n\ncentered and scaled predictors.\nRemove predictors with small PLS regression coefficients and small VIP (&lt;1)\nTo include nonlinear relationships add squared or cubic predictors\nTo include nonlinear relationships splits each predictor into two or more bins for those predictors that are thought to have a nonlinear relationship with the response. Cut points for the bins are selected by the user and are based on either prior knowledge or characteristics of the data. The original predictors that were binned are then excluded from the data set that includes the binned versions of the predictors. (GIFI approach)\n\nTuning: Cross-validation was used to determine the optimal number of PLS components to retain that minimize RMSE # of tuning parameter: PLS has one tuning parameter: the number of components to retain\nFor classification: NOTES: Produce continuous predictions that do not follow the definition of a probability-the predicted values are not necessarily between 0 and 1 and do not sum to 1. Therefore, a transformation (e.g., softmax transformation) must be used to coerce the predictions into “probability-like” values so that they can be interpreted and used for classification.\n\n5.2.1 Algorithmic Variations of PLS\n\nSIMPLS approach\n\nUSE: for data large data sets (e.g., &gt; 2,500 samples and &gt; 30 predictors)\n\nR̈annar et al. (1994) kernel\n\nUSE: when there are more predictors than samples."
  },
  {
    "objectID": "Linear Models and Its Cousins.html#penalized-models",
    "href": "Linear Models and Its Cousins.html#penalized-models",
    "title": "5  Linear Regression and Its Cousins",
    "section": "5.3 Penalized models",
    "text": "5.3 Penalized models\nFor classification and regressions: Finds parameter estimates that have lower variance. We introduce bias to reduce variance and avoid overfitting\nUSE: When sample size are small\nAdvantages: reduce variance and increases prediction on the long term\n\n5.3.1 Ridge regression\n\nAdvantages: better at reducing variance in models that contain usefull variables\n\n\n5.3.2 Lasso regression\n\nAdvantages: - better at reducing variance in models that contain useless variables - simplify model\n\n\n5.3.3 Elastic net\n\nUSE: When you don’t know if you have useless variables Advantages: Best of both ridge and lasso\n\n\n5.3.4 Logistic Regression\nFor classification only \n\n\n5.3.5 Linear Discriminant Analysis\nFor classification only\nIt is like PCA but it focuses on maximizing the separability among the known categories.\nCreate an axis (create two axis for three of more categories) that maximizes the distance between the means for the two categories while minimizing the scatter.\nAs in PCA the first axis created (LDA1) by LDA accounts for the most variation between the categories. LDA2 does the second better job, LDA 3 the third best job etc etc…\nNOTE: We can see which variables correlates the most with each LDA\n\n\n\n5.3.6 Partial Least Squares Discriminant Analysis\nFor classification only\n\n\n5.3.7 Nearest Shrunken Centroids\nFor classification only"
  },
  {
    "objectID": "Nonlinear Models.html#tree-based-models",
    "href": "Nonlinear Models.html#tree-based-models",
    "title": "6  Nonlinear Models",
    "section": "6.1 Tree-based models",
    "text": "6.1 Tree-based models\nAdvantages: - Handles high-dimensional data well. - Robust to outliers and noise. - Provides feature importance measures. - Requires minimal data preprocessing and is relatively easy to implement.\nDisadvantages:\n\nCan be computationally expensive for large datasets.\nMay not perform well on imbalanced datasets.\nLacks interpretability compared to single decision trees.\n\nUSE:\n\nWhen you have a large dataset with a high number of features.\nWhen interpretability is not a top priority.\nWhen you want to build a model that is robust to overfitting.\n\nTuning parameters:\n\nThe number of trees in the forest. Higher values generally improve performance but increase computational time (Common values are between 50 to 500).\nThe maximum depth of each decision tree: Controls the tree’s complexity and potential overfitting (Common values are between 5 to 50).\nThe minimum number of samples required to split an internal node: Higher values prevent overfitting Common values are between 2 to 20).\nThe number of features to consider when looking for the best split: Common values are ‘sqrt’ (square root of total features) or ‘log2’.\n\nFor regression:\n\nFor classification:\n\n\n6.1.1 Random forests\n\n\n\n6.1.2 Boosting trees\n\n6.1.2.1 AdaBoost\nAdvantages:\n\nCan achieve high accuracy by combining multiple weak learners.\nCan handle both classification and regression problems.\nLess susceptible to overfitting compared to individual decision trees.\nCan be combined with any base estimator that accepts sample weights.\n\nDisadvantages:\n\nSensitive to noisy data and outliers.\nCan be computationally expensive as it requires sequentially training multiple learners.\nMay not perform well on highly imbalanced datasets.\n\nUSE:\n\nWhen you have a moderately sized dataset and you want to improve the accuracy of weak learners.\nWhen you want to create a powerful ensemble with different weak learners.\n\nTuning parameters:\n\nn_estimators: The number of boosting stages (weak learners) to be run. Common values are between 50 to 500.\nlearning_rate: The contribution of each weak learner to the final combination. Common values are between 0.01 to 1.0.\nbase_estimator: The base estimator used for boosting. Common choices are decision trees with max_depth set or linear models.\n\n\n\n\n6.1.2.2 XgBoost\nAdvantages:\n\nHighly efficient and scalable, making it suitable for large datasets.\nCan handle missing data and supports regularization to prevent overfitting.\nProvides built-in cross-validation, early stopping, and feature importance.\nOften performs well even with default hyperparameters.\n\nDisadvantages:\n\nCan be sensitive to hyperparameter tuning.\nRequires more careful tuning and validation compared to Random Forest and AdaBoost.\nThe interpretation of feature importance may not be as straightforward as in Random Forest.\n\nUSE:\n\nWhen you have a large dataset and computational efficiency is crucial.\nWhen you need better performance compared to other algorithms on a wide range of problems.\nWhen you can invest time in tuning hyperparameters.\n\nTuning parameters:\n\nn_estimators: The number of boosting rounds. Common values are between 50 to 500.\nlearning_rate: The step size shrinkage used to prevent overfitting. Common values are between 0.01 to 0.3.\nmax_depth: The maximum depth of each tree. Common values are between 3 to 10.\nsubsample: The fraction of samples used for training each tree. Common values are between 0.5 to 1.0.\ncolsample_bytree: The fraction of features used for training each tree. Common values are between 0.5 to 1.0.\n\nFor regression:  \nFor classification:  \nTuning parameters:\n\nmaximum number of leaves: (generally between 8 and 32)\nlearning rate: Value between 0 and 1 (generally 0.1)"
  },
  {
    "objectID": "Nonlinear Models.html#multivariate-adaptive-regression-splines-mars",
    "href": "Nonlinear Models.html#multivariate-adaptive-regression-splines-mars",
    "title": "6  Nonlinear Models",
    "section": "6.2 Multivariate Adaptive Regression Splines (MARS)",
    "text": "6.2 Multivariate Adaptive Regression Splines (MARS)\nMARS uses surrogate features (usually a function of only one or two predictors (second degree) at a time which are broken into two groups and models linear relationships between the predictor and the outcome in each group) instead of the original predictors (like pls and neural networks).\nNOTES - GCV statistic is use to determine the contribution of each feature to the model.\nTuning parameters:\n\nthe degree of the features that are added to the model (number of interaction; e.g., 0-4)\nthe number of retained terms\n\nAdvantages:\n\nthe model automatically conducts feature selection\ninterpretability is high\nrequires very little pre-processing of the data (Correlated predictors do not drastically affect model performance, but they can complicate model interpretation.).\n\nDisadvantages: For MARS models that can include two or more terms at a time, we have observed occasional instabilities in the model predictions where a few sample predictions are wildly inaccurate (perhaps an order of magnitude off of the true value). This problem has not been observed with additive MARS models (models with degree of 1).\nUSE: When there is a clear indication that the relationship between the dependent variable and independent variables is non-linear and interpretability is impportante"
  },
  {
    "objectID": "Nonlinear Models.html#support-vector-machines-svm",
    "href": "Nonlinear Models.html#support-vector-machines-svm",
    "title": "6  Nonlinear Models",
    "section": "6.3 Support Vector Machines (SVM)",
    "text": "6.3 Support Vector Machines (SVM)\nUSE: When we seek to minimize the effect of outliers\n\nNOTE: This principle also apply for regression. However in this case the svm will search for hyperplane that holds the maximum of the observation within the margin (tolerance level)\n\n\nTuning parameters: - Kernel: SVM can use different kernel functions to transform the input data into a higher-dimensional space, where it becomes easier to find a separating hyperplane. Common kernel functions include: - Linear Kernel (If regression line is truly linear, the linear kernel function will be a better choice) - Polynomial Kernel (In general, quadratic models have smaller error rates than the linear models) (tuning parameters: degree and scale factor (coef0) and c) - Radial Basis Function (RBF) Kernel (radial basis function has been shown to be very effective overall and easier to tune than polynomial one less tuning parameter) (tuning parameters: σ (sigma) that controls the scale and c) - hyperbolic tangent\n\nThreshold ε (epsilon) (called margin in tidymodels?) (If the threshold is set to a relatively large value, then the outliers are the only points that define the regression line) (e.g., ε = 0.01; the cost parameter provides more flexibility for tuning the model. So it is suggested to fix a value for ε and tune over the other kernel parameters)\nC parameter (Cost; e.g., values between 0.25 and 2048):\n\nFor classification: It controls the trade-off between maximizing the margin and minimizing the classification error. A smaller C value creates a wider margin but may allow some misclassifications, while a larger C value creates a narrower margin but may result in fewer misclassifications on the training set.\nFor regression: The cost parameter is the main tool for adjusting the complexity of the model. When the cost is large, the model becomes very flexible since the effect of errors is amplified. When the cost is small, the model will “stiffen” and become less likely to over-fit (but more likely to underfit)\n\n\nPre-processing: Center and scale the predictors prior to building an SVM model since the predictors enter into the model as the sum of cross products, differences in the predictor scales can affect the model."
  },
  {
    "objectID": "Nonlinear Models.html#k-nearest-neighbors-knn",
    "href": "Nonlinear Models.html#k-nearest-neighbors-knn",
    "title": "6  Nonlinear Models",
    "section": "6.4 K-Nearest Neighbors (KNN)",
    "text": "6.4 K-Nearest Neighbors (KNN)\nFor classification: \nFor regression: \nTuning parameters: K number of neighbors.\nAdvantages: The KNN method can have poor predictive performance when local predictor structure is not relevant to the response.\nPre-processing: - Remove irrelevant, noise-laden predictors is a key pre-processing step for KNN, since these can cause similar samples to be driven away from each other in the predictor space\nNOTE: to enhance KNN predictability weight the neighbors’ contribution to the prediction of a new sample based on their distance to the new sample."
  },
  {
    "objectID": "Nonlinear Models.html#neural-networks",
    "href": "Nonlinear Models.html#neural-networks",
    "title": "6  Nonlinear Models",
    "section": "6.5 Neural Networks",
    "text": "6.5 Neural Networks\nNeural Networks uses surrogate features instead of the original predictors (like pls and MARS)\nFor classification: NOTES: Produce continuous predictions that do not follow the definition of a probability-the predicted values are not necessarily between 0 and 1 and do not sum to 1. Therefore, a transformation (e.g., softmax transformation) must be used to coerce the predictions into “probability-like” values so that they can be interpreted and used for classification.\nFor regression: To be continued"
  },
  {
    "objectID": "Nonlinear Models.html#nonlinear-discriminant-analysis",
    "href": "Nonlinear Models.html#nonlinear-discriminant-analysis",
    "title": "6  Nonlinear Models",
    "section": "6.6 Nonlinear Discriminant Analysis",
    "text": "6.6 Nonlinear Discriminant Analysis\nFor classification only:"
  },
  {
    "objectID": "Nonlinear Models.html#flexible-discriminant-analysis",
    "href": "Nonlinear Models.html#flexible-discriminant-analysis",
    "title": "6  Nonlinear Models",
    "section": "6.7 Flexible Discriminant Analysis",
    "text": "6.7 Flexible Discriminant Analysis\nFor classification only:"
  },
  {
    "objectID": "Nonlinear Models.html#naive-bayes",
    "href": "Nonlinear Models.html#naive-bayes",
    "title": "6  Nonlinear Models",
    "section": "6.8 Naive Bayes",
    "text": "6.8 Naive Bayes\nFor classification only:"
  },
  {
    "objectID": "Tree models.html",
    "href": "Tree models.html",
    "title": "7  Tree based models",
    "section": "",
    "text": "8 Treebased classification models\nCharacteristics: - resistant to outliers\n\n\n9 Treebased regression models"
  },
  {
    "objectID": "Deal with suboptimal data.html#class-imbalance",
    "href": "Deal with suboptimal data.html#class-imbalance",
    "title": "8  Remedies for suboptimal data",
    "section": "8.1 Class Imbalance",
    "text": "8.1 Class Imbalance\nAn imbalance occurs when one or more classes have very low proportions in the training data as compared to the other classes.\n\nOnline advertising: Ad clicked or not (2.4%)\nPharmaceutical research: Molecules with activity (vwry few) or not\nInsurance claims: Fraud (only 22%) or not fraud\nSpam detection: Spam or not spam\nSelling buisness: Buy (6%) or not buy\n\n\n8.1.1 The Effect of Class Imbalance\n\nThe models achieve good specificity (since almost every customer is predicted no insurance) but have poor sensitivity (Figure). \nThe imbalance also had a severe effect on the predicted class probabilities. (e.g., In the random forest model, for example, 82 % of the customers have a predicted probability of having insurance of 10 % or less. This highly left-skewed predicted probability distribution also occurs for the other two models. This means that the models are not very confident in predicting that most customers have insurance; they tend to assign low probabilities of having insurance to a significant portion of the customers.)\nImbalance cause that lift charts and ROC curves have similar patterns\n\n\n\n8.1.2 Strategies for overcoming class imbalances\n\n\n8.1.3 Hyperparameters selection\n\nModel tuning strategy: tune the model to maximize the accuracy or sensitivity of the minority class(es)\n\n\n\n8.1.4 Post-processing techniques (use model outputs)\n\nAlternate probability Cutoffs to improve the prediction accuracy of the minority class samples (i.e., post-processing the model predictions to redefine the class predictions). The most straightforward approach is to use the ROC curve since it calculates the sensitivity and specificity across a continuum of cutoffs. Using this curve, an appropriate balance between sensitivity and specificity can be determined.\n\nSeveral techniques exist for determining a new cutoff:\n\nFirst, if there is a particular target that must be met for the sensitivity or specificity, this point can be found on the ROC curve and the corresponding cutoff can be determined.\nAnother approach is to find the point on the ROC curve that is closest (i.e., the shortest distance) to the perfect model (with 100 % sensitivity and 100 % specificity), which is associated with the upper left corner of the plot. In Figure, a cutoff value of 0.064 would be the closest to the perfect model.\nThe cutoff associated with the largest value of the Youden index (measures the proportion of correctly predicted samples for both the event and nonevent groups / can be computed for each cutoff that is used to create the ROC curve): show superior performance relative to the default 50 % value. For the random forest ROC curve, the cutoff that maximizes the Youden index (0.021) is similar to the point closest to the optimal model.\n\n\n\n\n\n\nROC: The predicted sensitivity for the new cutoff of 0.064 is 64.4 %, which is a significant improvement over the value generated by the default cutoff. The consequence of the new cutoff is that the specificity is estimated to drop from 99 % to 75.9 %.\n\n\nNOTE: In our analysis, the alternate cutoff for the model was not derived from the training or test sets. It is important, especially for small samples sizes, to use an independent data (small evaluation set used for developing post-processing techniques ~ 10% ≠ training set used to tune model) set to derive the cutoff. If the training set predictions are used, there is likely a large optimistic bias in the class probabilities that will lead to inaccurate assessments of the sensitivity and specificity. If the test set is used, it is no longer an unbiased source to judge model performance.\n\nAdjusting Prior Probabilities: For models that use prior probabilities naiıve Bayes and discriminant analysis classifiers. Unless specified manually, these models typically derive the value of the priors from the training data. Weiss and Provost (2001a) suggest that priors that reflect the natural class imbalance will materially bias predictions to the majority class. Using more balanced priors or a balanced training set may help deal with a class imbalance. (e.g., when classes are 6 % and 94 % for the insured and uninsured it is better to use 60 % for the insured and 40 % for the uninsured). This strategy did not change the model (same ROC) but allows for different trade-offs between sensitivity and specificity.\n\n\n\n8.1.5 Alter training data prior to model training\n\nAdjust Sampling Methods: Non of them is a clear winner, it depends of the case study\n\npriori sampling approach: select a training set sample to have roughly equal event rates during the initial data collection. However, the test set should be sampled to be more consistent with the state of nature and should reflect the imbalance so that honest estimates of future performance can be computed.\npost hoc sampling approach:\n\n\nup-sampling:\n\nadding random samples with replacement from minority classes)\n\ndown-sampling:\n\nrandomly sample the majority classes so that all classes have approximately the same size\nbootstrap sample across all cases such that the classes are balanced in the bootstrap set (advatange of bootstrap is that we can obtain the estimate of variation about the down-sampling)\n\nSMOTE: adds new/synthetic samples to the minority class and down-sample cases from the majority class via random sampling\n\nNOTE: Adjusting sampling can bias model performance (e.g., up-sampling: the same sample can be use to predict and tune model)\n\n\n\n8.1.6 Alter model training process (model parameters are being modified)\n\nUnequal Case Weights: Many of the predictive models for classification (boosting trees which apply different case weights at each iteration) have the ability to use case weights where each individual data point can be given more emphasis in the model training phase. Increase the weights for the samples in the minority classes\nCost-Sensitive Training: Some models (SVM, CART trees, C5.0 trees) can alternatively optimize a cost or loss function that differentially weights specific types of errors of specific classes (it can cause that class probabilities cannot be generated and ROC cannot be use). For example, it may be appropriate to believe that misclassifying true events (false negatives) is X times as costly as incorrectly predicting nonevents (false positives). Correctly classify class A is more importante than correctly classify class B"
  },
  {
    "objectID": "Deal with suboptimal data.html#big-sample-size",
    "href": "Deal with suboptimal data.html#big-sample-size",
    "title": "8  Remedies for suboptimal data",
    "section": "8.2 Big sample size",
    "text": "8.2 Big sample size\nAn increase in the number of samples can have less positive consequences:\n\nComputational burdens as the number of samples (and predictors) grows\nThere are diminishing returns on adding more of the same data from the same population. Since models stabilize with a sufficiently large number of samples, garnering more samples is less likely to change the model fit."
  },
  {
    "objectID": "Measuring Predictor Importance.html#for-numeric-outcomes",
    "href": "Measuring Predictor Importance.html#for-numeric-outcomes",
    "title": "9  Measuring Predictor Importance",
    "section": "9.1 For Numeric Outcomes",
    "text": "9.1 For Numeric Outcomes\n\nLOESS\nt-statistic\nANOVA\nRelief"
  },
  {
    "objectID": "Measuring Predictor Importance.html#for-categorical-outcomes",
    "href": "Measuring Predictor Importance.html#for-categorical-outcomes",
    "title": "9  Measuring Predictor Importance",
    "section": "9.2 For Categorical Outcomes",
    "text": "9.2 For Categorical Outcomes\n\narea under the ROC curve\nt-statistics\nMIC\nRelief"
  },
  {
    "objectID": "Feature Selection.html#unsupervised-methods",
    "href": "Feature Selection.html#unsupervised-methods",
    "title": "10  Feature Selection",
    "section": "10.1 Unsupervised methods",
    "text": "10.1 Unsupervised methods\nWhen the outcome is ignored during the elimination of predictors, the technique is unsupervised.\n\nremoving predictors that have high correlations with other predictors\nremoving near-zero variance predictors"
  },
  {
    "objectID": "Feature Selection.html#supervised-methods",
    "href": "Feature Selection.html#supervised-methods",
    "title": "10  Feature Selection",
    "section": "10.2 Supervised methods",
    "text": "10.2 Supervised methods\nWhen, the outcome is typically used to quantify the importance of the predictors. Predictors are specifically selected for the purpose of increasing accuracy or to find a subset of predictors to reduce the complexity of the model"
  },
  {
    "objectID": "Feature Selection.html#consequences-of-using-non-informative-predictors",
    "href": "Feature Selection.html#consequences-of-using-non-informative-predictors",
    "title": "10  Feature Selection",
    "section": "10.3 Consequences of Using Non-informative Predictors",
    "text": "10.3 Consequences of Using Non-informative Predictors\nThe presence of non-informative variables can add uncertainty/noise to the predictions and reduce the overall effectiveness of the model (linear regression, partial least squares, neural networks, svm). Regression trees, MARS models and Random forests are not affected or very slightly in the case of random forests"
  },
  {
    "objectID": "Feature Selection.html#approaches-for-reducing-the-number-of-predictors",
    "href": "Feature Selection.html#approaches-for-reducing-the-number-of-predictors",
    "title": "10  Feature Selection",
    "section": "10.4 Approaches for Reducing the Number of Predictors",
    "text": "10.4 Approaches for Reducing the Number of Predictors\n\nWrapper methods evaluate multiple models using procedures that add and/or remove predictors to find the optimal combination that maximizes model performance. In essence, wrapper methods are search algorithms that treat the predictors as the inputs and utilize model performance as the output to be optimized.\n\nForward, Backward, and Stepwise Selection: Add, remove predictors or both to find the model that results in the smallest model RMSE/AIC (Used in linear regressions)\nCorrelation-based feature selection: find the best subset of predictors that have strong correlations with the outcome but weak between-predictor correlations\nSimulated annealing: Starting from an initial solution, the method iteratively explores neighboring solutions with the ability to accept worse solutions initially, gradually decreasing this acceptance as the process continues. This balance between exploration and exploitation helps the algorithm escape local optima and search for global optima in the solution space.\nGenetic Algorithms: GAs start with a population of potential solutions encoded as “genetic” representations. Through multiple generations, solutions are selected based on their fitness, which measures how well they solve the problem. These selected solutions then undergo crossover (combination of genetic material) and mutation (random changes) to produce a new population. Over successive generations, the algorithm converges towards better solutions as traits from successful solutions propagate and refine in the population.\n\nAdvantages: Disadvantages:\n\nmany models are evaluated (which may also require parameter tuning) and thus an increase in computation time\nincreased risk of over-fitting\n\nFilter methods evaluate the relevance of the predictors outside of the predictive models and subsequently model only the predictors that pass some criterion. For example, for classification problems, each predictor could be individually evaluated to check if there is a plausible relationship between it and the observed classes. Only predictors with important relationships would then be included in a classification model.\nAdvantages: more computationally efficient Disadvantages:\n\nthe selection criterion is not directly related to the effectiveness of the model\nmethods evaluate each predictor separately, and, consequently, redundant (i.e., highly-correlated) predictors may be selected and important interactions between variables will not be able to be quantified\nsubjective nature to the procedure. Most scoring methods have no obvious cut point to declare which predictors are important enough to go into the model (In practice, finding an appropriate value for the confidence value α may require several evaluations until acceptable performance is achieved.)\n\nEmbedded methods are models where the feature selection procedure occurs naturally in the course of the model fitting process. Here an example would be a simple decision tree where variables are selected when the model uses them in a split. If a predictor is never used in a split, the prediction equation is functionally independent of this variable and it has been selected out.\n\nNote: When using other search procedures or filters for reducing the number of predictors, there is still a risk. The following situations increase the likelihood of selection bias:\n\nThe data set is small.\nThe number of predictors is large (since the probability of a non-informative predictor being falsely declared to be important increases).\nThe predictive model is powerful (e.g., black-box models), which is more likely to over-fit the data.\nNo independent test set is available\n\nWhen the data set is large, it is recommended separate data sets for selecting features, tuning models, and validating the final model (and feature set). When training sets are small, proper resampling is critical. When the amount of data is not too small (333 obs), it is recommended setting aside a small test set to double check that no gross errors have been committed."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Kuhn, Max, and Kjell Johnson. 2013. Applied Predictive\nModeling. New York, NY: Springer.\nhttps://doi.org/10.1007/978-1-4614-6849-3.\n\n\n“What Is the Difference Between Correlation and Cointegration?\nIs Cointegration a Good Measure of Risk?” n.d.\nQuora.\nhttps://www.quora.com/What-is-the-difference-between-correlation-and-cointegration-Is-cointegration-a-good-measure-of-risk.\nAccessed July 17, 2023."
  },
  {
    "objectID": "Exploratory Visualizations.html",
    "href": "Exploratory Visualizations.html",
    "title": "2  Exploratory Visualizations",
    "section": "",
    "text": "Univariate visualizations (Box Plots, Violin Plots, and Histograms) to understand the distribution of a single variable.\n\n\nunderstand the distribution of the response variable (symmetric distribution / skewed distribution / distribution has multiple peaks or modes / outliers) to:\n\nknow its variation and provide a lower bound of the expectations of model performance (the residuals from a model that contains these predictors should have less variation than the variation of the response)\nknow if it should be transformed to a normal distribution prior to analysis to have better predictive performance\nprovide clues for including or creating features that help explain the response\n\n\nscatter plots of individual predictors and the response,\na pairwise correlation plot among the predictors,\na projection of high-dimensional predictors into a lower dimensional space,\nline plots for time-based predictors,\nthe first few levels of a regression or classification tree,\na heatmap across the samples and predictors\n\nmosaic plots for examining associations among categorical variables."
  },
  {
    "objectID": "Terms & definitions.html#residual",
    "href": "Terms & definitions.html#residual",
    "title": "1  Terms & definitions",
    "section": "1.6 Residual",
    "text": "1.6 Residual\ndifference between the observed value and the predicted value"
  },
  {
    "objectID": "Terms & definitions.html#greedy-algorithms",
    "href": "Terms & definitions.html#greedy-algorithms",
    "title": "1  Terms & definitions",
    "section": "1.9 Greedy algorithms",
    "text": "1.9 Greedy algorithms\nWhen a procedure is greedy, it means that it does not reevaluate past solutions."
  },
  {
    "objectID": "Terms & definitions.html#extrapolation",
    "href": "Terms & definitions.html#extrapolation",
    "title": "1  Terms & definitions",
    "section": "1.10 Extrapolation",
    "text": "1.10 Extrapolation\nExtrapolation is commonly defined as using a model to predict samples that are outside the range of the training data. To know if we can trust our model we need to compare the predictor space between the training data and new data.\n\nThis can be done using PCA. If the training data and new data are generated from the same mechanism, then the projection of these data will overlap in the scatter plot. However, if the training data and new data occupy different parts of the scatter plot, then the data may not be generated by the same mechanism and predictions for the new data should be used with caution.\n\n\n\n\nPCA and Extrapolation: the training and testing data appear to occupy the same space as determined by these components.\n\n\n\nThis can also be done using the following algorithm\n\n ## Autoencoders\nModels that can denoise or smooth the predictor values. The outcome is not required to create an autoencoder so unlabeled data can potentially improve the situation.\n\n\n\n\n“What Is the Difference Between Correlation and Cointegration? Is Cointegration a Good Measure of Risk?” n.d. Quora. https://www.quora.com/What-is-the-difference-between-correlation-and-cointegration-Is-cointegration-a-good-measure-of-risk. Accessed July 17, 2023."
  },
  {
    "objectID": "Feature Engineering.html#exploratory-visualizations",
    "href": "Feature Engineering.html#exploratory-visualizations",
    "title": "11  Feature Engineering",
    "section": "11.1 Exploratory Visualizations",
    "text": "11.1 Exploratory Visualizations\n\nUnivariate visualizations (Box Plots, Violin Plots, and Histograms) to understand the distribution of a single variable.\n\nunderstand the distribution of the response variable (symmetric distribution / skewed distribution / distribution has multiple peaks or modes / outliers) to:\n\nknow its variation and provide a lower bound of the expectations of model performance (the residuals from a model that contains these predictors should have less variation than the variation of the response)\nknow if it should be transformed to a normal distribution prior to analysis to have better predictive performance\nprovide clues for including or creating features that help explain the response\n\nunderstand the distribution of the predictors (when moderate number of predictors (&lt; ~100) / if not examine a subset of predictors that are thought to be important)\n\nscatter plots or mosaic plots to see if any of the responses “cluster” with others\n\ncorrespondence analysis can answered the question: linear regression for scatter plot or X2 for mosaic plot. And principal coordinates can be computed to create new variables\n\n\n\nscatter plots / bar charts + 95% confidence intervals / smoother of individual predictors and the response outcome:\n\nto easly test for new crucial predictor (e.g., if we find a strong linear relationship)\ntry to understand the relation between predictors and if some points don’t follow the overall pattern trying to understand them could lead to a new feature.\n\n\n  \n\na heatmap across the samples and predictors (e.g., to see when these unusual values occur)\na pairwise correlation plot among the predictors (e.g., high degree of correlation is a clear indicator that the information present across the stations is redundant and could be eliminated or reduced.)\n\nhierarchical cluster analysis to arrange samples in a way that those that are ‘close’ in the measurement space are also nearby in their location on the axis.\n\nline plots for time-based predictors (e.g., trends or patterns associated with time to know if variable’s current value is more related to recent values than to values further apart in time)\nPCA / PLS / MDS (multidimensional scaling) : to engineer features that effectively condense the original predictors’ information while retaining crucial predictive information. (if the first and first and second component captures 76.7% and 83.1% respectively the information is redundant and can likely be summarized in a more condensed fashion)\n\ncumulative amount of variation summarized: how many components are required to summarize a sufficient amount of variation in the data\nscatter plot of the first two components to detect clusters\nviolin plot of the first and second components against the underlying variables that appear to affect them the most"
  },
  {
    "objectID": "Feature Engineering.html#postmodeling-exploratory-visualizations",
    "href": "Feature Engineering.html#postmodeling-exploratory-visualizations",
    "title": "11  Feature Engineering",
    "section": "11.2 Postmodeling Exploratory Visualizations",
    "text": "11.2 Postmodeling Exploratory Visualizations\nTo understand the next set of improvements.\n\nMultiple linear regression (lm): identify relationships that may be useful to include in the model\n\npartial regression plot:\n\nthe first few levels of a regression or classification tree"
  },
  {
    "objectID": "Feature Engineering.html#encoding-categorical-predictors",
    "href": "Feature Engineering.html#encoding-categorical-predictors",
    "title": "11  Feature Engineering",
    "section": "11.3 Encoding Categorical Predictors",
    "text": "11.3 Encoding Categorical Predictors\n\n11.3.1 Creating Dummy Variables for Unordered Categories\nAdvatanges: Lead to zero-variance predictor which can be remove and omitting rarely occurring values and propagates this noise into the resampling estimates of performance. Disadvatanges: However if dummy zero-variance predictor is remove the model will not be able to predict USE: - When categories are small and it does not lead to zero-variance predictors NOTE: when model can support categorical data it is very difficult to predict if dummy variable will improve the model. Start without dummy variables and, if the model appears promising, to also try refitting using dummy variables.\n\n\n11.3.2 Encoding Predictors with Many Categories\n\nCreating Dummy Variables and remove zero-variance predictor\nHashing function to combine categories to create feature hashing\nCreate an “other” category\nSupervised Encoding Methods to encode categorical predictors to numeric columns using the outcome data as a guide\n\nUSE: When the predictor has many possible values\nTechnics:\n\neffect or likelihood encoding: (e.g., mean or median sale price of a house for each neighborhood from the training data and use this statistic to represent the factor level in the model)\nlogistic regression model (for classification problems):\nlinear regression model (for regression problems):\nword/entity embedding: estimate a smaller set of numeric features that can be used to adequately represent the categorical predictors\nhidden layers\n\nDisadvantage:\n\ngenerate error when a factor level has a single value (to solve the issue use shrinking methods such as Bayesian analysis)\nincreases the possibility of overfitting\ncan drastically underestimate the variation in the data and might give a falsely optimistic opinion of the utility of the new encoding column\n\nNOTES: It is strongly recommended that either different data sets be used to estimate the encodings and the predictive model or that their derivation is conducted inside resampling so that the assessment set can measure the overfitting (if it exists).\n\n\n\n\n11.3.3 Approaches for Novel Categories to enable the original model to be applied to new data without completely refitting it\n\nCreate a “other” category and asign the new category no other\nCreate a zero-variance dummy variable in the training or test set or both.\nSupervised Encoding Methods to encode categorical predictors to numeric columns using the outcome data as a guide\n\nUSE: When new levels appear after model training\n\nEncodings for Ordered Data (e.g., “low”, “medium”, and “high.”)\n\nTechnics: polynomial contrast\n\nAdvatanges: By employing polynomial contrasts, we can investigate multiple relationships (linear, quadratic, etc.) simultaneously by including these in the same model.\nDisadvantage:\n\npolynomial contrasts may not effectively relate a predictor to the response (For example, in some cases, one might expect a trend where “low” and “middle” samples have a roughly equivalent response but “high” samples have a much different response.)\nNot recommended when there are moderate to high number of categories\n\n\nTechnics: Translate the ordered categories into a single set of numeric scores based on context-specific information.\n\n\n\n\n11.3.4 Approaches for Text Data\n\nTransform the text data into the odds-ratio of containing a keywords/link this can be extended to the odds-ratio of containing a text/link for each response variable (The rate of hyperlinks in the STEM profiles (response variable) was 21%, while this rate was 12.4% in the non-STEM profiles. For the STEM profiles, the odds of containing a hyperlink are relatively small with a value of 0.21/1-0.21 = 0.27. For the non-STEM profiles, it is even smaller (0.142).)\ncreate “text-related” features: (e.g., number of commas, hashtags, mentions, exclamation points)\ncode sentiment values\ncode language used (e.g., first-, second-, or third-person text and other language elements)"
  },
  {
    "objectID": "Feature Engineering.html",
    "href": "Feature Engineering.html",
    "title": "11  Feature Engineering",
    "section": "",
    "text": "12 Engineering Numeric Predictors converting continuous predictors into a form that a model can better utilize.\nThis type of data can occur if a sample is measured repeatedly over time, if a sample has many highly related/correlated predictors, or if sample measurements occur through a hierarchical structure.\nBasic preprocessing steps for profiled data can include estimating and adjusting the baseline effect, reducing noise across the profile, and harnessing the information contained in the correlation among predictors. The latter in order to remove the characteristics that prevent this type of data from being used with most predictive models while simultaneously preserving the predictive signal between the profiles and the outcome."
  },
  {
    "objectID": "Feature Engineering.html#transformation",
    "href": "Feature Engineering.html#transformation",
    "title": "11  Feature Engineering",
    "section": "12.1 Transformation:",
    "text": "12.1 Transformation:\n\ncentering\nscaling\ntransforming a distribution to symmetry"
  },
  {
    "objectID": "Feature Engineering.html#feature-engeneering",
    "href": "Feature Engineering.html#feature-engeneering",
    "title": "11  Feature Engineering",
    "section": "12.2 Feature engeneering",
    "text": "12.2 Feature engeneering\n\ntransforming predictors in its original scale to nonlinear scales that may be informative.\n\nTechincs:\n\nbasis expansions (e.g., Squared predictors for simplistic models such as regressions)\nsplines\ncombination of kernel function and PCA\n\nDisadvantage: computational cost\n\nreduce the dimension of the predictors\n\nTechincs:\n\nPCA (unsupervised approach)\nICA (unsupervised approach)\nNNMF (unsupervised approach)\nPLS (supervised approach)\ncategorizing the response (only appropriate when the response is bimodal (or multimodal)).\n\n\nharness information in unlabeled data or dampen the effect of extreme samples\n\nTechincs:\n\nautoencoders\nspatial sign transformation\ndistance and depth measures (e.g., class centroids for classification models: centers of the predictor data for each class. For each predictor, the distance to each class centroid can be calculated and these distances can be added to the model)"
  },
  {
    "objectID": "SQL.html",
    "href": "SQL.html",
    "title": "12  SQL",
    "section": "",
    "text": "library(dbplyr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::ident()  masks dbplyr::ident()\n✖ dplyr::lag()    masks stats::lag()\n✖ dplyr::sql()    masks dbplyr::sql()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ncon &lt;- DBI::dbConnect(RSQLite::SQLite(), \":memory:\")\ncopy_to(con, mtcars)\n\n\nSELECT * \nFROM mtcars\n\n\nDisplaying records 1 - 10\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n18.1\n6\n225.0\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\n14.3\n8\n360.0\n245\n3.21\n3.570\n15.84\n0\n0\n3\n4\n\n\n24.4\n4\n146.7\n62\n3.69\n3.190\n20.00\n1\n0\n4\n2\n\n\n22.8\n4\n140.8\n95\n3.92\n3.150\n22.90\n1\n0\n4\n2\n\n\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\n\n\n\n\n\n\n13 SQL vs dplyr\n\n\n\n\n\n\n\nSQL\ndplyr\n\n\n\n\nSELECT\nselect\n\n\nFROM\ndf %&gt;%\n\n\nWHERE\nfilter\n\n\nGROUP BY\ngroup_by\n\n\nHAVING (filter results of aggregate functions applied to grouped data)\nfilter\n\n\nORDER BY\narrange"
  },
  {
    "objectID": "SQL.html#sql-vs-dplyr",
    "href": "SQL.html#sql-vs-dplyr",
    "title": "12  SQL",
    "section": "12.1 SQL vs dplyr",
    "text": "12.1 SQL vs dplyr\n\n\n\n\n\n\n\nSQL\ndplyr\n\n\n\n\nSELECT table1.column_name1\nselect\n\n\nFROM\ndf %&gt;%\n\n\nINNER JOIN table2 ON table1.column_name1 = table2.column_name1 AND table1.column_name2 = table2.column_name2\ninner_join(., table2, by = c(“column_name1” = “column_name1”, “column_name2” = “column_name2”))\n\n\nLEFT JOIN table2 ON table1.column_name = table2.column_name\nleft_join(., table2, by = c(“column_name” = “column_name”)\n\n\nRIGHT JOIN table2 ON table1.column_name = table2.column_name\nright_join(., table2, by = c(“column_name” = “column_name”)\n\n\nFULL OUTER JOIN table2 ON table1.column_name = table2.column_name\nfull_join(., table2, by = c(“column_name” = “column_name”)\n\n\nWHERE (IN [num/char]; BETWEEN [num] AND [num])\nfilter ( %in% ; %in% c(1:10))\n\n\nGROUP BY\ngroup_by\n\n\nHAVING (filter results of aggregate functions applied to grouped data)\nfilter\n\n\nORDER BY\narrange\n\n\nLIMIT [num]\nslice_head(n = [num])"
  },
  {
    "objectID": "SQL.html#functions",
    "href": "SQL.html#functions",
    "title": "12  SQL",
    "section": "12.2 functions",
    "text": "12.2 functions\nMOD Returns the remainder (number after the point 67 in 3.67) of a number divided by another number LENGTH Returns the length of a string (in bytes)"
  },
  {
    "objectID": "Choose model.html#model-process",
    "href": "Choose model.html#model-process",
    "title": "2  Choose machine learning model",
    "section": "2.1 Model Process",
    "text": "2.1 Model Process\n\n\n\nModeling Process: four distinct models are being evaluated. When modeling data, there is almost never a single model fit or feature set that will immediately solve the problem. The process is more likely to be a campaign of trial and error to achieve the best results. The effect of feature sets can be much larger than the effect of different models. The interplay between models and features is complex and somewhat unpredictable. With the right set of predictors, is it common that many different types of models can achieve the same level of performance.\n\n\n\nExploratory data analysis (evaluating simple summary measures or identifying predictors that have strong correlations with the outcome / how the predictors will be represented)\nPre-processing the predictor data\nEstimating model parameters\nSelecting predictors for the model\nEvaluating model performance\nFine tuning class prediction rules (via ROC curves, etc.)\noptimization routines (e.g., Nelder–Mead simplex method = direct methods) can later be use to search the optimal key value (e.g., determine possible mixtures with improved compressive strength)\nEDA can be conducted on the model results (e.g., residual analysis)\n\n\n\n\n\nKuhn, Max, and Kjell Johnson. 2013. Applied Predictive Modeling. New York, NY: Springer. https://doi.org/10.1007/978-1-4614-6849-3."
  }
]