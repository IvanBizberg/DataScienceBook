[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science Book",
    "section": "",
    "text": "Preface\nWelcome to the world of data science, a captivating realm where art, mathematics, and technology converge to unlock the hidden insights lying dormant within vast troves of information. In this book, we embark on an exhilarating journey through the most important aspects of data science techniques, exploring their foundations, applications, and transformative potential.\nThe rapid advancement of technology and the explosive growth of data have ushered in an era of unprecedented opportunities. Every click, every transaction, and every interaction generates a data footprint that can be harnessed to drive innovation, solve complex problems, and shape the future. Data science equips us with the tools to make sense of this digital universe, enabling us to extract meaning from seemingly chaotic data and turn it into actionable knowledge.\nAt its core, data science is a multidisciplinary field that draws from various domains, including statistics, mathematics, computer science, and domain expertise. It encompasses a wide range of techniques, algorithms, and methodologies designed to collect, analyze, interpret, and visualize data to uncover patterns, make predictions, and generate insights.\n\n\n\n\n\n\nNote\n\n\n\nThis book is in the process of being made and is intended to serve as a comprehensive compilation of the most important aspects of data science techniques and tools. However, it is important to note that the content presented herein reflects the author’s personal understanding and interpretation of these concepts, which may occasionally deviate from the precise and mathematical definitions of certain data science principles."
  },
  {
    "objectID": "Terms & definitions.html#correlation",
    "href": "Terms & definitions.html#correlation",
    "title": "1  Terms & definitions",
    "section": "1.1 Correlation",
    "text": "1.1 Correlation\n\nCorrelation measures the strength and direction of the linear relationship between two variables, but it standardizes the measure to fall between -1 and 1.\n\n\n\n\n\n\n\nWhen to use it:\n\n\n\nCorrelation is a more useful measure than covariance when comparing relationships across different datasets or variables, as it removes the influence of the scales of the variables."
  },
  {
    "objectID": "Terms & definitions.html#correlation-vs-covariance",
    "href": "Terms & definitions.html#correlation-vs-covariance",
    "title": "1  Terms & definitions",
    "section": "1.2 Correlation vs covariance",
    "text": "1.2 Correlation vs covariance\n\nCovariance measures the direction and magnitude of the linear relationship between two variables. It calculates how changes in one variable are related to changes in another variable. Covariance can take any value, positive or negative, depending on the nature of the relationship.\n\n\n\n\n\n\n\nWhen to use it:\n\n\n\n\nScaling and Interpretation: If you are primarily interested in the magnitude of the relationship between two variables, without the need for standardized interpretation, covariance can be used. Since covariance is not standardized, it preserves the original scale of the variables. This can be helpful when the units of measurement carry important information or when you want to maintain the original context of the data.\nNon-linear Relationships: If you suspect a non-linear relationship between variables, covariance can still provide insights into the direction and magnitude of the relationship, albeit without quantifying the strength in a standardized manner.\n\n\n\n\n\n\n\n\n\nExample:\n\n\n\n\n\nBy calculating the covariance between height and weight, you can obtain a measure of how the two variables vary together.\n\n\n\n\n\n\n\n\n\nExample:\n\n\n\n\n\nIf you were interested in comparing the relationship between height and weight with other datasets or variables, or if you wanted a standardized measure of the strength of the relationship, then calculating the correlation coefficient would be more appropriate. The correlation coefficient would provide a standardized measure between -1 and 1, allowing for easier comparison and interpretation across different contexts."
  },
  {
    "objectID": "Terms & definitions.html#correlation-vs-cointegration",
    "href": "Terms & definitions.html#correlation-vs-cointegration",
    "title": "1  Terms & definitions",
    "section": "1.3 Correlation vs cointegration",
    "text": "1.3 Correlation vs cointegration\nBoth are statistical concepts used to measure the relationship between variables.\n\nCointegration measures whether the variables tend to move together over time, despite possibly having short-term fluctuations.\n\n\n\n\n\n\n\nExample:\n\n\n\n\n\nA man leaves the pub with his dog.\nWhen the man and the dog first leave the pub, their paths are correlated. They generally move in the same direction, but the distance between the dog and the man has no actual limit. It increases at times, decreases at times, but is generally random and poorly defined. The direction of the two, however, is generally the same.\nWhen the man leashes his dog to cross the road, they become cointegrated. Now, while their direction is still the same, their distance from one another is finite. The dog cannot move beyond the length of the leash from the man. (“What Is the Difference Between Correlation and Cointegration? Is Cointegration a Good Measure of Risk?” n.d.)"
  },
  {
    "objectID": "Terms & definitions.html#p-value",
    "href": "Terms & definitions.html#p-value",
    "title": "1  Terms & definitions",
    "section": "1.4 P-value",
    "text": "1.4 P-value\nHow surprised should we be by a result if the null hypothesis is true\n\n\n\n\n\n\nExample:\n\n\n\n\n\nA p-value of 0.001 indicates that if the null hypothesis tested were indeed true, then there would be a one-in-1,000 chance of observing results at least as extreme. It suggests that the observed data is unlikely to have occurred by random chance alone, and you may reject the null hypothesis in favor of the alternative/new hypothesis"
  },
  {
    "objectID": "Terms & definitions.html#bias-and-variance",
    "href": "Terms & definitions.html#bias-and-variance",
    "title": "1  Terms & definitions",
    "section": "1.5 Bias and Variance",
    "text": "1.5 Bias and Variance\nWe search a model with low bias and low variance\nBias: The inability for a machine learning method to capture the true relationship is called bias (can’t capture underlying patterns = underfit)\nVariance: The difference in fits between train set and test set (can’t generalize on unseen data = overfit)\nRed model: low variance: This model has low variance if a new point it would not substantially change the model fit (leads to under-fitting) high bias: Ineffective at modeling the data\nBlue model: high variance: Small perturbations in the data will significantly change the model fit (leads to over-fitting) low bias: More complex and flexible allowing to model very good the data\n\n\n\nvariance-bias trade-off"
  },
  {
    "objectID": "Terms & definitions.html#parametric-non-parametric",
    "href": "Terms & definitions.html#parametric-non-parametric",
    "title": "1  Terms & definitions",
    "section": "1.6 Parametric / Non-Parametric",
    "text": "1.6 Parametric / Non-Parametric\nML models can be divided into two types:\n– Parametric : uses a fixed number of parameters with respect to sample size – Non-Parametric : uses a flexible number of parameters and doesn’t make particular assumptions on the data"
  },
  {
    "objectID": "Terms & definitions.html#residual",
    "href": "Terms & definitions.html#residual",
    "title": "1  Terms & definitions",
    "section": "1.7 Residual",
    "text": "1.7 Residual\nDifference between the observed value and the predicted value"
  },
  {
    "objectID": "Terms & definitions.html#regularization",
    "href": "Terms & definitions.html#regularization",
    "title": "1  Terms & definitions",
    "section": "1.8 Regularization",
    "text": "1.8 Regularization\nIt make the prediction less sensitive to the training data and reduces overfitting."
  },
  {
    "objectID": "Terms & definitions.html#quantiles",
    "href": "Terms & definitions.html#quantiles",
    "title": "1  Terms & definitions",
    "section": "1.9 Quantiles",
    "text": "1.9 Quantiles\nMedian: The 50th percentile, which divides the data into two equal halves. Half of the data points are below the median, and half are above it.\nQuartiles: The 25th, 50th, and 75th percentiles are called the first quartile (Q1), the median (Q2), and the third quartile (Q3), respectively. They divide the data into four equal parts, each containing 25% of the data.\nPercentiles: These are any values that divide the data into 100 equal parts. For example, the 20th percentile is the value below which 20% of the data falls, and the 80th percentile is the value below which 80% of the data falls."
  },
  {
    "objectID": "Terms & definitions.html#greedy-algorithms",
    "href": "Terms & definitions.html#greedy-algorithms",
    "title": "1  Terms & definitions",
    "section": "1.10 Greedy algorithms",
    "text": "1.10 Greedy algorithms\nWhen a procedure is greedy, it means that it does not reevaluate past solutions, it makes locally optimal choices at each step with the hope of finding a global optimum. Advantages: simplify the decision-making process and lead to a solution that is close to optimal. Disadvantages: not always guarantee finding the globally optimal solution"
  },
  {
    "objectID": "Terms & definitions.html#extrapolation",
    "href": "Terms & definitions.html#extrapolation",
    "title": "1  Terms & definitions",
    "section": "1.11 Extrapolation",
    "text": "1.11 Extrapolation\nExtrapolation is commonly defined as using a model to predict samples that are outside the range of the training data. To know if we can trust our model we need to compare the predictor space between the training data and new data.\n\nThis can be done using PCA. If the training data and new data are generated from the same mechanism, then the projection of these data will overlap in the scatter plot. However, if the training data and new data occupy different parts of the scatter plot, then the data may not be generated by the same mechanism and predictions for the new data should be used with caution.\n\n\n\n\nPCA and Extrapolation: the training and testing data appear to occupy the same space as determined by these components.\n\n\n\nThis can also be done using the following algorithm\n\n\n\n\nAlgorithm and Extrapolation\n\n\n\n\n\n\n“What Is the Difference Between Correlation and Cointegration? Is Cointegration a Good Measure of Risk?” n.d. Quora. https://www.quora.com/What-is-the-difference-between-correlation-and-cointegration-Is-cointegration-a-good-measure-of-risk. Accessed July 17, 2023."
  },
  {
    "objectID": "Choose model.html#supervised-models",
    "href": "Choose model.html#supervised-models",
    "title": "2  Choose the machine learning model",
    "section": "2.1 Supervised models",
    "text": "2.1 Supervised models\n\n\n\n\n\n\n  \n    \n    \n      Model\n      Mode\n      Allows n &lt; p\n      Pre-processing\n      Interpretable\n      Automatic feature selection\n      Number of tuning parameters\n      Robust to predictor noise\n      Computation time\n      Section\n    \n  \n  \n    Linear regression\nregression\nno\ncentering and scaling, remove near-zero predictors, remove highly correlated predictors, remove non-informative predictors\nhigh\nno\n0\nlow\nfast\nSection\n    Partial least squares\nregression\ndepends\ncentering and scaling, remove non-informative predictors\nlow\nyes*\n1\nNA\nfast\nSection\n    Logistic regression\nclassification\nno\ncentering and scaling, remove near-zero predictors, remove highly correlated predictors, remove non-informative predictors\nhigh\nno\n0\nlow\nfast\nSection\n    Ridge regression\nregression & classification\nno\ncentering and scaling, remove near-zero predictors, remove non-informative predictors\nhigh\nno\n1\nlow\nfast\nSection\n    Elastic net/Lasso\nregression & classification\nyes\ncentering and scaling, remove near-zero predictors\nhigh\nyes\n1-2\nlow\nfast\nSection\n    Support vector machines\nregression & classification\nyes\ncentering and scaling, remove non-informative predictors\nlow\nno\n1-3\nlow\nslow\nSection\n    MARS/FDA\nregression & classification\nyes\n\nmedium\nyes\n1-2\nmid\nmid\nSection\n    K-nearest neighbors\nregression & classification\nyes\ncentering and scaling, remove near-zero predictors\nlow\nno\n1\nmid\nfast\nSection\n    Random forest\nregression & classification\nyes\n\nlow\ndepends\n0-1\nhigh\nslow\nSection\n    Boosted trees\nregression & classification\nyes\n\nlow\nyes\n3\nhigh\nslow\nSection\n    Neural networks\nregression & classification\nyes\ncentering and scaling, remove near-zero predictors, remove highly correlated predictors, remove non-informative predictors\nlow\nno\n2\nlow\nslow\nSection\n    LDA\nclassification\nno\nremove near-zero predictors\nmedium\nno\n0-2\nlow\nfast\nSection\n    Naive Bayes\nclassification\nyes\nremove near-zero predictors\nlow\nno\n0-1\nmid\nmid\nSection\n    C5.0\nclassification\nyes\n\nmedium\nyes\n0-3\nhigh\nlow\nSection\n  \n  \n  \n\n\n\n\n(Kuhn and Johnson 2013)"
  },
  {
    "objectID": "Choose model.html#model-process-for-supervised-models",
    "href": "Choose model.html#model-process-for-supervised-models",
    "title": "2  Choose the machine learning model",
    "section": "2.2 Model Process for supervised models",
    "text": "2.2 Model Process for supervised models\n\n\n\nModeling Process: four distinct models are being evaluated. When modeling data, there is almost never a single model fit or feature set that will immediately solve the problem. The process is more likely to be a campaign of trial and error to achieve the best results. The effect of feature sets can be much larger than the effect of different models. The interplay between models and features is complex and somewhat unpredictable. With the right set of predictors, is it common that many different types of models can achieve the same level of performance.\n\n\n\nExploratory data analysis (evaluating simple summary measures or identifying predictors that have strong correlations with the outcome / how the predictors will be represented)\nPre-processing the predictor data\nEstimating model parameters\nSelecting predictors for the model\nEvaluating model performance\nFine tuning class prediction rules (via ROC curves, etc.)\noptimization routines (e.g., Nelder–Mead simplex method = direct methods) can later be use to search the optimal key value (e.g., determine possible mixtures with improved compressive strength)\nEDA can be conducted on the model results (e.g., residual analysis)"
  },
  {
    "objectID": "Choose model.html#unsupervised-models",
    "href": "Choose model.html#unsupervised-models",
    "title": "2  Choose the machine learning model",
    "section": "2.3 Unsupervised models",
    "text": "2.3 Unsupervised models\n\n2.3.1 Clustering models\nGroups similar data points together based on distance\nk-Means\n\nHierarchical Clustering\nHierarchical clustering focuses on the similarities between the individual instances and how similarities link them together. Tells you pairwise what two things are most similar.\n\n\n\n2.3.2 Autoencoders\nNeural networks that can denoise or smooth the predictor values and focus on capturing important features in the data.\n\n\n\n\n\n\nUse case\n\n\n\nAnomaly Detection: The decoder struggles to capture anomalous patterns, and the reconstruction error acts as a score to detect anomalies. Identifies unusual patterns that differ from the majority of the data. Assumes that anomalies are:\n\nRare : the minority class that occurs rarely in the data\n\nDifferent : have feature values that are very different from normal observations\n\nImage processing dimension reduction information retrieval\n\n\n\n\n\n\nKuhn, Max, and Kjell Johnson. 2013. Applied Predictive Modeling. New York, NY: Springer. https://doi.org/10.1007/978-1-4614-6849-3."
  },
  {
    "objectID": "Pre-processing.html#transform-the-predictorindependent-variables",
    "href": "Pre-processing.html#transform-the-predictorindependent-variables",
    "title": "3  Pre-processing",
    "section": "3.1 Transform the predictor/independent variables",
    "text": "3.1 Transform the predictor/independent variables\nImportant to transform independent variables that are skewn or containt outliers for models sensitive to them See Table\n\n3.1.1 Centering and scaling\nAdvantages: Improve the numerical stability to minimize potential numerical errors\nDisadvantage: Loss of interpretability\n\n\n3.1.2 Resolve Skewness\nSkewed data: Ratio of the highest value to the lowest value is greater than 20 have significant skewness. (e.g., if the lowest income is $1,000 and the highest income is $20,000 or more, the dataset exhibits significant skewness due to the large ratio between the highest and lowest values.)\n\nLeft Skew: Mean &lt; Median ≤ Mode (value that appears most frequently)\nRight Skew: Mean &gt; Median ≥ Mode\n\nTechniques:\n\nlog\nsquare root\ninverse\nBox and Cox (can only be applied to data that is strictly positive)\n\n\n\n3.1.3 Resolve Outliers\nTechniques:\n\nSpatial sign\n\n\n\n\n\n\n\nWarning\n\n\n\n\nit is important to center and scale the predictor data prior to using this transformation\nAvoid removing predictor variables after applying this technique as spatial sign transformation transforms predictors as a group.\n\n\n\n\n\n3.1.4 Data Reduction and Signal/Feature Extraction\nThese methods reduce the data by generating a smaller set of predictors that seek to capture a majority of the information in the original variables.\n\n3.1.4.1 PCA (unsupervised technique)\nThe number of components to retain is choosen by creating a scree plot (Fig 1)\nFor most data sets, the first few PCs will summarize a majority of the variability, and the plot will show a steep descent; variation will then taper off for the remaining components. Generally, the component number prior to the tapering off of variation is the maximal component that is retained. In an automated model building process, the optimal number of components can be determined by cross-validation (see Resampling Techniques).\n\n\n\nFigure 1: The variation tapers off at component 5. Using this rule of thumb, four PCs would be retained\n\n\nAdvantages: The primary advantage is that it creates components that are uncorrelated. The smaller dataset may be easier to deal with or to process. Moreover, the smaller dataset may better reveal the information.\nDisadvantage:\n\nLoss of interpretability.\nUnsupervised technique which means that PCA it does not consider the modeling objective or response variable when summarizing variability. PCA can generate components that summarize characteristics of the data that are irrelevant to the underlying structure of the data and also to the ultimate modeling objective. If the predictive relationship between the predictors and response is not connected to the predictors’ variability, then the derived PCs will not provide a suitable relationship with the response. In this case, a supervised technique, like PLS, will derive components while simultaneously considering the corresponding response.\nData should be linearly related\n\n\n\n\n\n\n\nWarning\n\n\n\nFirst transform skewed predictors and then center and scale the predictors prior to performing PCA. Centering and scaling enables PCA to find the underlying relationships in the data without being influenced by the original measurement scales.\n\n\n\n\n\n\n\n\nPro tips\n\n\n\n\nIf PCA has captured a sufficient amount of information in the data. Visually examining the principal components is a critical step for assessing data quality and gaining intuition for the problem. To do this, the first few principal components can be plotted against each other and the plot symbols can be colored by relevant characteristics, such as the class labels.\n\nCheck for blatant outliers that may prompt a closer examination of the individual data points\nCheck for clusters of samples (for classification problems; Try other models that could better accommodate the data to have a final conclusion)\nChecks loadings to characterize which predictors are associated with each component (Loadings close to zero indicate that the predictor variable did not contribute much to that component; Fig 2)\nCheck for multicollinearity (substantial correlation between multiple predictors): For example, if the first principal component accounts for a large percentage of the variance, this implies that there is at least one group of predictors that represent the same information. For example, Fig 1 indicates that the first 3–4 components have relative contributions to the total variance. This would indicate that there are at least 3–4 significant relationships between the predictors. Colinearity can increase the model variance\n\nIf the percentages of variation explained are not large (e.g., less than 48 %) for the first three components, it is important not to over-interpret the resulting image.\n\n\n\n\n\n\nFigure 2: loadings for the first three components in the cell segmentation data. For the first principal component, the loadings for the first channel are on the extremes. This indicates that channel 1 have the largest effect on the first principal component and by extension the predictor values. Also note that the majority of the loadings for the third channel are closer to zero for the first component. Conversely, the third principal component is mostly associated with the third channel while the first channel plays a minor role here.\n\n\n\n\n3.1.4.2 PLS (supervised technique)\nDerive components while simultaneously considering the corresponding response.\n\n\n\n3.1.5 Dealing with Missing Values\nBefore proceeding: It is important to understand why the values are missing to check for informative missing using visualization such as heatmap or cooccurrence plot (for smaller data) or by plotting the first two scores from a PCA model of the missing data indicator matrix (for larger data sets). Informative missingness can induce significant bias in the model.\nExamples: - people are more compelled to rate products when they have strong opinions (good or bad) - The tested drug was extremely ineffective or had significant side effects. The patient may be likely to miss doctor visits or to drop out of the study.\n\n1) Remove predictors from models if the percentage of missing data is substantial\n2) For large data sets, removal of samples based on missing values is not a problem\n3 If we do not remove the missing data:\n\nUse tree-based techniques, which account for missing data\ncode missing data as “missing”\nimpute missing data using information in the training set predictors (for small to moderate amounts of missingness). This amounts to a predictive model within a predictive model. If we are using resampling to select tuning parameter values or to estimate performance, the imputation should be incorporated within the resampling. This will increase the computational time for building models, but it will also provide honest estimates of model performance.\n\nK-nearest neighbor model: Advantages: The imputed data are confined to be within the range of the training set values. Disadvantages: The entire training set is required every time a missing value needs to be imputed and The number of neighbors is a tuning parameter\nLinear regression model: between a predictor with few missing points strongly associated with the predictor with missing data (correlation / visualizations / PCA.\nBagged trees\n\n\n\nNOTES Censored data ≠ Missing data: The exact value is missing but something is known about its value. For example: If a customer has not yet returned a movie to blockbuster, we do not know the actual time span, only that it is as least as long as the current duration.\n\nFor inference models: the censoring is usually taken into account in a formal manner by making assumptions about the censoring mechanism\nFor predictive models, it is more common to treat these data as simple missing data or use the censored value as the observed value.\n\n\n\n3.1.6 Removing Predictors\nAdvantages: Does not compromise the performance and stability of the model. Decreased computational time and complexity. Lead to a more parsimonious and interpretable model\n\nRemove near-zero predictors (e.g., predictor variable where the percentage of unique values is low &lt; 10% = unique values/total values and The ratio of the frequency of the most prevalent value to the frequency of the second most prevalent value is large &gt; 20).\nRemove problematic predictors with degenerate distributions as some models can be crippled by them\nRemove highly correlated predictors as both mesure the same underlaying information. For linear regressions use VIF for other sensitive models ensure that all pairwise correlations are below 0.75 threshold\nFeature Selection (see Feature Selection section)\n\n\n\n3.1.7 Adding Predictors\nSee Feature_engineering section"
  },
  {
    "objectID": "Model Tuning.html",
    "href": "Model Tuning.html",
    "title": "4  Model Tuning",
    "section": "",
    "text": "5 Choosing between models\nOnce the settings for the tuning parameters have been determined for each model, the question remains: how do we choose between multiple models?"
  },
  {
    "objectID": "Model Tuning.html#data-splitting-method",
    "href": "Model Tuning.html#data-splitting-method",
    "title": "4  Model Tuning",
    "section": "4.1 Data splitting method",
    "text": "4.1 Data splitting method\n\n4.1.1 Nonrandom approaches to splitting the data\n\n\n\n\n\n\nUse case\n\n\n\n\nIn time-series data, it’s crucial to maintain the temporal order. Randomly shuffling the data can break the temporal dependencies and result in unrealistic evaluations of a model’s performance.\nIf your data has spatial dependencies, such as in geospatial analysis, maintaining the spatial structure during data split becomes essential. Random splitting might scatter spatially related data points across different sets, leading to poor model generalization.\nIn situations where data privacy is a concern, you might want to ensure that certain sensitive records or individuals are not included in the training set. A nonrandom approach allows for more control over the inclusion or exclusion of specific data points.\nSpecific Use Cases:\n\nIn spam filtering; it is more important for the model to catch the new spamming techniques rather than prior spamming schemes.\nIf a model was being used to predict patient outcomes, the model may be created using certain patient sets (e.g., from the same clinical site or disease stage), and then tested on a different sample population to understand how well the model generalizes.\nIn chemical modeling for drug discovery, new “chemical space” is constantly being explored. We are most interested in accurate predictions in the chemical space that is currently being investigated rather than the space that was evaluated years prior.\n\n\n\n\n\n\n4.1.2 Random sampling methods\n\n4.1.2.1 Simple random sample\nThe simplest way to split the data randomly into a training and test.\nDisadvantage: limited ability to characterize the uncertainty in the results.\n\n\n4.1.2.2 Simple k-Fold Cross-Validation:\nThe samples are randomly partitioned into k sets of roughly equal size. A model is fit using the all samples except one subset. The held-out samples are predicted by this model and used to estimate performance measures. The first subset is returned to the training set an procedure repeats with the next subset held out, and so on. Performance estimates, are calculated from each set of held-out samples and then averaged.\nNOTES: The choice of k is usually 5 or 10, but there is no formal rule. The bias is smaller for k = 10 than k = 5. As k gets larger, the difference in size between the training set and the resampling subsets gets smaller. But larger values of k are more computationally burdensome.\nAdvantage: Low computational costs. Disadvantage: k-fold cross-validation generally has high variance compared to other methods (only for small training sets). USE: If sample sizes are large (&gt; 10 000) and we want to choose tuning parameters\n\n\n\n3-Fold Cross-Validation\n\n\n\n\n4.1.2.3 Repeated k-Fold Cross-Validation\nRepeated k-fold cross-validation replicates Simple k-Fold Cross-Validation multiple times.\nAdvantage: Increase the precision of the estimates while still maintaining a small bias. The bias and variance properties are good Disdvantage: Larger computational costs if large sample size.\nUSE: with k = 10; If the samples size is small (&lt; 1000 obs) and we want to choose tuning parameters. For example, if 10-fold cross-validation was repeated five times, 50 different held-out sets would be used to estimate model efficacy.\n\n\n4.1.2.4 leave-one-out Cross-Validation / LOOCV:\nFits as many models as there are samples in the training set, should only be considered when the number of samples is very small.\nNOTE: leave-one-out and k =10-fold cross-validation yielded similar results, indicating that k = 10 is more attractive from the perspective of computational efficiency.\n\n\n4.1.2.5 leave-group-out Cross-Validation / Repeated training/test splits / Monte Carlo cross-validation:\nSame as k-fold cross-validation except that samples can be represented in multiple held-out subsets. Also, the number of repetitions is usually larger than in k-fold cross-validation. the bias of the resampling technique decreases as the amount of data in the subset approaches the amount in the modeling set. A good rule of thumb is about 75–80 %. Higher proportions are a good idea if the number of repetitions is large.\nNOTES: Increase the number of repetition can allow to increase the proportion of data in the train set and decreasing the uncertainty of the performance estimates. To get stable estimates of performance, it is suggested to choose a larger number of repetitions (say 50–200)\n\n\n\nleave-group-out Cross-Validation\n\n\n\n\n4.1.2.6 The Bootstrap:\nEach train subset is the same size as the original and can contain multiple instances of the same data point (taken with replacement). Samples not selected by the bootstrap (“out-of-bag” samples) are predicted and used to estimate model performance\nAdvantage: error rates have less uncertainty than k-fold cross-validation. Very low variance. Disadvantage: On average, 63.2 % of the data points the bootstrap sample are represented at least once, so this technique has bias. similar to k-fold cross-validation when k ≈ 2. If the training set size is small, this bias may be problematic, but will decrease as the training set sample size becomes larger. USE: If the goal is to choose between models (boosted trees vs support vector machines…), as opposed to getting the best indicator of performance\n\n\n\nBootstrap\n\n\n\nThe Bootstrap 632 method\nAdvantage: The modified bootstrap estimate reduces the bias.\nDisadvantage: The estimate is unstable with small samples sizes. This estimate can also result in unduly optimistic results when the model severely over-fits the data, since the apparent error rate will be close to zero.\nThe Bootstrap 632+ method Advantage: Allows to adjust the bootstrap 632 method estimates\n\n\n\n4.1.2.7 Stratified random\nTo account for the outcome when splitting the data. Applies random sampling within subgroups (such as the classes or is outcomes are numbers the numeric values are broken into similar groups (e.g., low, medium, and high)).\n\n\n4.1.2.8 Maximum dissimilarity sampling\nThe data is split on the basis of the predictor values."
  },
  {
    "objectID": "Model Tuning.html#choosing-the-best-tuning-parameters",
    "href": "Model Tuning.html#choosing-the-best-tuning-parameters",
    "title": "4  Model Tuning",
    "section": "4.2 Choosing the best tuning parameters",
    "text": "4.2 Choosing the best tuning parameters\n\nPick the settings associated with the numerically best performance estimates but Disadvantage: lead to models that are overly complicated (see Figure fitting graph)\n\n - Pick simpler models that provide acceptable performance (relative to the numerically optimal settings)\n\nThe “one-standard error” method: pick the simpler model within a single standard error of the numerically best value. In table below we would pick cost value of 2.\nthe “percent decrease in performance” method: pick the simplier model that is within a certain tolerance of the numerically best value. (e.g., The percent decrease in performance could be quantified by (X − O)/O where X is the performance value and O is the numerically optimal value. For example, in Figure below the best accuracy value across the profile was 75 %. If a 4 % loss in accuracy was acceptable as a trade-off for a simpler model, accuracy values greater than 71.2 % would be acceptable. For the profile in Figure below, a cost value of 1 would be chosen using this approach.)\n\n\n\n\nCross-validation accuracy"
  },
  {
    "objectID": "Model Tuning.html#metrics-performance-measures",
    "href": "Model Tuning.html#metrics-performance-measures",
    "title": "4  Model Tuning",
    "section": "4.3 Metrics / performance measures",
    "text": "4.3 Metrics / performance measures\n\n4.3.1 For models predicting a categorical outcome\n\n4.3.1.1 Accuracy based metrics\nA good model has generally a metric above 0.7 / 70%\n\nAccuracy:\n\n\nHigher Value: Better performance (values form 0 to 1)\nWhen to use: Use when the classes are balanced (e.g., Suppose the rate of this disorder1 in fetuses is approximately 1 in 800 or about one-tenth of one percent. A predictive model can achieve almost perfect accuracy by predicting all samples to be negative for Down syndrome.), and misclassification of different classes has similar consequences.\nAdvantage: Simple and easy to interpret.\nDisadvantage: Can be misleading when classes are imbalanced / make no distinction about the type of errors being made\nDescription: Accuracy measures the proportion of correct predictions out of all predictions made by the model.\nExample: Suppose you have a binary classification problem to identify whether an email is spam or not. If your model has an accuracy of 90%, it means it correctly classified 90% of the emails.\nCalculation: (True Positives + True Negatives) / (True Positives + True Negatives + False Positives + False Negatives) (Number of Correct Predictions) / (Total Number of Predictions)\nNotes:\n\nWhen evaluating the accuracy of a model, the baseline accuracy rate to beat would be the percentage which could be achieve by simply predicting all samples to the dominant category (e.g., In the data set, 70 % were rated as having good, accuracy rate to beat would be 70 % which is the no-information rate).\nerror rate: (Number of Incorrect Predictions) / (Total Number of Predictions)\n\n\n1.5) Kappa:\n\nHigher Value: Better performance (values from -1 to 1; 0.30 to 0.50 indicate reasonable agreement)\nWhen to use: Rather than calculate the overall accuracy and compare it to the no-information rate, Kappa can be used that take into account the class distributions of the training set samples.\nAdvantage: Takes into account the accuracy that would be generated simply by chance.\nDisadvantage: NA\nDescription: assess the agreement between two raters\nExample: 0 means there is no agreement between the observed and predicted classes, while a value of 1 indicates perfect concordance of the model prediction and the observed classes. Negative values indicate that the prediction is in the opposite direction of the truth, but large negative values seldom occur, if ever, when working with predictive models.\nCalculation: Kappa = O − E / 1−E: O is the observed accuracy and E is the expected accuracy based on the marginal totals of the confusion matrix.\nNote: The Kappa statistic can also be extended to evaluate concordance in problems with more than two classes. When there is a natural ordering to the classes (e.g., “low,”“medium,” and “high”), an alternate form of the statistic called weighted Kappa can be used to enact more substantial penalties on errors that are further away from the true result. For example, a “low” sample erroneously predicted as “high” would reduce the Kappa statistic more than an error were “low” was predicted to be “medium.” See (Agresti 2002)for more details.\n\n\nPrecision:\n\n\nHigher Value: Better performance (good = 0.7)\nWhen to use: Use when the cost of false positives is high (e.g., medical diagnosis, fraud detection).\nAdvantage: Focuses on the relevance of positive predictions.\nDisadvantage: Ignores true negatives and may not be suitable for imbalanced datasets.\nDescription: Precision is the proportion of true positive predictions (correctly predicted positive class) out of all positive predictions made by the model.\nExample: In the spam email example, if your model has a precision of 80%, it means that out of all the emails it predicted as spam, 80% of them were actually spam.\nCalculation: True Positives / (True Positives + False Positives)\n\n\nSensitivity / True positive rate / Recall:\n\n\nHigher Value: Better performance\nWhen to use: Use when the cost of false negatives is high (e.g., medical diagnosis, safety-critical applications).\nAdvantage: Focuses on the completeness of positive predictions (includes true positive and false negatives).\nDisadvantage: Ignores true negatives. For many classification problems, sensitivity may be misleading specially under class imbalance. Since a better cutoff may be possible, an analysis of the ROC curve can lead to improvements in these metrics. Consequently, performance metrics that are independent of probability cutoffs are likely to produce more meaningful contrasts between models.\nDescription: Is the proportion of true positive predictions out of all actual positive instances in the dataset.\nExample: % of people with heart diseases were correctly identify by the model\nCalculation: TP / (TP + FN)\nNotes:\n\nIf the data set includes more events than nonevents, the sensitivity can be estimated with greater precision than the specificity and sensitivity shouls be use to choose between models.\nWhen we want to make unconditional evaluations of the data: know for example what are the chances that … (e.g., If PPV = 0.75 this means that out of all the individuals who tested positive for Disease X, 75% of them actually have the disease, while the remaining 25% are false positives) we can use positive predicted value (PPV = Sensitivity × Prevalence / (Sensitivity × Prevalence) + ((1 − Specificity) × (1 − Prevalence))) IMPOTANT: Predictive values are not often used to characterize the model. There are several reasons why, most of which are related to prevalence. First, prevalence is hard to quantify.\n\n\n\nSpecificity / True Negative Rate:\n\n\nHigher Value: Better performance\nWhen to use: Use when you want to focus on correctly identifying negative cases and the cost of false positives is high.\nAdvantage: Focuses on the negative class and avoids false positives. Can be misleading specially under class imbalance.\nDisadvantage: Ignores true positives.\nDescription: Specificity measures the proportion of true negative predictions out of all actual negative samples.\nExample: % of people without heart diseases were correctly identify by the model\nCalculation: True Negatives / (True Negatives + False Positives);\nNotes:\n\nWhen we want to make unconditional evaluations of the data: know for example what are the chances that … (If NPV = 0.966, this means that out of all the individuals who tested negative for Disease X, 96.6% of them truly do not have the disease, while the remaining 3.4% are false negatives (individuals who have the disease but were incorrectly identified as negative).) we can use negative predicted value (NPV = Specificity × (1 − Prevalence) / (Prevalence × (1 − Sensitivity)) + (Specificity × (1 − Prevalence))). IMPORTANT: idem\nFalse-positive rate : one minus the specificity\n\n\n4.3) Youden’s J Index\n\nHigher Value: Better performance\nWhen to use: Use when you want a measure that reflects the false-positive and false-negative rates and summarize the magnitude of both types of errors.\nAdvantage: Focuses on the negative class and avoids false positives.\nDisadvantage: Ignores true positives and may not be suitable for imbalanced datasets.\nDescription: measures the proportions of correctly predicted samples for both the event and nonevent groups.\nExample: % of people without heart diseases were correctly identify by the model\nCalculation: J = Sensitivity + Specificity − 1\n\n\nF1 Score:\n\n\nHigher Value: Better performance\nWhen to use: Use when classes are imbalanced and there is a trade-off between precision and recall.\nAdvantage: Incorporates both precision and recall into a single metric.\nDisadvantage: Ignores true negatives, which can be important in some cases. May not be ideal for highly imbalanced datasets.\nDescription: F1 score is the harmonic mean of precision and recall, providing a balance between the two.\nExample: Let’s say your model has an F1 Score of 0.75, it means there is a balanced trade-off between correctly identifying positive samples and minimizing false positives.\nCalculation: 2 * (Precision * Recall) / (Precision + Recall)\n\n\n\n4.3.1.2 Class probabilities\nClass probabilities potentially offer more information about model predictions than the simple class value. This\n4.7) ROC:\n\nHigher Value: Better performance (A perfect model that completely separates the two classes would have 100 % sensitivity and specificity / A completely ineffective model would result in an ROC curve that closely follows the 45◦ diagonal line and would have an area under the ROC curve of approximately 0.50.) Area under the curve can be used as a quantitative measure of performance\nWhen to use: Helpful tool for choosing a threshold that appropriately maximizes the trade-off between sensitivity and specificity (how many false positive we are willing to accept) (e.g., Lowering the threshold (aka 50%) can we improve the sensitivity to capture more true positives). Make a quantitative assessment of the model\nAdvantage: the curve is insensitive to disparities in the class proportions. Metrics that is independent of probability cutoffs\nDisadvantage: disadvantage of using the area under the curve to evaluate models is that it obscures information (i.e., the curves cross both AUC can be the same).\nDescription: AUC-ROC measures the area under the receiver operating characteristic curve, which plots the true positive rate (recall) against the false positive rate (1 - Specificity) at various classification thresholds (10%, 20%… 50% = commonly used).\nExample: An AUC-ROC score of 0.85 indicates that the model has an 85% chance of correctly ranking a randomly chosen positive instance higher than a randomly chosen negative instance.\nCalculation: AUC-ROC can be calculated using various methods, such as the trapezoidal rule or Mann-Whitney U statistic.\nNotes:\n\nWe can use the partial area under the ROC curve as a technique to summarize these curves that focuses on specific parts of the curve.\nROC technique can be extended to fit three or more classes problems\n\n\n\nRecall - Precision Curve:\n\n\nWhen to use: When data is imbalanced as it focuses on the correct prediction of the minority class\nDescription: Plot precision in X axis and recall in Y axis\n\n\nLift Charts:\n\n\nHigher Value: Better performance (Figure)\nWhen to use: To assess the ability of a model to detect events in a data set with two classes and allow us to choose a quasithreshold for a model.\nAdvantage: Easy connect the model to the buisness value: Using the lift plot, the expected profit can be calculated for each point on the curve to determine if the lift is sufficient to beat the baseline profit\nDisadvantage: Bad for comparing different models\nDescription: The lift chart plots the cumulative gain/lift against the cumulative percentage of samples that have been screened\nExample: Figure shows the best and worse case lift curves for a data set with a 50 % event rate. The non-informative model has a curve that is close to the 45◦ reference line, meaning that the model has no benefit for ranking samples. The other curve is indicative of a model that can perfectly separate two classes. At the 50 % point on the x-axis, all of the events have been captured by the model.\nCalculation: NA\nNotes:\n\nThe section of the curve associated with the highest-ranked samples should have an enriched true-positive rate and is likely to be the most important part of the curve.\n\n\n\n\n\nLift Charts\n\n\nNOTES:\n\nIt is important to test whether the estimated class probabilities are reflective of the true underlying probability of the sample (well-calibrated Probabilities) using a calibration plot. This plot shows some measure of the observed probability of an event versus the predicted class probability. One approach for creating this visualization is to score a collection of samples with known outcomes (preferably a test set) using a classification model. The next step is to bin the data into groups based on their class probabilities. For example, a set of bins might be [0, 10 %], (10 %, 20 %], …, (90 %, 100 %]. For each bin, determine the observed event rate. Suppose that 50 samples fell into the bin for class probabilities less than 10 % and there was a single event. The midpoint of the bin is 5 % and the observed event rate would be 2 %. The calibration plot would display the midpoint of the bin on the x-axis and the observed event rate on the y-axis. If the points fall along a 45◦ line, the model has produced well-calibrated probabilities.\n\n\n\n\nA calibration plot of the test set probabilities for random forest and quadratic discriminant analysis models\n\n\n\nIf there are three or more classes, a heat map of the class probabilities can help gauge the confidence in the predictions.\nAn approach to improving classification performance is to create an equivocal or indeterminate zone where the class is not formally predicted when the confidence is not high. (e.g., For a two-class problem that is nearly balanced in the response, the equivocal zone could be defined as 0.50 ± z.Ifz were 0.10, then samples with prediction probabilities between 0.40 and 0.60 would be called “equivocal.” In this case, model performance would be calculated excluding the samples in the indeterminate zone.)\n\n\n\n4.3.1.3 Non-Accuracy-Based Criteria\nWhen accuracy is not the primary goal for the predictive model and we want to quantify the consequences of correct and incorrect predictions (i.e., the benefits and costs)\nExamples:\n\nPredict investment opportunities that maximize return\nImprove customer satisfaction by market segmentation\nLower inventory costs by improving product demand forecasts\nReduce costs associated with fraudulent transactions: For example, in fraud detection, a model might be used to quantify the likelihood that a transaction is fraudulent. Suppose that fraud is the event of interest. Any model predictions of fraud (correct or not) have an associated cost for a more in-depth review of the case. For true positives, there is also a quantifiable benefit to catching bad transactions. Likewise, a false negative results in a loss of income.\n\n\nprofit = Cost/Benefit * TP − Cost/Benefit FP − Cost/Benefit FN\nNEC (normalized expected cost / classification_cost_penalized) = PCF × (1 − TP)+(1− PCF) × FP (between 0 and 1)\n\n\n\n\n4.3.2 For models predicting a numeric outcome\n\nRMSE:\n\n\nHigher Value: Worse performance\nWhen to use: Commonly used to measure the average magnitude of prediction errors.\nAdvantage: Penalizes larger errors more heavily, sensitive to outliers and unit is the same as the target variable, making it more interpretable.\nDisadvantage: Sensitive to outliers.\nDescription: The average distance between the observed values and the model predictions.\nExample: Continuing with the house price prediction example, an RMSE of 100 means that, on average, the predicted house prices deviate from the actual prices by $100.\nCalculation: Squared root of the (sum the residuals (the observed values minus the model predictions) and dividing by the number of samples) For example, if we have actual values [5, 10, 15] and predicted values [6, 12, 10], the MSE would be calculated as ((1^2) + (2^2) + (5^2)) / 3 = 10.\n\n\nMAE:\n\n\nHigher Value: Worse performance\nWhen to use: Suitable when you want to avoid the influence of outliers.\nAdvantage: Not sensitive to outliers as it uses the absolute error.\nDisadvantage: It does not penalize large errors as heavily as RMSE.\nDescription: The Mean Absolute Error measures the average of the absolute differences between predicted and actual values.\nExample: For the house price prediction, an MAE of $50 means that, on average, the predicted house prices deviate from the actual prices by $50.\nCalculation: For example, with the same actual and predicted values, the MAE would be calculated as (|1| + |2| + |5|) / 3 = 2.67.\n\n\nR2:\n\n\nHigher Value: Better performance\nWhen to use: Commonly used to measure the average magnitude of prediction errors. It is a measure of correlation, not accuracy. Bad for predicting a number (accuracy) but good for determining the rank correlation between the observed and predicted values (e.g., pharmaceutical scientists want to find the compounds predicted to be the most biologically active).\nAdvantage: Represents the proportion of the variance in the dependent variable that is predictable from the independent variables.\nDisadvantage: It can be misleading when used with complex models or when the number of predictors is large. It is dependent on the variation in the outcome (e.g., If the range of the houses in the test set was large, say from $60K to $2M, the variance of the sale price would also be very large. One might view a model with a 90 % R2 positively, but the RMSE may be in the tens of thousands of dollars—poor predictive accuracy for anyone selling a moderately priced property)\nDescription: The proportion of the information in the data that is explained by the model\nExample: An R-squared of 0.75 means that 75% of the variance in the house prices can be explained by the model, and the remaining 25% is due to random variation.\nCalculation: Correlation coefficient between the observed and predicted values\nNote: By plotting R2 we can see where the model is overpredict (e.g., low values) and underpredict (e.g., higher values). If this happend depending on the context, this systematic bias in the predictions may be acceptable if the model otherwise works well.\n\n\nR2 adjusted:\n\n\nHigher Value: Better performance\nWhen to use: Helpful when you have multiple predictors and want to account for model complexity.\nAdvantage: It adjusts R-squared for the number of predictors, giving a more reliable assessment of model performance when compared to R-squared.\nDisadvantage: It might not penalize overfitting adequately with large numbers of predictors.\nDescription: R-squared adjusted is similar to R-squared but takes into account the number of predictors in the model. It penalizes models with more predictors if they don’t contribute significantly to the variance explained.\nExample:\nCalculation:\n\n\nMAPE:\n\n\nHigher Value: Worse performance\nWhen to use: Useful when you want to evaluate the performance in percentage terms.\nAdvantage: Represents the percentage difference between predicted and actual values, making it interpretable and independent of the scale of the data.\nDisadvantage: It can be problematic when actual values are close to zero.\nDescription: The Mean Absolute Percentage Error calculates the mean percentage difference between predicted and actual values.\nExample: An MAPE of 10 means that, on average, the predicted house prices deviate from the actual prices by 10%.\nCalculation: For example, if we have actual values [100, 50, 75] and predicted values [90, 40, 70], the MAPE would be calculated as (|(100-90)/100| + |(50-40)/50| + |(75-70)/75|) / 3 ≈ 0.16.\n\n\nEV:\n\n\nHigher Value: Better performance (good value &gt; 0.6)\nWhen to use: Useful to understand how well the model explains the variance in the target variable.\nAdvantage: Measures the proportion of variance explained by the model, similar to R-squared.\nDisadvantage: It might not penalize the model adequately for underfitting or overfitting.\nDescription: The Explained Variance Score quantifies the proportion of variance in the target variable that is explained by the model. It ranges from 0 to 1, with 1 indicating a perfect fit. For example, an EV of 0.85 means that 85% of the variance is explained by the model.\nExample: An EV of 0.9 means that the model explains 90% of the variance in the house prices, leaving 10% unexplained by the model.\nCalculation:\n\n\nMSLE:\n\n\nHigher Value: Worse performance\nWhen to use: Suitable when you want to focus on the ratio of errors rather than their absolute differences. It can be useful when predictions are on a large scale.\nAdvantage: Penalizes underestimation and overestimation proportionally and is less sensitive to large errors.\nDisadvantage: The logarithmic transformation can be problematic for data containing zero or negative values.\nDescription: The Mean Squared Logarithmic Error calculates the mean of the squared logarithmic differences between predicted and actual values.\nExample: For the house price prediction, an MSLE of 0.1 means that, on average, the predicted house prices deviate from the actual prices by 10% when measured on a logarithmic scale.\nCalculation: For instance, if we have actual values [100, 50, 75] and predicted values [110, 40, 80], the MSLE would be calculated as ((log(110) - log(100))^2 + (log(40) - log(50))^2 + (log(80) - log(75))^2) / 3 ≈ 0.015.\n\n\n\n4.3.3 Calculate expected value\nSee Data science for Business chapter\n\n\n4.3.4 Visualizing Model Performance\nIt is often revealing to visualize model behavior under a broad range of conditions.\n\nProfit curves:\n\nUse Case: Basic profit graph can be useful to compare models of interest under a range of conditions. These graphs may be easy to comprehend for stakeholders who are not data scientists, since they reduce model performance to their basic “bottom line” cost or profit.\nDisadvantages: The disadvantage of a profit graph is that it requires that operating conditions be known and specified exactly. With many real-world problems, the operating conditions are imprecise or change over time, and the data scientist must contend with uncertainty. In such cases other graphs may be more useful.\n\n\n\n\n\nProfit curves\n\n\n\nCumulative response curves:\n\nUse Case: To understand what percentage of the population has to be targeted. When costs and benefits cannot be specified with confidence, but the class mix will likely not change, a cumulative response or lift graph is useful. Both show the relative advantages of classifiers, independent of the value (monetary or otherwise) of the advantages.\n\n\n\n\n\nCumulative response curves.\n\n\n\nLift curves:\n\nUse Case: Same as Cumulative response curves.\n\n\n\n\n\nLift curves\n\n\n\nROC/AUC curves:\n\nUse Case: Finally, ROC curves are a valuable visualization tool for the data scientist. Though they take some practice to interpret readily, they separate out performance from operating conditions. In doing so they convey the fundamental trade-offs that each model is making.\n\n\n\n\n\nROC"
  },
  {
    "objectID": "Model Tuning.html#tools",
    "href": "Model Tuning.html#tools",
    "title": "4  Model Tuning",
    "section": "5.1 Tools",
    "text": "5.1 Tools\n\n5.1.1 Paired t-test\nTo evaluate if the differences between models are statistically significant. It is also recommended to plot confidence intervals that were derived using the bootstrap (Figure) for two reasons.\n\nThe interval quantifies the variation in the model but is also reflective of the data. For example, smaller test sets or noise (or mislabeling) in the response can lead to wider intervals.\nFacilitate trade-offs between models. If the confidence intervals for two models significantly overlap, this is an indication of (statistical) equivalence between the two and might provide a reason to favor the less complex or more interpretable model.\n\n\n\n\nA plot of the test set ROC curve AUCs and their associated 95 % confidence intervals\n\n\n\n\n5.1.2 Learning curves\n\nThe learning curve may show that generalization performance has leveled off so investing in more training data is probably not worthwhile; instead, one should accept the current performance or look for another way to improve the model, such as by devising better features\n\n\n\n\nLearning curves for tree induction and logistic regression for the churn problem. As the training size grows (x axis), generalization performance (y axis) improves. Importantly, the improvement rates are different for the two induction technique, and change over time. Logistic regression has less flexibility, which allows it to overfit less with small data, but keeps it from modeling the full complexity of the data. Tree induction is much more flexible, leading it to overfit more with small data, but to model more complex regularities with larger training sets."
  },
  {
    "objectID": "Linear Models and Its Cousins.html#sec-Linear_regression",
    "href": "Linear Models and Its Cousins.html#sec-Linear_regression",
    "title": "5  Linear Regression and Its Cousins",
    "section": "5.1 Ordinary linear regression",
    "text": "5.1 Ordinary linear regression\nFinds parameter estimates that have minimum bias using the NIPALS approach Advantages:\n\nhighly interpretable\nenables us to compute standard errors of the coefficients allowing to assess the statistical significance of each predictor\n\nAssumptions:\n\nLinear relationship and independent observations\nHomoscedasticity error terms have constant variance\nErrors are uncorrelated and normally distributed\nLow multicollinearity"
  },
  {
    "objectID": "Linear Models and Its Cousins.html#penalized-models",
    "href": "Linear Models and Its Cousins.html#penalized-models",
    "title": "5  Linear Regression and Its Cousins",
    "section": "5.2 Penalized models",
    "text": "5.2 Penalized models\nFor classification and regressions: Finds parameter estimates that have lower variance. We introduce bias to reduce variance and avoid overfitting\nUSE: When sample size are small\nAdvantages: reduce variance and increases prediction on the long term\n\n5.2.1 Ridge regression\n\nAdvantages: better at reducing variance in models that contain usefull variables\n\n\n5.2.2 Lasso regression\n\nAdvantages: - better at reducing variance in models that contain useless variables - simplify model\n\n\n5.2.3 Elastic net\n\nUSE: When you don’t know if you have useless variables Advantages: Best of both ridge and lasso"
  },
  {
    "objectID": "Linear Models and Its Cousins.html#sec-Logistic_Regression",
    "href": "Linear Models and Its Cousins.html#sec-Logistic_Regression",
    "title": "5  Linear Regression and Its Cousins",
    "section": "5.3 Logistic Regression",
    "text": "5.3 Logistic Regression\nFor classification only \nAssumptions: - Linear relationship between X and log-odds of Y - Independent observations - Low multicollinearity"
  },
  {
    "objectID": "Linear Models and Its Cousins.html#sec-LDA",
    "href": "Linear Models and Its Cousins.html#sec-LDA",
    "title": "5  Linear Regression and Its Cousins",
    "section": "5.4 Linear Discriminant Analysis",
    "text": "5.4 Linear Discriminant Analysis\nFor classification only\nIt is like PCA but it focuses on maximizing the separability among the known categories.\nCreate an axis (create two axis for three of more categories) that maximizes the distance between the means for the two categories while minimizing the scatter.\nAs in PCA the first axis created (LDA1) by LDA accounts for the most variation between the categories. LDA2 does the second better job, LDA 3 the third best job etc etc…\nNOTE: We can see which variables correlates the most with each LDA"
  },
  {
    "objectID": "Linear Models and Its Cousins.html#sec-PLS",
    "href": "Linear Models and Its Cousins.html#sec-PLS",
    "title": "5  Linear Regression and Its Cousins",
    "section": "5.5 Partial least squares (PLS)",
    "text": "5.5 Partial least squares (PLS)\nFor regression and classification:\nsupervised dimension reduction procedure while PCR (PCA + linear regression) is unsupervised\nUSE: when there are correlated predictors and a linear regression-type solution is desired instead of PCA then linear regression (AKA PCR; If, the variability in the predictor space is not related to the variability of the response, then PCR can have difficulty identifying a predictive relationship when one might actually exist).\nEfficiently for data sets of small-to moderate size (e.g., &lt; 2,500 samples and &lt; 30 predictors)\nPre-prossesing:\n\ncentered and scaled predictors.\nRemove predictors with small PLS regression coefficients and small VIP (&lt;1)\nTo include nonlinear relationships add squared or cubic predictors\nTo include nonlinear relationships splits each predictor into two or more bins for those predictors that are thought to have a nonlinear relationship with the response. Cut points for the bins are selected by the user and are based on either prior knowledge or characteristics of the data. The original predictors that were binned are then excluded from the data set that includes the binned versions of the predictors. (GIFI approach)\n\nTuning: Cross-validation was used to determine the optimal number of PLS components to retain that minimize RMSE # of tuning parameter: PLS has one tuning parameter: the number of components to retain\nFor classification: NOTES: Produce continuous predictions that do not follow the definition of a probability-the predicted values are not necessarily between 0 and 1 and do not sum to 1. Therefore, a transformation (e.g., softmax transformation) must be used to coerce the predictions into “probability-like” values so that they can be interpreted and used for classification.\n\n5.5.1 Algorithmic Variations of PLS\n\nSIMPLS approach\n\nUSE: for data large data sets (e.g., &gt; 2,500 samples and &gt; 30 predictors)\n\nR̈annar et al. (1994) kernel\n\nUSE: when there are more predictors than samples.\n\n\n5.5.2 Partial Least Squares Discriminant Analysis\nFor classification only\n\n\n5.5.3 Nearest Shrunken Centroids\nFor classification only"
  },
  {
    "objectID": "Nonlinear Models.html#sec-Tree_based_models",
    "href": "Nonlinear Models.html#sec-Tree_based_models",
    "title": "6  Nonlinear Models",
    "section": "6.1 Tree-based models",
    "text": "6.1 Tree-based models\nAdvantages:\n\nHandles high-dimensional data well.\nRobust to outliers and noise.\nProvides feature importance measures.\nRequires minimal data preprocessing and is relatively easy to implement.\n\nDisadvantages:\n\nCan be computationally expensive for large datasets.\nMay not perform well on imbalanced datasets.\nLacks interpretability compared to single decision trees.\n\nUSE:\n\nWhen you have a large dataset with a high number of features.\nWhen interpretability is not a top priority.\nWhen you want to build a model that is robust to overfitting.\n\nTuning parameters:\n\nThe number of trees in the forest. Higher values generally improve performance but increase computational time (Common values are between 50 to 500).\nThe maximum depth of each decision tree: Controls the tree’s complexity and potential overfitting (Common values are between 5 to 50).\nThe minimum number of samples required to split an internal node: Higher values prevent overfitting Common values are between 2 to 20).\nThe number of features to consider when looking for the best split: Common values are ‘sqrt’ (square root of total features) or ‘log2’.\n\nFor regression:\n\nFor classification:\n\n\n6.1.1 Random forests\n\n\n\n6.1.2 Boosting trees\nSequentially fits many simple models that account for the previous model’s errors. As opposed to bagging, boosting trains on all the data and combines models using the learning rate α.\n\n6.1.2.1 AdaBoost\nAdvantages:\n\nCan achieve high accuracy by combining multiple weak learners.\nCan handle both classification and regression problems.\nLess susceptible to overfitting compared to individual decision trees.\nCan be combined with any base estimator that accepts sample weights.\n\nDisadvantages:\n\nSensitive to noisy data and outliers.\nCan be computationally expensive as it requires sequentially training multiple learners.\nMay not perform well on highly imbalanced datasets.\n\nUSE:\n\nWhen you have a moderately sized dataset and you want to improve the accuracy of weak learners.\nWhen you want to create a powerful ensemble with different weak learners.\n\nTuning parameters:\n\nn_estimators: The number of boosting stages (weak learners) to be run. Common values are between 50 to 500.\nlearning_rate: The contribution of each weak learner to the final combination. Common values are between 0.01 to 1.0.\nbase_estimator: The base estimator used for boosting. Common choices are decision trees with max_depth set or linear models.\n\n\n\n\n6.1.2.2 XgBoost\nAdvantages:\n\nHighly efficient and scalable, making it suitable for large datasets.\nCan handle missing data and supports regularization to prevent overfitting.\nProvides built-in cross-validation, early stopping, and feature importance.\nOften performs well even with default hyperparameters.\n\nDisadvantages:\n\nCan be sensitive to hyperparameter tuning.\nRequires more careful tuning and validation compared to Random Forest and AdaBoost.\nThe interpretation of feature importance may not be as straightforward as in Random Forest.\n\nUSE:\n\nWhen you have a large dataset and computational efficiency is crucial.\nWhen you need better performance compared to other algorithms on a wide range of problems.\nWhen you can invest time in tuning hyperparameters.\n\nTuning parameters:\n\nn_estimators: The number of boosting rounds. Common values are between 50 to 500.\nlearning_rate: The step size shrinkage used to prevent overfitting. Common values are between 0.01 to 0.3.\nmax_depth: The maximum depth of each tree. Common values are between 3 to 10.\nsubsample: The fraction of samples used for training each tree. Common values are between 0.5 to 1.0.\ncolsample_bytree: The fraction of features used for training each tree. Common values are between 0.5 to 1.0.\n\nFor regression:  \nFor classification:  \n\n\n6.1.2.3 C5.0\nTuning parameters:\n\nmaximum number of leaves: (generally between 8 and 32)\nlearning rate: Value between 0 and 1 (generally 0.1)"
  },
  {
    "objectID": "Nonlinear Models.html#sec-MARS",
    "href": "Nonlinear Models.html#sec-MARS",
    "title": "6  Nonlinear Models",
    "section": "6.2 Multivariate Adaptive Regression Splines (MARS)",
    "text": "6.2 Multivariate Adaptive Regression Splines (MARS)\nMARS uses surrogate features (usually a function of only one or two predictors (second degree) at a time which are broken into two groups and models linear relationships between the predictor and the outcome in each group) instead of the original predictors (like pls and neural networks).\nNOTES - GCV statistic is use to determine the contribution of each feature to the model.\nTuning parameters:\n\nthe degree of the features that are added to the model (number of interaction; e.g., 0-4)\nthe number of retained terms\n\nAdvantages:\n\nthe model automatically conducts feature selection\ninterpretability is high\nrequires very little pre-processing of the data (Correlated predictors do not drastically affect model performance, but they can complicate model interpretation.).\n\nDisadvantages: For MARS models that can include two or more terms at a time, we have observed occasional instabilities in the model predictions where a few sample predictions are wildly inaccurate (perhaps an order of magnitude off of the true value). This problem has not been observed with additive MARS models (models with degree of 1).\nUSE: When there is a clear indication that the relationship between the dependent variable and independent variables is non-linear and interpretability is impportante"
  },
  {
    "objectID": "Nonlinear Models.html#sec-SVM",
    "href": "Nonlinear Models.html#sec-SVM",
    "title": "6  Nonlinear Models",
    "section": "6.3 Support Vector Machines (SVM)",
    "text": "6.3 Support Vector Machines (SVM)\nUSE: When we seek to minimize the effect of outliers\n\nNOTE: This principle also apply for regression. However in this case the svm will search for hyperplane that holds the maximum of the observation within the margin (tolerance level)\n\n\nTuning parameters: - Kernel: SVM can use different kernel functions to transform the input data into a higher-dimensional space, where it becomes easier to find a separating hyperplane. Common kernel functions include: - Linear Kernel (If regression line is truly linear, the linear kernel function will be a better choice) - Polynomial Kernel (In general, quadratic models have smaller error rates than the linear models) (tuning parameters: degree and scale factor (coef0) and c) - Radial Basis Function (RBF) Kernel (radial basis function has been shown to be very effective overall and easier to tune than polynomial one less tuning parameter) (tuning parameters: σ (sigma) that controls the scale and c) - hyperbolic tangent\n\nThreshold ε (epsilon) (called margin in tidymodels?) (If the threshold is set to a relatively large value, then the outliers are the only points that define the regression line) (e.g., ε = 0.01; the cost parameter provides more flexibility for tuning the model. So it is suggested to fix a value for ε and tune over the other kernel parameters)\nC parameter (Cost; e.g., values between 0.25 and 2048):\n\nFor classification: It controls the trade-off between maximizing the margin and minimizing the classification error. A smaller C value creates a wider margin but may allow some misclassifications, while a larger C value creates a narrower margin but may result in fewer misclassifications on the training set.\nFor regression: The cost parameter is the main tool for adjusting the complexity of the model. When the cost is large, the model becomes very flexible since the effect of errors is amplified. When the cost is small, the model will “stiffen” and become less likely to over-fit (but more likely to underfit)\n\n\nPre-processing: Center and scale the predictors prior to building an SVM model since the predictors enter into the model as the sum of cross products, differences in the predictor scales can affect the model."
  },
  {
    "objectID": "Nonlinear Models.html#sec-KNN",
    "href": "Nonlinear Models.html#sec-KNN",
    "title": "6  Nonlinear Models",
    "section": "6.4 K-Nearest Neighbors (KNN)",
    "text": "6.4 K-Nearest Neighbors (KNN)\nFor classification: \nFor regression: \nTuning parameters: K number of neighbors.\nAdvantages: The KNN method can have poor predictive performance when local predictor structure is not relevant to the response.\nPre-processing: - Remove irrelevant, noise-laden predictors is a key pre-processing step for KNN, since these can cause similar samples to be driven away from each other in the predictor space\nNOTE: to enhance KNN predictability weight the neighbors’ contribution to the prediction of a new sample based on their distance to the new sample."
  },
  {
    "objectID": "Nonlinear Models.html#sec-Neural_Networks",
    "href": "Nonlinear Models.html#sec-Neural_Networks",
    "title": "6  Nonlinear Models",
    "section": "6.5 Neural Networks",
    "text": "6.5 Neural Networks\nNeural Networks uses surrogate features instead of the original predictors (like pls and MARS)\n\nFor classification: NOTES: Produce continuous predictions that do not follow the definition of a probability-the predicted values are not necessarily between 0 and 1 and do not sum to 1. Therefore, a transformation (e.g., softmax transformation) must be used to coerce the predictions into “probability-like” values so that they can be interpreted and used for classification.\nFor regression: To be continued"
  },
  {
    "objectID": "Nonlinear Models.html#nonlinear-discriminant-analysis",
    "href": "Nonlinear Models.html#nonlinear-discriminant-analysis",
    "title": "6  Nonlinear Models",
    "section": "6.6 Nonlinear Discriminant Analysis",
    "text": "6.6 Nonlinear Discriminant Analysis\nFor classification only:"
  },
  {
    "objectID": "Nonlinear Models.html#flexible-discriminant-analysis",
    "href": "Nonlinear Models.html#flexible-discriminant-analysis",
    "title": "6  Nonlinear Models",
    "section": "6.7 Flexible Discriminant Analysis",
    "text": "6.7 Flexible Discriminant Analysis\nFor classification only:"
  },
  {
    "objectID": "Nonlinear Models.html#sec-Naive_Bayes",
    "href": "Nonlinear Models.html#sec-Naive_Bayes",
    "title": "6  Nonlinear Models",
    "section": "6.8 Naive Bayes",
    "text": "6.8 Naive Bayes\nNaive Bayes is included in nearly every data mining toolkit and serves as a common baseline classifier against which more sophisticated methods can be compared\nAdvantages:\nFor classification only:\n\nIt is very simple and efficient in terms of storage space and computation time. Training consists only of storing counts of classes and feature occurrences as each example is seen.\nNaive Bayes is that it is naturally an “incremental learner.” An incremental learner is an induction technique that can update its model one training example at a time. It does not need to reprocess all past training examples when new training data become available. Incremental learning is especially advantageous in applications where training labels are revealed in the course of the application, and we would like the model to reflect this new information as quickly as possible.\n\n\n\n\n\n\n\nWarning\n\n\n\nNaive Bayes assume independence between variables although violating this rule don’t hurt classification performance it becomes a problem if we’re going to be using the probability estimates themselves (e.g., ranking if email 1 has a bigger likelihood of been spam than email 2).\n\n\n\n\n6.8.1 Multinomial Naive Bayes\nGaussian Naive Bayes"
  },
  {
    "objectID": "Clustering.html#algorithms",
    "href": "Clustering.html#algorithms",
    "title": "7  Clustering",
    "section": "7.1 Algorithms",
    "text": "7.1 Algorithms\n\n7.1.1 K-means\nWe seek to partition the observations hierarchical clustering into a pre-specified number of clusters\nAdvantages:\n\nEase of Interpretation: K-means produces non-overlapping clusters, which can be easier to interpret and analyze compared to the nested structure of hierarchical clustering.\nEfficiency: K-means is computationally efficient, especially when dealing with a large dataset. It can handle large datasets much better than hierarchical clustering.\nScalability: K-means can work well with high-dimensional data, making it suitable for a wide range of applications, including text mining and image segmentation.\n\nDisadvantages:\n\nit requires us to pre-specify the number of clusters K\nAssumes Equal Sized Clusters: K-means assumes that clusters are spherical, equally sized, and have similar densities, which may not hold true in real-world datasets.\n\n\n\n\n7.1.2 Hierarchical (bottom-up or agglomerative: dendrogram is build starting from the leaves)\nWe do not know in advance how many clusters we want and the clusters are later choose base on the generated dendrogram.\nAdvantages:\n\nDoes not requires us to pre-specify the number of clusters K and the results; thus more flexible\nHierarchy Visualization: Attractive tree-based representation of the observations.\nFlexibility: You can cut the hierarchical tree at different levels to obtain clusters of different sizes and shapes, making it adaptable to various scenarios.\nRobustness: less sensitive to the initial conditions compared to K-means.\n\nDisadvantages:\n\nComputationally Intensive and may not perform well with high-dimensional data or extremely large datasets due to the computational burden."
  },
  {
    "objectID": "Clustering.html#preprocessing-and-tuning",
    "href": "Clustering.html#preprocessing-and-tuning",
    "title": "7  Clustering",
    "section": "7.2 Preprocessing and Tuning",
    "text": "7.2 Preprocessing and Tuning\nWe try several different choices, and look for the one with the most useful or interpretable solution.\nPreprocessing: - standardized observations or features - Outlier handling\nTuning: - For K-means: - n clusters\n\nFor hierarchical clustering:\n\ndissimilarity measure\ntype of linkage\ncutting points in the dendrogram"
  },
  {
    "objectID": "Clustering.html#sec-Validating",
    "href": "Clustering.html#sec-Validating",
    "title": "7  Clustering",
    "section": "7.3 Validating the Clusters Obtained",
    "text": "7.3 Validating the Clusters Obtained\nAssign a p-value to a cluster in order to assess whether there is more evidence for the cluster than one would expect due to chance. (see Hastie, Tibshirani, and Friedman 2009)"
  },
  {
    "objectID": "Clustering.html#deal-with-outliers",
    "href": "Clustering.html#deal-with-outliers",
    "title": "7  Clustering",
    "section": "7.4 Deal with outliers",
    "text": "7.4 Deal with outliers\nMixture models (soft version of K-means clustering) are an attractive approach for accommodating the presence of small subset of the observations are quite different from each other and from all other observations (outliers that do not belong to any cluster) (see Hastie, Tibshirani, and Friedman 2009)."
  },
  {
    "objectID": "Clustering.html#references",
    "href": "Clustering.html#references",
    "title": "7  Clustering",
    "section": "7.5 References",
    "text": "7.5 References\ndata from (James et al. 2021)\n\n\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. The Elements of Statistical Learning. Springer Series in Statistics. New York, NY: Springer. https://doi.org/10.1007/978-0-387-84858-7.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. Springer Texts in Statistics. New York, NY: Springer US. https://doi.org/10.1007/978-1-0716-1418-1."
  },
  {
    "objectID": "Deal with suboptimal data.html#class-imbalance",
    "href": "Deal with suboptimal data.html#class-imbalance",
    "title": "8  Remedies for suboptimal data",
    "section": "8.1 Class Imbalance",
    "text": "8.1 Class Imbalance\nAn imbalance occurs when one or more classes have very low proportions in the training data as compared to the other classes.\n\nOnline advertising: Ad clicked or not (2.4%)\nPharmaceutical research: Molecules with activity (vwry few) or not\nInsurance claims: Fraud (only 22%) or not fraud\nSpam detection: Spam or not spam\nSelling buisness: Buy (6%) or not buy\n\n\n8.1.1 The Effect of Class Imbalance\n\nThe models achieve good specificity (since almost every customer is predicted no insurance) but have poor sensitivity (Figure). \nThe imbalance also had a severe effect on the predicted class probabilities. (e.g., In the random forest model, for example, 82 % of the customers have a predicted probability of having insurance of 10 % or less. This highly left-skewed predicted probability distribution also occurs for the other two models. This means that the models are not very confident in predicting that most customers have insurance; they tend to assign low probabilities of having insurance to a significant portion of the customers.)\nImbalance cause that lift charts and ROC curves have similar patterns\n\n\n\n8.1.2 Strategies for overcoming class imbalances\n\n8.1.2.1 Hyperparameters selection\n\nModel tuning strategy: tune the model to maximize the accuracy or sensitivity of the minority class(es)\n\n\n\n8.1.2.2 Post-processing techniques (use model outputs)\n\nAlternate probability Cutoffs to improve the prediction accuracy of the minority class samples (i.e., post-processing the model predictions to redefine the class predictions). The most straightforward approach is to use the ROC curve since it calculates the sensitivity and specificity across a continuum of cutoffs. Using this curve, an appropriate balance between sensitivity and specificity can be determined.\n\nSeveral techniques exist for determining a new cutoff:\n\nFirst, if there is a particular target that must be met for the sensitivity or specificity, this point can be found on the ROC curve and the corresponding cutoff can be determined.\nAnother approach is to find the point on the ROC curve that is closest (i.e., the shortest distance) to the perfect model (with 100 % sensitivity and 100 % specificity), which is associated with the upper left corner of the plot. In Figure, a cutoff value of 0.064 would be the closest to the perfect model.\nThe cutoff associated with the largest value of the Youden index (measures the proportion of correctly predicted samples for both the event and nonevent groups / can be computed for each cutoff that is used to create the ROC curve): show superior performance relative to the default 50 % value. For the random forest ROC curve, the cutoff that maximizes the Youden index (0.021) is similar to the point closest to the optimal model.\n\n\n\n\n\n\nROC: The predicted sensitivity for the new cutoff of 0.064 is 64.4 %, which is a significant improvement over the value generated by the default cutoff. The consequence of the new cutoff is that the specificity is estimated to drop from 99 % to 75.9 %.\n\n\nNOTE: In our analysis, the alternate cutoff for the model was not derived from the training or test sets. It is important, especially for small samples sizes, to use an independent data (small evaluation set used for developing post-processing techniques ~ 10% ≠ training set used to tune model) set to derive the cutoff. If the training set predictions are used, there is likely a large optimistic bias in the class probabilities that will lead to inaccurate assessments of the sensitivity and specificity. If the test set is used, it is no longer an unbiased source to judge model performance.\n\nAdjusting Prior Probabilities: For models that use prior probabilities naiıve Bayes and discriminant analysis classifiers. Unless specified manually, these models typically derive the value of the priors from the training data. Weiss and Provost (2001a) suggest that priors that reflect the natural class imbalance will materially bias predictions to the majority class. Using more balanced priors or a balanced training set may help deal with a class imbalance. (e.g., when classes are 6 % and 94 % for the insured and uninsured it is better to use 60 % for the insured and 40 % for the uninsured). This strategy did not change the model (same ROC) but allows for different trade-offs between sensitivity and specificity.\n\n\n\n8.1.2.3 Alter training data prior to model training\n\nAdjust Sampling Methods: Non of them is a clear winner, it depends of the case study\n\npriori sampling approach: select a training set sample to have roughly equal event rates during the initial data collection. However, the test set should be sampled to be more consistent with the state of nature and should reflect the imbalance so that honest estimates of future performance can be computed.\npost hoc sampling approach:\n\n\nup-sampling:\n\nadding random samples with replacement from minority classes)\n\ndown-sampling:\n\nrandomly sample the majority classes so that all classes have approximately the same size\nbootstrap sample across all cases such that the classes are balanced in the bootstrap set (advatange of bootstrap is that we can obtain the estimate of variation about the down-sampling)\n\nSMOTE: adds new/synthetic samples to the minority class and down-sample cases from the majority class via random sampling\n\nNOTE: Adjusting sampling can bias model performance (e.g., up-sampling: the same sample can be use to predict and tune model)\n\n\n\n8.1.2.4 Alter model training process (model parameters are being modified)\n\nUnequal Case Weights: Many of the predictive models for classification (boosting trees which apply different case weights at each iteration) have the ability to use case weights where each individual data point can be given more emphasis in the model training phase. Increase the weights for the samples in the minority classes\nCost-Sensitive Training: Some models (SVM, CART trees, C5.0 trees) can alternatively optimize a cost or loss function that differentially weights specific types of errors of specific classes (it can cause that class probabilities cannot be generated and ROC cannot be use). For example, it may be appropriate to believe that misclassifying true events (false negatives) is X times as costly as incorrectly predicting nonevents (false positives). Correctly classify class A is more importante than correctly classify class B"
  },
  {
    "objectID": "Deal with suboptimal data.html#big-sample-size",
    "href": "Deal with suboptimal data.html#big-sample-size",
    "title": "8  Remedies for suboptimal data",
    "section": "8.2 Big sample size",
    "text": "8.2 Big sample size\nAn increase in the number of samples can have less positive consequences:\n\nComputational burdens as the number of samples (and predictors) grows\nThere are diminishing returns on adding more of the same data from the same population. Since models stabilize with a sufficiently large number of samples, garnering more samples is less likely to change the model fit."
  },
  {
    "objectID": "Measuring Predictor Importance.html#for-numeric-outcomes",
    "href": "Measuring Predictor Importance.html#for-numeric-outcomes",
    "title": "9  Measuring Predictor Importance",
    "section": "9.1 For Numeric Outcomes",
    "text": "9.1 For Numeric Outcomes\n\nLOESS\nt-statistic\nANOVA\nRelief"
  },
  {
    "objectID": "Measuring Predictor Importance.html#for-categorical-outcomes",
    "href": "Measuring Predictor Importance.html#for-categorical-outcomes",
    "title": "9  Measuring Predictor Importance",
    "section": "9.2 For Categorical Outcomes",
    "text": "9.2 For Categorical Outcomes\n\narea under the ROC curve\nt-statistics\nMIC\nRelief"
  },
  {
    "objectID": "Feature Selection.html#unsupervised-methods",
    "href": "Feature Selection.html#unsupervised-methods",
    "title": "10  Feature Selection",
    "section": "10.1 Unsupervised methods",
    "text": "10.1 Unsupervised methods\nWhen the outcome is ignored during the elimination of predictors, the technique is unsupervised.\n\nremoving predictors that have high correlations with other predictors\nremoving near-zero variance predictors"
  },
  {
    "objectID": "Feature Selection.html#supervised-methods",
    "href": "Feature Selection.html#supervised-methods",
    "title": "10  Feature Selection",
    "section": "10.2 Supervised methods",
    "text": "10.2 Supervised methods\nWhen, the outcome is typically used to quantify the importance of the predictors. Predictors are specifically selected for the purpose of increasing accuracy or to find a subset of predictors to reduce the complexity of the model"
  },
  {
    "objectID": "Feature Selection.html#consequences-of-using-non-informative-predictors",
    "href": "Feature Selection.html#consequences-of-using-non-informative-predictors",
    "title": "10  Feature Selection",
    "section": "10.3 Consequences of Using Non-informative Predictors",
    "text": "10.3 Consequences of Using Non-informative Predictors\nThe presence of non-informative variables can add uncertainty/noise to the predictions and reduce the overall effectiveness of the model (linear regression, partial least squares, neural networks, svm). Regression trees, MARS models and Random forests are not affected or very slightly in the case of random forests"
  },
  {
    "objectID": "Feature Selection.html#approaches-for-reducing-the-number-of-predictors",
    "href": "Feature Selection.html#approaches-for-reducing-the-number-of-predictors",
    "title": "10  Feature Selection",
    "section": "10.4 Approaches for Reducing the Number of Predictors",
    "text": "10.4 Approaches for Reducing the Number of Predictors\n\nWrapper methods evaluate multiple models using procedures that add and/or remove predictors to find the optimal combination that maximizes model performance. In essence, wrapper methods are search algorithms that treat the predictors as the inputs and utilize model performance as the output to be optimized.\n\nForward, Backward, and Stepwise Selection: Add, remove predictors or both to find the model that results in the smallest model RMSE/AIC (Used in linear regressions)\nCorrelation-based feature selection: find the best subset of predictors that have strong correlations with the outcome but weak between-predictor correlations\nSimulated annealing: Starting from an initial solution, the method iteratively explores neighboring solutions with the ability to accept worse solutions initially, gradually decreasing this acceptance as the process continues. This balance between exploration and exploitation helps the algorithm escape local optima and search for global optima in the solution space.\nGenetic Algorithms: GAs start with a population of potential solutions encoded as “genetic” representations. Through multiple generations, solutions are selected based on their fitness, which measures how well they solve the problem. These selected solutions then undergo crossover (combination of genetic material) and mutation (random changes) to produce a new population. Over successive generations, the algorithm converges towards better solutions as traits from successful solutions propagate and refine in the population.\n\nAdvantages: Disadvantages:\n\nmany models are evaluated (which may also require parameter tuning) and thus an increase in computation time\nincreased risk of over-fitting\n\nFilter methods evaluate the relevance of the predictors outside of the predictive models and subsequently model only the predictors that pass some criterion. For example, for classification problems, each predictor could be individually evaluated to check if there is a plausible relationship between it and the observed classes. Only predictors with important relationships would then be included in a classification model.\nAdvantages: more computationally efficient Disadvantages:\n\nthe selection criterion is not directly related to the effectiveness of the model\nmethods evaluate each predictor separately, and, consequently, redundant (i.e., highly-correlated) predictors may be selected and important interactions between variables will not be able to be quantified\nsubjective nature to the procedure. Most scoring methods have no obvious cut point to declare which predictors are important enough to go into the model (In practice, finding an appropriate value for the confidence value α may require several evaluations until acceptable performance is achieved.)\n\nEmbedded methods are models where the feature selection procedure occurs naturally in the course of the model fitting process. Here an example would be a simple decision tree where variables are selected when the model uses them in a split. If a predictor is never used in a split, the prediction equation is functionally independent of this variable and it has been selected out.\n\n\n\n\n\n\n\nWarning\n\n\n\nWhen using other search procedures or filters for reducing the number of predictors, there is still a risk. The following situations increase the likelihood of selection bias:\n\nThe data set is small.\nThe number of predictors is large (since the probability of a non-informative predictor being falsely declared to be important increases).\nThe predictive model is powerful (e.g., black-box models), which is more likely to over-fit the data.\nNo independent test set is available\n\n\n\n\n\n\n\n\n\n\ntips\n\n\n\n\nWhen the data set is large, it is recommended separate data sets for selecting features, tuning models, and validating the final model (and feature set).\nWhen training sets are small, proper resampling is critical.\nWhen the amount of data is not too small (333 obs), it is recommended setting aside a small test set to double check that no gross errors have been committed."
  },
  {
    "objectID": "Feature Engineering.html",
    "href": "Feature Engineering.html",
    "title": "11  Feature Engineering",
    "section": "",
    "text": "12 Engineering Numeric Predictors converting continuous predictors into a form that a model can better utilize.\nThis type of data can occur if a sample is measured repeatedly over time, if a sample has many highly related/correlated predictors, or if sample measurements occur through a hierarchical structure.\nBasic preprocessing steps for profiled data can include estimating and adjusting the baseline effect, reducing noise across the profile, and harnessing the information contained in the correlation among predictors. The latter in order to remove the characteristics that prevent this type of data from being used with most predictive models while simultaneously preserving the predictive signal between the profiles and the outcome."
  },
  {
    "objectID": "Feature Engineering.html#exploratory-visualizations",
    "href": "Feature Engineering.html#exploratory-visualizations",
    "title": "11  Feature Engineering",
    "section": "11.1 Exploratory Visualizations",
    "text": "11.1 Exploratory Visualizations\n\nUnivariate visualizations (Box Plots, Violin Plots, and Histograms) to understand the distribution of a single variable.\n\nunderstand the distribution of the response variable (symmetric distribution / skewed distribution / distribution has multiple peaks or modes / outliers) to:\n\nknow its variation and provide a lower bound of the expectations of model performance (the residuals from a model that contains these predictors should have less variation than the variation of the response)\nknow if it should be transformed to a normal distribution prior to analysis to have better predictive performance\nprovide clues for including or creating features that help explain the response\n\nunderstand the distribution of the predictors (when moderate number of predictors (&lt; ~100) / if not examine a subset of predictors that are thought to be important)\n\nscatter plots or mosaic plots to see if any of the responses “cluster” with others\n\ncorrespondence analysis can answered the question: linear regression for scatter plot or X2 for mosaic plot. And principal coordinates can be computed to create new variables\n\n\n\nscatter plots / bar charts + 95% confidence intervals / smoother of individual predictors and the response outcome:\n\nto easly test for new crucial predictor (e.g., if we find a strong linear relationship)\ntry to understand the relation between predictors and if some points don’t follow the overall pattern trying to understand them could lead to a new feature.\n\n\n  \n\na heatmap across the samples and predictors (e.g., to see when these unusual values occur)\na pairwise correlation plot among the predictors (e.g., high degree of correlation is a clear indicator that the information present across the stations is redundant and could be eliminated or reduced.)\n\nhierarchical cluster analysis to arrange samples in a way that those that are ‘close’ in the measurement space are also nearby in their location on the axis.\n\nline plots for time-based predictors (e.g., trends or patterns associated with time to know if variable’s current value is more related to recent values than to values further apart in time)\nPCA / PLS / MDS (multidimensional scaling) : to engineer features that effectively condense the original predictors’ information while retaining crucial predictive information. (if the first and first and second component captures 76.7% and 83.1% respectively the information is redundant and can likely be summarized in a more condensed fashion)\n\ncumulative amount of variation summarized: how many components are required to summarize a sufficient amount of variation in the data\nscatter plot of the first two components to detect clusters\nviolin plot of the first and second components against the underlying variables that appear to affect them the most"
  },
  {
    "objectID": "Feature Engineering.html#postmodeling-exploratory-visualizations",
    "href": "Feature Engineering.html#postmodeling-exploratory-visualizations",
    "title": "11  Feature Engineering",
    "section": "11.2 Postmodeling Exploratory Visualizations",
    "text": "11.2 Postmodeling Exploratory Visualizations\nTo understand the next set of improvements.\n\nMultiple linear regression (lm): identify relationships that may be useful to include in the model\n\npartial regression plot:\n\nthe first few levels of a regression or classification tree"
  },
  {
    "objectID": "Feature Engineering.html#sec-Feature_engineering",
    "href": "Feature Engineering.html#sec-Feature_engineering",
    "title": "11  Feature Engineering",
    "section": "11.3 Encoding Categorical Predictors",
    "text": "11.3 Encoding Categorical Predictors\n\n11.3.1 Creating Dummy Variables for Unordered Categories\nAdvatanges: Lead to zero-variance predictor which can be remove and omitting rarely occurring values and propagates this noise into the resampling estimates of performance. Disadvatanges: However if dummy zero-variance predictor is remove the model will not be able to predict USE: - When categories are small and it does not lead to zero-variance predictors NOTE: when model can support categorical data it is very difficult to predict if dummy variable will improve the model. Start without dummy variables and, if the model appears promising, to also try refitting using dummy variables.\n\n\n11.3.2 Encoding Predictors with Many Categories\n\nCreating Dummy Variables and remove zero-variance predictor\nHashing function to combine categories to create feature hashing\nCreate an “other” category\nSupervised Encoding Methods to encode categorical predictors to numeric columns using the outcome data as a guide\n\nUSE: When the predictor has many possible values\nTechnics:\n\neffect or likelihood encoding: (e.g., mean or median sale price of a house for each neighborhood from the training data and use this statistic to represent the factor level in the model)\nlogistic regression model (for classification problems):\nlinear regression model (for regression problems):\nword/entity embedding: estimate a smaller set of numeric features that can be used to adequately represent the categorical predictors\nhidden layers\n\nDisadvantage:\n\ngenerate error when a factor level has a single value (to solve the issue use shrinking methods such as Bayesian analysis)\nincreases the possibility of overfitting\ncan drastically underestimate the variation in the data and might give a falsely optimistic opinion of the utility of the new encoding column\n\nNOTES: It is strongly recommended that either different data sets be used to estimate the encodings and the predictive model or that their derivation is conducted inside resampling so that the assessment set can measure the overfitting (if it exists).\n\n\n\n\n11.3.3 Approaches for Novel Categories to enable the original model to be applied to new data without completely refitting it\n\nCreate a “other” category and asign the new category no other\nCreate a zero-variance dummy variable in the training or test set or both.\nSupervised Encoding Methods to encode categorical predictors to numeric columns using the outcome data as a guide\n\nUSE: When new levels appear after model training\n\nEncodings for Ordered Data (e.g., “low”, “medium”, and “high.”)\n\nTechnics: polynomial contrast\n\nAdvatanges: By employing polynomial contrasts, we can investigate multiple relationships (linear, quadratic, etc.) simultaneously by including these in the same model.\nDisadvantage:\n\npolynomial contrasts may not effectively relate a predictor to the response (For example, in some cases, one might expect a trend where “low” and “middle” samples have a roughly equivalent response but “high” samples have a much different response.)\nNot recommended when there are moderate to high number of categories\n\n\nTechnics: Translate the ordered categories into a single set of numeric scores based on context-specific information.\n\n\n\n\n11.3.4 Approaches for Text Data\n\nTransform the text data into the odds-ratio of containing a keywords/link this can be extended to the odds-ratio of containing a text/link for each response variable (The rate of hyperlinks in the STEM profiles (response variable) was 21%, while this rate was 12.4% in the non-STEM profiles. For the STEM profiles, the odds of containing a hyperlink are relatively small with a value of 0.21/1-0.21 = 0.27. For the non-STEM profiles, it is even smaller (0.142).)\ncreate “text-related” features: (e.g., number of commas, hashtags, mentions, exclamation points)\ncode sentiment values\ncode language used (e.g., first-, second-, or third-person text and other language elements)"
  },
  {
    "objectID": "Feature Engineering.html#transformation",
    "href": "Feature Engineering.html#transformation",
    "title": "11  Feature Engineering",
    "section": "12.1 Transformation:",
    "text": "12.1 Transformation:\n\ncentering\nscaling\ntransforming a distribution to symmetry"
  },
  {
    "objectID": "Feature Engineering.html#feature-engeneering",
    "href": "Feature Engineering.html#feature-engeneering",
    "title": "11  Feature Engineering",
    "section": "12.2 Feature engeneering",
    "text": "12.2 Feature engeneering\n\ntransforming predictors in its original scale to nonlinear scales that may be informative.\n\nTechincs:\n\nbasis expansions (e.g., Squared predictors for simplistic models such as regressions)\nsplines\ncombination of kernel function and PCA\n\nDisadvantage: computational cost\n\nreduce the dimension of the predictors\n\nTechincs:\n\nPCA (unsupervised approach)\nICA (unsupervised approach)\nNNMF (unsupervised approach)\nPLS (supervised approach)\ncategorizing the response (only appropriate when the response is bimodal (or multimodal)).\n\n\nharness information in unlabeled data or dampen the effect of extreme samples\n\nTechincs:\n\nautoencoders\nspatial sign transformation\ndistance and depth measures (e.g., class centroids for classification models: centers of the predictor data for each class. For each predictor, the distance to each class centroid can be calculated and these distances can be added to the model)"
  },
  {
    "objectID": "SQL.html",
    "href": "SQL.html",
    "title": "12  SQL",
    "section": "",
    "text": "13 SQL in R"
  },
  {
    "objectID": "SQL.html#sec-SQL",
    "href": "SQL.html#sec-SQL",
    "title": "12  SQL",
    "section": "12.1 SQL vs dplyr",
    "text": "12.1 SQL vs dplyr\n\n\n\n\n\n\n\nSQL\ndplyr\n\n\n\n\nSELECT table1.column_name1\nselect\n\n\nFROM\ndf %&gt;%\n\n\nUNION(remove duplicates)/UNION ALL (SELECT FROM WHERE … UNION SELECT FROM WHERE …)\nbind_rows()\n\n\nINNER JOIN table2 ON table1.column_name1 = table2.column_name1 AND table1.column_name2 = table2.column_name2\ninner_join(., table2, by = c(“column_name1” = “column_name1”, “column_name2” = “column_name2”))\n\n\nLEFT JOIN table2 ON table1.column_name = table2.column_name\nleft_join(., table2, by = c(“column_name” = “column_name”)\n\n\nRIGHT JOIN table2 ON table1.column_name = table2.column_name\nright_join(., table2, by = c(“column_name” = “column_name”)\n\n\nFULL OUTER JOIN table2 ON table1.column_name = table2.column_name\nfull_join(., table2, by = c(“column_name” = “column_name”)\n\n\nWHERE (IN [num/char]; BETWEEN [num] AND [num]; LIKE [char]; REGEXP/NOT REGEXP regex[char]; AND\nfilter ( %in% ; %in% c(1:10); ==; grepl/!grepl)\n\n\nGROUP BY (all columns in select)\ngroup_by\n\n\nHAVING (filter results of aggregate functions applied to grouped data)\nfilter\n\n\nORDER BY column1 ASC, column2 DESC\narrange(column1, desc(column2))\n\n\nLIMIT [num]\nslice_head(n = [num])"
  },
  {
    "objectID": "SQL.html#functions",
    "href": "SQL.html#functions",
    "title": "12  SQL",
    "section": "12.2 functions",
    "text": "12.2 functions\n\nMOD Returns the remainder (number after the point 67 in 3.67) of a number divided by another number\nLENGTH Returns the length of a string (in bytes)\nCONCAT(col1, col2) Combine two or more strings into a single string\nRIGHT/LEFT Extracts a number of characters from a string (starting from RIGHT/LEFT)\nROUND(VALUE, number of decimals) Rounds a number to a specified number of decimal places\nCEILING round it up to the next integer.\nFLOOR round it up to the last integer.\nAVG Average\nCAST(column AS data_type): convert the data type of a column\nREPLACE(original_string, old_substring, new_substring): replace occurrences of a specified substring with another substring in a given string. eg: SELECT REPLACE(‘Hello World’, ‘World’, ‘Universe’) AS Result;\nLAG/LEAD(column OVER (ODER BY column_date))\nMONTH extract month\nLPAD(MONTH(trans_date), 2, ‘0’) stands for “Left PADding,” and it is a string function used in SQL to add characters to the left side of a string until it reaches a specified length.\nDATE_SUB(‘2019-07-27’, INTERVAL 28 DAY) function used to subtract 28 days from the reference date\nLOWER/UPPER (column1) convert all characters in a string to lowercase/uppercase ex: CONCAT( UPPER(RIGHT(name, 1)), LOWER(LEFT(name, LENGTH(name) - 1))) AS name"
  },
  {
    "objectID": "SQL.html#regex",
    "href": "SQL.html#regex",
    "title": "12  SQL",
    "section": "12.3 Regex",
    "text": "12.3 Regex\n^[char] start with\n[char]$ end with\n.* any number of characters"
  },
  {
    "objectID": "SQL.html#match-regex-used-with-the-like",
    "href": "SQL.html#match-regex-used-with-the-like",
    "title": "12  SQL",
    "section": "12.4 Match / regex used with the LIKE",
    "text": "12.4 Match / regex used with the LIKE\n% is a wildcard character to represent zero or more characters. When used in a LIKE pattern, % matches any sequence of characters."
  },
  {
    "objectID": "SQL.html#subqueries",
    "href": "SQL.html#subqueries",
    "title": "12  SQL",
    "section": "12.5 Subqueries",
    "text": "12.5 Subqueries\nWHERE column = (SELECT MAX(column) FROM data WHERE …);\nFROM (SELECT MAX(column) FROM data);"
  },
  {
    "objectID": "SQL.html#joins",
    "href": "SQL.html#joins",
    "title": "12  SQL",
    "section": "12.6 Joins",
    "text": "12.6 Joins\nWhen JOIN is use we need to include table1.column_name1 to select column"
  },
  {
    "objectID": "SQL.html#mutate",
    "href": "SQL.html#mutate",
    "title": "12  SQL",
    "section": "12.7 Mutate",
    "text": "12.7 Mutate\n\n12.7.1 Case when\nCASE\nWHEN condition1 THEN “result1”\nWHEN condition2 THEN “result2”\nELSE “result”\nEND AS new_column_name\n\n\n12.7.2 IF (only in MYSQL)\nSELECT if(column=“confirmed”,1,0) | mutate(action = if_else(column=“confirmed”,1,0))"
  },
  {
    "objectID": "SQL.html#exist",
    "href": "SQL.html#exist",
    "title": "12  SQL",
    "section": "12.8 Exist",
    "text": "12.8 Exist\nThe EXISTS operator is used to test for the existence of any record in a subquery.\nThe EXISTS operator returns TRUE if the subquery returns one or more records.\n\nWHERE EXISTS (SELECT column_name FROM table_name WHERE condition)\nTHEN “result1” # (if condition is TRUE return “result1”)\nELSE “result2”\n\nCASE WHEN EXISTS (SELECT column_name FROM table_name WHERE condition) THEN result"
  },
  {
    "objectID": "SQL.html#joins-1",
    "href": "SQL.html#joins-1",
    "title": "12  SQL",
    "section": "12.9 Joins",
    "text": "12.9 Joins\n\n12.9.1 SELF JOIN\nSELECT w1., w2. FROM Weather w1 JOIN Weather w2 ON w1.recordDate = w2.recordDate + 1\nw1 and w2 are different table aliases for the same table.\n\n\n12.9.2 CROSS JOIN\nUsed generally when merging a column with only one columns to avoid creating a huge table with all possible combinations keyword returns all matching records from both tables whether the other table matches or not."
  },
  {
    "objectID": "SQL.html#with-df-as",
    "href": "SQL.html#with-df-as",
    "title": "12  SQL",
    "section": "12.10 WITH df AS",
    "text": "12.10 WITH df AS\nThe WITH clause in SQL is used to define a Common Table Expression (CTE). A CTE is a temporary result set that can be referenced within the context of a SELECT, INSERT, UPDATE, or DELETE statement. The purpose of a CTE is to simplify complex queries, make the code more readable, and avoid repeating the same subquery multiple times.\nWITH df AS ( SELECT Signups.user_id, action, COUNT(Confirmations.action) AS conf FROM Signups LEFT JOIN Confirmations ON Signups.user_id = Confirmations.user_id GROUP BY action, Signups.user_id )\nSELECT * FROM df"
  },
  {
    "objectID": "SQL.html#pivot-tables",
    "href": "SQL.html#pivot-tables",
    "title": "12  SQL",
    "section": "12.11 PIVOT tables",
    "text": "12.11 PIVOT tables\nCASE WHEN … THEN … END\nSELECT user_id, CASE WHEN action = “timeout” THEN conf END AS timeout, CASE WHEN action = “confirmed” THEN conf END AS confirmed, CASE WHEN action = “confirmed” THEN conf END / (CASE WHEN action = “timeout” THEN conf END + CASE WHEN action = “confirmed” THEN conf END) AS confirmation_rate FROM df"
  },
  {
    "objectID": "SQL.html#windows-functions",
    "href": "SQL.html#windows-functions",
    "title": "12  SQL",
    "section": "12.12 Windows functions",
    "text": "12.12 Windows functions\n\nOVER (PARTITION BY)\n\nSELECT , COUNT() OVER(PARTITION BY column1) AS name1, COUNT(*) OVER(PARTITION BY column2, column3) AS name2 FROM data\nSELECT *, SUM(weight) OVER (ORDER BY turn) # cumulative sum FROM Queue\nsee website SQL Window Functions"
  },
  {
    "objectID": "SQL.html#common-table-expressions-ctes",
    "href": "SQL.html#common-table-expressions-ctes",
    "title": "12  SQL",
    "section": "12.13 Common Table Expressions (CTEs)",
    "text": "12.13 Common Table Expressions (CTEs)\nCTEs in SQL provide a way to create temporary result sets that can be referenced within a SELECT, INSERT, UPDATE, or DELETE statement.\n\nWITH tablename AS ( SELECT column1, column2 FROM your_table WHERE some_condition )\n\n-CREATE, ALTER, DROP, RENAME, TRUNCATE, COMMENT"
  },
  {
    "objectID": "SQL.html#data-manipulation",
    "href": "SQL.html#data-manipulation",
    "title": "12  SQL",
    "section": "12.14 Data manipulation",
    "text": "12.14 Data manipulation\nUsing DML statements to retrieve and manipulate data.\n-INSERT, UPDATE, DELETE, MERGE, CALL, EXPLAIN PLAN, LOCK TABLE"
  },
  {
    "objectID": "SQL.html#data-security",
    "href": "SQL.html#data-security",
    "title": "12  SQL",
    "section": "12.15 Data security",
    "text": "12.15 Data security\nUsing DCL (data control language) commands to manage database security.\n\nGRANT\nREVOKE"
  },
  {
    "objectID": "SQL.html#stored-procedures",
    "href": "SQL.html#stored-procedures",
    "title": "12  SQL",
    "section": "12.16 Stored procedures",
    "text": "12.16 Stored procedures\nCREATE PROCEDURE procedure_name\nAS\nsql_statement (e.g., SELECT * FROM Customers)\nGO;\n\nEXEC procedure_name;"
  },
  {
    "objectID": "SQL.html#index",
    "href": "SQL.html#index",
    "title": "12  SQL",
    "section": "12.17 Index",
    "text": "12.17 Index\nTo accelerate SQL performance\n\nclustered\nnon clustered"
  },
  {
    "objectID": "SQL.html#schema-design",
    "href": "SQL.html#schema-design",
    "title": "12  SQL",
    "section": "12.18 Schema design",
    "text": "12.18 Schema design"
  },
  {
    "objectID": "SQL.html#query-efficiency",
    "href": "SQL.html#query-efficiency",
    "title": "12  SQL",
    "section": "12.19 Query efficiency",
    "text": "12.19 Query efficiency\n\nindexes\navoiding subqueries\noptimizing the database schema"
  },
  {
    "objectID": "SQL.html#database-normalization",
    "href": "SQL.html#database-normalization",
    "title": "12  SQL",
    "section": "12.20 Database Normalization",
    "text": "12.20 Database Normalization\nStructure the table to eliminate redundant information, improve understanding, and make it easy to enhance, extend, and protect against insertion, update, and deletion anomalies\n\n1NF : Don’t use row order to convey information / don’t mix data types within a column / don’t have primary column (also call key) / don’t store a repeating group of data on a single row\n2NF: each columns (which are non-key atributes) must depend on all the column(s) that are considered primary key (e.g. player ID)\n\nExample: player_rating depend on player_ID but not player_inventory column which is also a primary key in this table\n\n3NF: every attributes (column) in a table should depend on the key column(s) the hole key and nothing but the key\n\nExample: player_ID is the primary key/column. Non-key atribute player_rating depend on player_ID but non-key atribute player_skill depend on player_rating which is not a primary key\n\n\nsee Video for more detail about 4NF and 5NF"
  },
  {
    "objectID": "SQL.html#create-connection",
    "href": "SQL.html#create-connection",
    "title": "12  SQL",
    "section": "13.1 Create connection",
    "text": "13.1 Create connection\n\ncon &lt;- DBI::dbConnect(RSQLite::SQLite(), \":memory:\")\ncopy_to(con, mtcars)\ncopy_to(con, iris)\nDBI::dbListTables(con)\n\n[1] \"iris\"         \"mtcars\"       \"sqlite_stat1\" \"sqlite_stat4\"\n\nDBI::dbRemoveTable(con, \"sqlite_stat1\")\n\n\nCREATE TABLE OccupationData (\n    PersonName VARCHAR(50),\n    OccupationType VARCHAR(20),\n    Occupation VARCHAR(50)\n);\n\n\nSELECT *\nFROM OccupationData;\n\n\n0 records\n\n\nPersonName\nOccupationType\nOccupation\n\n\n\n\n\n\n\n\nINSERT INTO OccupationData (PersonName, OccupationType, Occupation)\nVALUES\n('John', 'Actor', 'JohnActor'),\n('John', 'Doctor', 'JohnDoctor'),\n('Jane', 'Actor', 'JaneActor'),\n('Jane', 'Doctor', 'JaneDoctor');\n\n\nSELECT *\nFROM OccupationData;\n\n\n4 records\n\n\nPersonName\nOccupationType\nOccupation\n\n\n\n\nJohn\nActor\nJohnActor\n\n\nJohn\nDoctor\nJohnDoctor\n\n\nJane\nActor\nJaneActor\n\n\nJane\nDoctor\nJaneDoctor\n\n\n\n\n\n\nSELECT *\nFROM iris\nWHERE Species LIKE \"virginica\"\nUNION\nSELECT *\nFROM iris\nWHERE Species LIKE \"setosa\"\n\n\nDisplaying records 1 - 10\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n4.3\n3.0\n1.1\n0.1\nsetosa\n\n\n4.4\n2.9\n1.4\n0.2\nsetosa\n\n\n4.4\n3.0\n1.3\n0.2\nsetosa\n\n\n4.4\n3.2\n1.3\n0.2\nsetosa\n\n\n4.5\n2.3\n1.3\n0.3\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4.6\n3.2\n1.4\n0.2\nsetosa\n\n\n4.6\n3.4\n1.4\n0.3\nsetosa\n\n\n4.6\n3.6\n1.0\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n\n\n\n\nSELECT COUNT(cyl)\nFROM mtcars\nWHERE cyl &gt; 6\n\n\n1 records\n\n\nCOUNT(cyl)\n\n\n\n\n14\n\n\n\n\n\n\nmtcarsSQL &lt;- tbl(con, \"mtcars\")\nquery &lt;- mtcarsSQL %&gt;% filter(across(everything(), ~!is.na(.)))\nquery %&gt;% show_query()\n\n&lt;SQL&gt;\nSELECT `mtcars`.*\nFROM `mtcars`\nWHERE\n  (NOT((`mpg` IS NULL))) AND\n  (NOT((`cyl` IS NULL))) AND\n  (NOT((`disp` IS NULL))) AND\n  (NOT((`hp` IS NULL))) AND\n  (NOT((`drat` IS NULL))) AND\n  (NOT((`wt` IS NULL))) AND\n  (NOT((`qsec` IS NULL))) AND\n  (NOT((`vs` IS NULL))) AND\n  (NOT((`am` IS NULL))) AND\n  (NOT((`gear` IS NULL))) AND\n  (NOT((`carb` IS NULL)))\n\n\n\n\n\ndata from (“SQL Tutorial” n.d.)\n\n\n\n\n“SQL Tutorial.” n.d. https://www.w3schools.com/sql/default.asp. Accessed August 26, 2023."
  },
  {
    "objectID": "Big Data.html#databases-vs-warehouse-vs-data-lake",
    "href": "Big Data.html#databases-vs-warehouse-vs-data-lake",
    "title": "13  Big Data",
    "section": "13.1 DataBases vs Warehouse vs Data Lake",
    "text": "13.1 DataBases vs Warehouse vs Data Lake\nData Lake: Store all types of data DataBases: Raw data stored in tables Warehousen: Generally subset of data importe from the database through ETL used to perform analysis\n\n\n\nDataBases, Warehouse Data Lake"
  },
  {
    "objectID": "Big Data.html#connect-to-databricks",
    "href": "Big Data.html#connect-to-databricks",
    "title": "13  Big Data",
    "section": "13.2 Connect to databricks",
    "text": "13.2 Connect to databricks\n\n13.2.1 Libraries\n\nlibrary(sparklyr)\nlibrary(pysparklyr)\n\n\n\n13.2.2 Connection\n\nsc &lt;- spark_connect(\n  master = \"\",\n  cluster_id = \"1026-175310-7cpsh3g8\",\n  token = \"\",\n  method = \"databricks_connect\"\n)"
  },
  {
    "objectID": "Big Data.html#save-data-from-dplyr-to-databricks",
    "href": "Big Data.html#save-data-from-dplyr-to-databricks",
    "title": "13  Big Data",
    "section": "13.3 Save data from dplyr to databricks",
    "text": "13.3 Save data from dplyr to databricks\nCreate a notebook in databricks\n\nlibrary(tidyverse)\nspark_available_versions()\nsc &lt;- spark_connect(\n  master = \"local\",\n  version = \"3.5\"\n)\n\ndf &lt;- mtcars\ndf_spark &lt;- copy_to(sc, df, \"df_spark\")\nsdf_register(df_spark, \"temp_table\")\nSparkR::registerTempTable(df_sparkr, \"temp_df\")\n\nsee more on (Ruiz n.d.)\n\n\n\n\nRuiz, Edgar, Kevin Kuo. n.d. Mastering Spark with R. Accessed September 22, 2023."
  },
  {
    "objectID": "Data Science for Business.html#ml-process",
    "href": "Data Science for Business.html#ml-process",
    "title": "14  Data Science for Business",
    "section": "14.1 ML process",
    "text": "14.1 ML process\n\nWhat is the goal of the analysis? (It is never to maximize accuracy.)\nThink in context: What are the baseline benefits, and is it worth trying to improve them? (Understand the time cost of complex systems.)\nExpected value framework:\n\n\nDecompose the problem into subtasks that we think we can solve, usually starting with existing tools.\nCalculate expected benefit\nEstimate each probabilities and values using data (statistics or ml, business values often need to be acquired from other sources)\nWe may discover knowledge that will help us to solve the problem we had set out to solve, or we may discover something unexpected that leads us to other important successes\nAcknowledge the biases\n\n\nPlan a way to evaluate/measure the benefits of the system (e.g., offline metrics like A/B tests / cost and benefit framework and online metrics like accuracy).\nCommunicate the results using explainable methods.\nEthical considerations: Be careful of biased data used to test the model, as the model will likely exacerbate those biases.\n\n\n14.1.1 Expected value framework:\n\n\n\nEach oi is a possible decision outcome; p(oi) is its probability and v(o i) is its value.\n\n\n\n\n\n\n\n\nExpected value example:\n\n\n\n\n\n\nprobability of response * (response benefit - expenses) - (1 - probability of response) * (benefit of not responding (zero) - the cost of the solicitation)\nprobability of response * (response benefit - expenses) - (1 - probability of response) * (benefit of not responding (zero) - the cost of the solicitation) &gt; 0\nprobability of response * (response benefit - expenses) &gt; (1 - probability of response) * (benefit of not responding (zero) - the cost of the solicitation)\nprobability of response &gt; 1 / ((benefit of not responding (zero) - the cost of the solicitation) + (response benefit - expenses))\n\nWith these example values, we should target the consumer as long as the estimated probability of responding is greater than 1 / ((benefit of not responding (zero) - the cost of the solicitation) + (response benefit - expenses))%.\n\n\n\n\n\n\nExpected value calculation: Each oi corresponds to one cell of the confusion matrix. For example, what is the probability associated with the particular combination of a consumer being predicted to churn and actually does not churn? That would be estimated by the number of test-set consumers who fell into the confusion matrix cell (Y,n), divided by the total number of test-set consumers.\n\n\nA common way of expressing expected profit is to factor out the probabilities of seeing each class, often referred to as the class priors. The class priors, p(p) and p(n), specify the likelihood of seeing positive and negative instances, respectively. Factoring these out allows us to separate the influence of class imbalance from the fundamental predictive power of the model, as we will discuss in more detail in Chapter 8.  \n\n\n\n\n\n\nTwo pitfalls that are common when formulating cost-benefit matrices:\n\n\n\n\n\n\nIt is important to make sure the signs of quantities in the costbenefit matrix are consistent. In this book we take benefits to be positive and costs to be negative. In many data mining studies, the focus is on minimizing cost rather than maximizing profit, so the signs are reversed. Mathematically, there is no difference. However, it is important to pick one view and be consistent.\nAn easy mistake in formulating cost-benefit matrices is to “double count” by putting a benefit in one cell and a negative cost for the same thing in another cell (or vice versa). A useful practical test is to compute the benefit improvement for changing the decision on an example test instance.\nFor example, say you’ve built a model to predict which accounts have been defrauded. You’ve determined that a fraud case costs $1,000 on average. If you decide that the benefit of catching fraud is therefore +$1,000/case on average, and the cost of missing fraud is -$1,000/case, then what would be the improvement in benefit for catching a case of fraud? You would calculate: b( Y,p ) - b(N,p) = $1000 - (-$1000) = $2000 But intuitively you know that this improvement should only be about $1,000, so this error indicates double counting. The solution is to specify either that the benefit of catching fraud is $1,000 or that the cost of missing fraud is -$1,000, but not both. One should be zero."
  },
  {
    "objectID": "Data Science for Business.html#include-costs-of-aquiring-data",
    "href": "Data Science for Business.html#include-costs-of-aquiring-data",
    "title": "14  Data Science for Business",
    "section": "14.2 Include costs of aquiring data",
    "text": "14.2 Include costs of aquiring data\nDifferent data sources may have different associated costs, and careful evaluation may show which can be chosen to maximize the return on investment."
  },
  {
    "objectID": "Data Science for Business.html#crisp-dm",
    "href": "Data Science for Business.html#crisp-dm",
    "title": "14  Data Science for Business",
    "section": "14.3 CRISP-DM",
    "text": "14.3 CRISP-DM\nExtracting useful knowledge from data to solve business problems can be treated systematically by following a process with reasonably well-defined stages.\n\n\n\nCRISP"
  },
  {
    "objectID": "Data Science for Business.html#ml-list-of-data-science-tasks-and-tools",
    "href": "Data Science for Business.html#ml-list-of-data-science-tasks-and-tools",
    "title": "14  Data Science for Business",
    "section": "14.4 ML list of data science tasks and tools",
    "text": "14.4 ML list of data science tasks and tools\n\n\n\n\n\n\n\n\nTasks\nDescription\nTools\n\n\n\n\nPrediction\nEstimate or predict, for each individual, which of a (small) set of classes this individual belongs to or the numerical value of some variable for that individual\nSupervised: Classification/Regression models\n\n\nCausal modeling\nUnderstand what events or actions actually influence other\nSupervised: A/B tests\n\n\nSimilarity matching\nIdentify similar individuals based on data known about them\nGenerally unsupervised: clustering (also Classification, regression)\n\n\nClustering\nGroup individuals in a population together by their similarity, but not driven by any specific purpose / Exploratory analysis\nUnsupervised: K-means, Hierarchical Clustering\n\n\nCo-occurrence grouping\nFind associations between entities based on transactions involving them: e.g., What items are commonly purchased together? While clustering looks at similarity between objects based on the objects’ attributes, co-occurrence grouping considers similarity of objects based on their appearing together in transactions.\nUnsupervised: cluster algorithms, Hidden Markov Models\n\n\nProfiling\ncharacterize the typical behavior of an individual, group, or population e.g. “What is the typical cell phone usage of this customer segment?”\nGenerally Unsupervised: Cluster analysis, Anomalies detection; (also NLP, descriptive statistics)"
  },
  {
    "objectID": "Data Science for Business.html#data-science-process-questions",
    "href": "Data Science for Business.html#data-science-process-questions",
    "title": "14  Data Science for Business",
    "section": "14.5 Data Science process questions",
    "text": "14.5 Data Science process questions\nBusiness and Data Understanding\n\nWhat exactly is the business problem to be solved?\nIs the data science solution formulated appropriately to solve this business problem? NB: sometimes we have to make judicious approximations.\nWhat business entity does an instance/example correspond to?\nthe problem a supervised or unsupervised problem? — If supervised — Is a target variable defined? — If so, is it defined precisely? — Think about the values it can take.\nAre the attributes defined precisely? — Think about the values they can take.\nFor supervised problems: will modeling this target variable improve the stated business problem? An important subproblem? If the latter, is the rest of the business problem addressed?\nDoes framing the problem in terms of expected value help to structure the subtasks that need to be solved?\nIf unsupervised, is there an “exploratory data analysis” path well defined? (That is, where is the analysis going?)\n\nData Preparation\n\nWill it be practical to get values for attributes and create feature vectors, and put them into a single table?\nIf not, is an alternative data format defined clearly and precisely? Is this taken into account in the later stages of the project? (Many of the later methods/techniques assume the dataset is in feature vector format.)\nIf the modeling will be supervised, is the target variable well defined? Is it clear how to get values for the target variable (for training and testing) and put them into the table?\nHow exactly will the values for the target variable be acquired? Are there any costs involved? If so, are the costs taken into account in the proposal?\nAre the data being drawn from the similar population to which the model will be applied? If there are discrepancies, are the selection biases noted clearly? Is there a plan for how to compensate for them?\n\nModeling\n\nIs the choice of model appropriate for the choice of target variable?\n\nClassification, class probability estimation, ranking, regression, clustering, etc.\n\nDoes the model/modeling technique meet the other requirements of the task?\n\nGeneralization performance, comprehensibility, speed of learning, speed of application, amount of data required, type of data, missing values?\nIs the choice of modeling technique compatible with prior knowledge of problem (e.g., is a linear model being proposed for a definitely nonlinear problem)?\n\nShould various models be tried and compared (in evaluation)?\nFor clustering, is there a similarity metric defined? Does it make sense for the business problem?\n\nEvaluation and Deployment\n\nIs there a plan for domain-knowledge validation?\n\nWill domain experts or stakeholders want to vet the model before deployment? If so, will the model be in a form they can understand?\n\nIs the evaluation setup and metric appropriate for the business task? Recall the original formulation.\n\nAre business costs and benefits taken into account?\nFor classification, how is a classification threshold chosen?\nAre probability estimates used directly?\nIs ranking more appropriate (e.g., for a fixed budget)?\nFor regression, how will you evaluate the quality of numeric predictions? Why is this the right way in the context of the problem?\n\nDoes the evaluation use holdout data?\n\nCross-validation is one technique.\n\nAgainst what baselines will the results be compared?\n\nWhy do these make sense in the context of the actual problem to be solved?\nIs there a plan to evaluate the baseline methods objectively as well?\n\nFor clustering, how will the clustering be understood?\nWill deployment as planned actually (best) address the stated business problem?\nIf the project expense has to be justified to stake"
  },
  {
    "objectID": "Data Science for Business.html#proposal-example",
    "href": "Data Science for Business.html#proposal-example",
    "title": "14  Data Science for Business",
    "section": "14.6 Proposal Example",
    "text": "14.6 Proposal Example\n\nExplain the problem.\nDefine the goal (pay attention to the KPI used and report if the problem has slighlty change due to data limitations).\nOutline the approach to achieving the goal:\n\nSpecify the data to be used/collected.\nWhat attributes are going to be used\nJustify the choice of model(s). Think about the comprehensibility of the model to stakeholders\n\nDocument the assumptions made during modeling.\nDescribe the method for testing model performance.\nIf pilot study already has been conducted and learning curves having been produced on data samples report and estimate of model performance\nIdentify individuals responsible for peer reviewing the project (keep in mind that favour a simple model to allow other people to understand the model).\nDetail the strategy for tracking model performance."
  },
  {
    "objectID": "Data Science for Business.html#databricks",
    "href": "Data Science for Business.html#databricks",
    "title": "14  Data Science for Business",
    "section": "14.7 DataBricks",
    "text": "14.7 DataBricks\nCreate a notebook in databricks"
  },
  {
    "objectID": "A B Testing.html",
    "href": "A B Testing.html",
    "title": "16  A/B Testing",
    "section": "",
    "text": "Examines user experience through randomized tests with two variants.\nTypical steps\n\nDetermine the evaluation metric and experiment goals\nSelect a significance level α and power threshold 1 - β\nCalculate the required sample size per variation\nRandomly assign users into control and treatment groups\nMeasure and analyze results using the appropriate test\n\nThe required sample size depends on α, β, and the MDE Minimum Detectable Effect - the target relative minimum increase over the baseline that should be observed from a test Overall Evaluation Criterion - quantitative measure of the test’s objective, commonly used when short and long-term metrics have inverse relationships\nTools\nUnivariate Testing:\n\n\n\n\n\n\n\n\nStatistical Test\nDescription\nUse case\n\n\n\n\nZ-Test\nTest differences between two means\nWhen large sample size and known population variance (&gt;30)\n\n\nT-Test\nTest differences between two means\nWhen small sample size and unknown population variance (&lt;30)\n\n\nWelch’s T-Test\nTest differences between two means\nAdaptation of the t-test that does not assume equal variances (homoscedasticity: spread or dispersion of data points around the mean is consistent across all groups) offering more flexibility\n\n\nMann-Whitney U Test\nComparing two independent groups\nWhen data is not normally distributed\n\n\nANOVA\nTest differences between three or more means\n\n\n\nChi-Squarde Test\nTest if there is a significant association between two categorical variables\ne.g., sexe and energy drinks\n\n\nFisher’s Exact Test\nTest if there is a significant association between two categorical variables\nWhen small sample size &lt;30\n\n\nMultivariate Testing\ncompares 3+ variants or combinations, but requires larger sample sizes\n\n\n\n\nBonferroni Correction - when conducting n tests, run each test at the α n significance level, which lowers the false positive rate of| finding effects by chance\n\n16.0.1 Network Effects\nChanges that occur due to effect spillover from other groups.\nTypical steps To detect group interference:\n\nSplit the population into distinct clusters\nRandomly assign half the clusters to the control and treatment groups A1 and B1\nRandomize the other half at the user-level and assign to control and treatment groups A2 and B2\nIntuitively, if there are network effects, then the tests will have different results To account for network effects, randomize users based on time, cluster, or location\n\n###Sequential Testing\nAllows for early experiment stopping by drawing statistical borders based on the Type I Error rate. If the effect reaches a border, the test can be stopped. Used to combat peeking (preliminarily checking results of a test), which can inflate p-values and lead to incorrect conclusions.\n###Cohort Analysis\nExamines specific groups of users based on behavior or time and can help identify whether novelty or primacy effects are present\n(wangDataScienceCheatsheet2021?)"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. The\nElements of Statistical Learning.\nSpringer Series in Statistics. New York,\nNY: Springer. https://doi.org/10.1007/978-0-387-84858-7.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2021. An Introduction to Statistical\nLearning: With Applications in R.\nSpringer Texts in Statistics. New York,\nNY: Springer US. https://doi.org/10.1007/978-1-0716-1418-1.\n\n\nKuhn, Max, and Kjell Johnson. 2013. Applied Predictive\nModeling. New York, NY: Springer.\nhttps://doi.org/10.1007/978-1-4614-6849-3.\n\n\nRuiz, Edgar, Kevin Kuo. n.d. Mastering Spark with\nR. Accessed September 22, 2023.\n\n\n“SQL Tutorial.” n.d.\nhttps://www.w3schools.com/sql/default.asp. Accessed August 26, 2023.\n\n\n“What Is the Difference Between Correlation and Cointegration?\nIs Cointegration a Good Measure of Risk?” n.d.\nQuora.\nhttps://www.quora.com/What-is-the-difference-between-correlation-and-cointegration-Is-cointegration-a-good-measure-of-risk.\nAccessed July 17, 2023."
  }
]