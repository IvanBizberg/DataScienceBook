---
title: "Choose the machine learning model"
number-sections: true
---

## Supervised models
```{r, echo=FALSE, message=FALSE, warning=FALSE}

library(tidymodels)
library(stringr)
library(gt)

Summary <- tibble(
  Model = c("Linear regression", "Partial least squares", "Logistic regression", "Ridge regression", "Elastic net/Lasso", "Support vector machines", "MARS/FDA", "K-nearest neighbors", "Random forest", "Boosted trees", "Neural networks", "LDA", "Naive Bayes", "C5.0"),
  Mode = c("regression", "regression", "classification", "regression & classification", "regression & classification", "regression & classification", "regression & classification", "regression & classification", "regression & classification", "regression & classification", "regression & classification", "classification", "classification", "classification"),
  Allows = c("no", "depends", "no", "no", "yes", "yes", "yes", "yes", "yes", "yes", "yes", "no", "yes", "yes"),
  Preprocessing = c("centering and scaling, remove near-zero predictors, remove highly correlated predictors, remove non-informative predictors", "centering and scaling, remove non-informative predictors", "centering and scaling, remove near-zero predictors, remove highly correlated predictors, remove non-informative predictors", "centering and scaling, remove near-zero predictors, remove non-informative predictors", "centering and scaling, remove near-zero predictors", "centering and scaling, remove non-informative predictors", "", "centering and scaling, remove near-zero predictors", "", "", "centering and scaling, remove near-zero predictors, remove highly correlated predictors, remove non-informative predictors", "remove near-zero predictors", "remove near-zero predictors", ""),
  Interpretable = c("high", "low", "high", "high", "high", "low", "medium", "low", "low", "low", "low", "medium", "low", "medium"),
  Feature_selection = c("no", "yes*", "no", "no", "yes", "no", "yes", "no", "depends", "yes", "no", "no", "no", "yes"),
  Tuning_parameters = c("0", "1", "0", "1", "1-2", "1-3", "1-2", "1", "0-1", "3", "2", "0-2", "0-1", "0-3"),
  Robustness_to_noise = c("low", "NA", "low", "low", "low", "low", "mid", "mid", "high", "high", "low", "low", "mid", "high"),
  Computation_time = c("fast", "fast", "fast", "fast", "fast", "slow", "mid", "fast", "slow", "slow", "slow", "fast", "mid", "low"),
  Section = c("Linear Models and Its Cousins.html#sec-Linear_regression", "Linear Models and Its Cousins.html#sec-PLS", "Linear Models and Its Cousins.html#sec-Logistic_Regression", "Linear Models and Its Cousins.html#sec-Ridge_Regression", "Linear Models and Its Cousins.html#sec-Lasso_Regression", "Nonlinear Models.html#sec-SVM", "Nonlinear Models.html#sec-MARS", "Nonlinear Models.html#sec-KNN", "Nonlinear Models.html#sec-Tree_based_models", "Nonlinear Models.html#sec-Tree_based_models", "Nonlinear Models.html#sec-Neural_Networks", "Linear Models and Its Cousins.html#sec-LDA", "Nonlinear Models.html#sec-Naive_Bayes", "Nonlinear Models.html#sec-Tree_based_models")
) %>% 
  mutate(
    Section = str_replace_all(Section, " ", "%20"),
    Section = glue::glue("[Section]({Section})"),
    Section = map(Section, gt::md)
  )


Summary %>% gt() %>% cols_label(Allows = "Allows n < p", Preprocessing = "Pre-processing", Feature_selection = "Automatic feature selection", Tuning_parameters = "Number of tuning parameters", Robustness_to_noise = "Robust to predictor noise", Computation_time = "Computation time") %>% 
  tab_options(
    table.font.size = 12,
  ) %>% 
  cols_align(align = c("center"),
             columns = everything())
```
[@kuhnAppliedPredictiveModeling2013]

## Model Process for supervised models

![Modeling Process: four distinct models are being evaluated. When modeling data, there is almost never a single model fit or feature set that will immediately solve the problem. The process is more likely to be a campaign of trial and error to achieve the best results. The effect of feature sets can be much larger than the effect of different models. The interplay between models and features is complex and somewhat unpredictable. With the right set of predictors, is it common that many different types of models can achieve the same level of performance.](Images/modeling process.png)

0) Exploratory data analysis (evaluating simple summary measures or identifying predictors that have strong correlations with the outcome / how the predictors will be represented)
1) Pre-processing the predictor data 
2) Estimating model parameters 
3) Selecting predictors for the model 
4) Evaluating model performance 
5) Fine tuning class prediction rules (via ROC curves, etc.)

6) optimization routines (e.g., Nelderâ€“Mead simplex method = *direct methods*) can later be use to search the optimal key value (e.g., determine possible mixtures with improved compressive strength)
7) EDA can be conducted on the model results (e.g., residual analysis)

## Unsupervised models

### Clustering models 

Groups similar data points together based on distance

**k-Means** 

{{< video https://www.youtube.com/watch?v=4b5d3muPQmA start="60">}}

**Hierarchical Clustering** 

Hierarchical clustering focuses on the similarities between the individual instances and how similarities link them together. Tells you pairwise what two things are most similar.


{{< video https://www.youtube.com/watch?v=7xHsRkOdVwo start="134">}}

### Autoencoders

Neural networks that can denoise or smooth the predictor values and focus on capturing important features in the data. 

:::{.callout-tip}

## Use case
**Anomaly Detection**: The decoder struggles to capture anomalous patterns, and the reconstruction error acts as a score to detect anomalies. Identifies unusual patterns that differ from the majority of the data. Assumes that anomalies are: 

- **Rare** : the minority class that occurs rarely in the data\
- **Different** : have feature values that are very different from normal observations

**Image processing**
**dimension reduction**
**information retrieval**
:::

