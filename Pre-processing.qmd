---
title: "Pre-processing"
number-sections: true
---

## Transform the predictor/independent variables 

Important to transform independent variables that are skewn or containt outliers for models sensitive to them See Table

### Centering and scaling

**Advantages**: Improve the numerical stability to minimize potential numerical errors\
**Disadvantage**: Loss of interpretability

### Resolve Skewness 

Skewed data: Ratio of the highest value to the lowest value is greater than 20 have significant skewness. (e.g., if the lowest income is $1,000 and the highest income is $20,000 or more, the dataset exhibits significant skewness due to the large ratio between the highest and lowest values.)

- Left Skew: Mean < Median ≤ Mode (value that appears most frequently) 
- Right Skew: Mean > Median ≥ Mode

**Techniques**:

- log
- square root
- inverse
- Box and Cox (can only be applied to data that is strictly positive)

### Resolve Outliers 

**Techniques**:

- Spatial sign

::: {.callout-warning}

  - it is important to **center and scale** the predictor data prior to using this transformation
  - Avoid removing predictor variables after applying this technique as spatial sign transformation transforms predictors as a group.
  
:::

### Data Reduction and Signal/Feature Extraction

These methods reduce the data by generating a smaller set of predictors that seek to capture a majority of the information in the original variables. 

#### PCA (*unsupervised technique*)

The number of components to retain is choosen by creating a scree plot (Fig 1)

For most data sets, the first few PCs will summarize a majority of the variability, and the plot will show a steep descent; variation will then taper off for the remaining components. Generally, the component number prior to the tapering off of variation is the maximal component that is retained. In an automated model building process, the optimal number of components can be determined by cross-validation (see Resampling Techniques).

![**Figure 1**: The variation tapers off at component 5. Using this rule of thumb, four PCs would be retained](Images\scree plot.png)

**Advantages**: The primary advantage is that it creates components that are uncorrelated. The smaller dataset may be easier to deal with or to process. Moreover, the smaller dataset may better reveal the information.

**Disadvantage**: 

- Loss of interpretability. 

- Unsupervised technique which means that PCA it does not consider the modeling objective or response variable when summarizing variability. PCA can generate components that summarize characteristics of the data that are irrelevant to the underlying structure of the data and also to the ultimate modeling objective. **If the predictive relationship between the predictors and response is not connected to the predictors’ variability, then the derived PCs will not provide a suitable relationship with the response.** In this case, a supervised technique, like PLS, will derive components while simultaneously considering the corresponding response.

- Data should be linearly related

::: {.callout-warning}
First transform skewed predictors and then center and scale the predictors prior to performing PCA. Centering and scaling enables PCA to find the underlying relationships in the data without being influenced by the original measurement scales.
:::

::: {.callout-tip}
## Pro tips 
- If PCA has captured a sufficient amount of information in the data. **Visually examining the principal components** is a critical step for assessing data quality and gaining intuition for the problem. To do this, the first few principal components can be plotted against each other and the plot symbols can be colored by relevant characteristics, such as the class labels. 

  - Check for blatant **outliers** that may prompt a closer examination of the individual data points
  
  - Check for **clusters** of samples (for classification problems; Try other models that could better accommodate the data to have a final conclusion)
  
  - **Checks loadings to characterize which predictors are associated with each component** (Loadings close to zero indicate that the predictor variable did not contribute much to that component; Fig 2)
  
  - **Check for multicollinearity** (substantial correlation between multiple predictors): For example, if the first principal component accounts for a large percentage of the variance, this implies that there is at least one group of predictors that represent the same information. For example, Fig 1 indicates that the first 3–4 components have relative contributions to the total variance. This would indicate that there are at least 3–4 significant relationships between the predictors. Colinearity can increase the model variance
  
- If the percentages of variation explained are not large (e.g., less than 48 %) for the first three components, it is **important not to over-interpret** the resulting image.

:::

![**Figure 2**: loadings for the first three components in the cell segmentation data. For the first principal component, the loadings for the first channel are on the extremes. This indicates that channel 1 have the largest effect on the first principal component and by extension the predictor values. Also note that the majority of the loadings for the third channel are closer to zero for the first component. Conversely, the third principal component is mostly associated with the third channel while the first channel plays a minor role here.](Images/A plot of the loadings of the first three principal components.png)

  
#### PLS (*supervised technique*)

Derive components while simultaneously considering the corresponding response.

### Dealing with Missing Values

Before proceeding: It is important to understand why the values are missing to check for *informative missing* using visualization such as heatmap or cooccurrence plot (for smaller data) or by plotting the first two scores from a PCA model of the missing data indicator matrix (for larger data sets). Informative missingness can induce significant bias in the model.

**Examples**: 
 - people are more compelled to rate products when they have strong opinions (good or bad)
 - The tested drug was extremely ineffective or had significant side effects. The patient may be likely to miss doctor visits or to drop out of the study.
 

- **1)** **Remove predictors** from models if the percentage of missing data is substantial
- **2)** For large data sets, **removal of samples** based on missing values is not a problem
- **3** If we **do not remove** the missing data:
  - Use tree-based techniques, which account for missing data
  - code missing data as "missing"
  - impute missing data using information in the training set predictors (for small to moderate amounts of missingness). This amounts to a predictive model within a predictive model. If we are using resampling to select tuning parameter values or to estimate performance, the imputation should be incorporated within the resampling. This will increase the computational time for building models, but it will also provide honest estimates of model performance.
    - K-nearest neighbor model:
      *Advantages*: The imputed data are confined to be within the range of the training set values.
      *Disadvantages*: The entire training set is required every time a missing value needs to be imputed **and** The number of neighbors is a tuning parameter
    - Linear regression model: between a predictor with few missing points strongly associated with the predictor with missing data (correlation / visualizations / PCA. 
    - Bagged trees
      


**NOTES** 
Censored data &#8800; Missing data: The exact value is missing but something is known about its value. For example: If a customer has not yet returned a movie to blockbuster, we do not know the actual time span, only that it is as least as long as the current duration.

- For inference models: the censoring is usually taken into account in a formal manner by making assumptions about the censoring mechanism

- For predictive models, it is more common to treat these data as simple missing data or use the censored value as the observed value.


### Removing Predictors

**Advantages**: Does not compromise the performance and stability of the model. Decreased computational time and complexity. Lead to a more parsimonious and interpretable model

- Remove near-zero predictors (e.g., predictor variable where the percentage of unique values is low < 10% = unique values/total values and The ratio of the frequency of the most prevalent value to the frequency of the second most prevalent value is large > 20). 

- Remove problematic predictors with degenerate distributions as some models can be crippled by them 

- Remove highly correlated predictors as both mesure the same underlaying information. **For linear regressions use VIF for other sensitive models ensure that all pairwise correlations are below 0.75 threshold**

- Feature Selection  (see [Feature Selection](Feature%20Selection.html) section)

### Adding Predictors 

See [Feature_engineering](Feature%20Engineering.html#sec-Feature_engineering) section
